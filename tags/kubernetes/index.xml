<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes on minuk.dev</title><link>https://minuk.dev/tags/kubernetes/</link><description>Recent content in Kubernetes on minuk.dev</description><generator>Hugo</generator><language>ko-kr</language><lastBuildDate>Sat, 01 Oct 2022 22:07:56 +0900</lastBuildDate><atom:link href="https://minuk.dev/tags/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>Prometheus Intro and Deep Dive</title><link>https://minuk.dev/wiki/prometheus-intro-and-deep-dive/</link><pubDate>Sat, 01 Oct 2022 21:20:08 +0900</pubDate><guid>https://minuk.dev/wiki/prometheus-intro-and-deep-dive/</guid><description>&lt;ul>
&lt;li>&lt;a href="https://youtu.be/eM3RXdK1yys">원본 링크&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="what-is-prometheus">What is Prometheus&lt;/h2>
&lt;ul>
&lt;li>OSS metrics-based monitoring &amp;amp; alerting stack.&lt;/li>
&lt;li>Instrumentation - Metrics collection and storage - Querying, Alerting, Dashboarding&lt;/li>
&lt;li>Works for all levels of the stack!&lt;/li>
&lt;li>Mode for dynamic cloud environments.&lt;/li>
&lt;/ul>
&lt;h2 id="architecture">Architecture&lt;/h2>
&lt;ul>
&lt;li>Instrumentation &amp;amp; Exposition - Target (web app, API server, Linux VM, mysqld, cgouprs &amp;hellip;)&lt;/li>
&lt;li>Collection, Storage &amp;amp; Processing - Prometheus + Service Discovery (DNS, Kubernetes, AWS, &amp;hellip;)&lt;/li>
&lt;li>Qurying, Dashboards - Grafana, Prometheus Web UI, Automation&lt;/li>
&lt;li>Alertmanager&lt;/li>
&lt;/ul>
&lt;h2 id="selling-points">Selling Points&lt;/h2>
&lt;ul>
&lt;li>Dimensional data model&lt;/li>
&lt;li>Powerful query language&lt;/li>
&lt;li>Simple architecture &amp;amp; efficient server&lt;/li>
&lt;li>Service discovery integration&lt;/li>
&lt;/ul>
&lt;h2 id="data-model">Data Model&lt;/h2>
&lt;ul>
&lt;li>What is a time series?:
&lt;ul>
&lt;li>&lt;code>&amp;lt;identifyer&amp;gt; -&amp;gt; [(t0, v0), (t1, v1), ...]&lt;/code>&lt;/li>
&lt;li>metric name&lt;/li>
&lt;li>labels&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="querying">Querying&lt;/h2>
&lt;ul>
&lt;li>PromQL:
&lt;ul>
&lt;li>Functional query language&lt;/li>
&lt;li>Great for time series computations&lt;/li>
&lt;li>Not SQL-style&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="example">Example&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>All partitions in my entire infrastructure with more than 100GB capacity that are not mounted on root?:&lt;/p></description></item><item><title>CoreDNS - Intro and Deep Dive</title><link>https://minuk.dev/wiki/coredns-into-and-deep-dive/</link><pubDate>Thu, 22 Sep 2022 00:53:03 +0900</pubDate><guid>https://minuk.dev/wiki/coredns-into-and-deep-dive/</guid><description>&lt;ul>
&lt;li>&lt;a href="https://youtu.be/rNlSgYZoIYs">원본링크&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="what-is-coredns">What is CoreDNS?&lt;/h2>
&lt;ul>
&lt;li>Flexible DNS server written in GO&lt;/li>
&lt;li>Focus on service discovery&lt;/li>
&lt;li>Plugin based architecture, easily extended&lt;/li>
&lt;li>Support serving via DNS, DNS over TLS, DNS over HTTP/2, DNS over gRPC&lt;/li>
&lt;li>Support forwarding to upstream via DNS, DNS over TLS, DNS over gRPC&lt;/li>
&lt;li>Integration with Route53/Google Cloud DNS/Azuer DNS&lt;/li>
&lt;li>Integrates with Prometheus, Open Tracing, OPA&lt;/li>
&lt;li>Default DNS server in Kubernetes&lt;/li>
&lt;li>Basis for node local cache feature in K8s&lt;/li>
&lt;/ul>
&lt;h2 id="latest-updates">Latest Updates&lt;/h2>
&lt;ul>
&lt;li>1.8.5 - 1.9.2:
&lt;ul>
&lt;li>1.9.2. Released May, 2022&lt;/li>
&lt;li>New plugins: geoip, header:
&lt;ul>
&lt;li>geoip reports where the query comes from&lt;/li>
&lt;li>header allows fiddle with header bits&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backwards incompatible changes:
&lt;ul>
&lt;li>kubernetes: Removed wild card query functionality.&lt;/li>
&lt;li>route53: Plaintext secret in Corefile deprecated.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Built with golang 1.17.8+ since 1.9.1:
&lt;ul>
&lt;li>golang &amp;lt; 1.17.6 security issues&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="security-audit">Security Audit&lt;/h3>
&lt;ul>
&lt;li>Completed by Tail of Bits (March 2022)&lt;/li>
&lt;li>Sponsored by Linux Foundation&lt;/li>
&lt;li>1 high severity issue (TOB-CDNS-8):
&lt;ul>
&lt;li>May lead to cache poisoning attacks&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>1 medium issue (TOB-CDNS-12):
&lt;ul>
&lt;li>Mitigation possible withouth coredns update&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>12 low or informational issues.&lt;/li>
&lt;li>All have been resolved now.&lt;/li>
&lt;/ul>
&lt;h2 id="three-ways-to-cusomize-coredns">Three ways to cusomize CoreDNS&lt;/h2>
&lt;ul>
&lt;li>Rebuilding with external plugins&lt;/li>
&lt;li>Using CoreDNS as a library&lt;/li>
&lt;li>Building your own plugin&lt;/li>
&lt;/ul>
&lt;h3 id="rebuilding-with-external-plugins">Rebuilding with External Plugins&lt;/h3>
&lt;ul>
&lt;li>You do not need to know Go to do this!&lt;/li>
&lt;li>&amp;ldquo;External&amp;rdquo;:
&lt;ul>
&lt;li>Not built into the standard binaries and Docker images&lt;/li>
&lt;li>Not supported by core team&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>No dynamic loading of plugins:
&lt;ul>
&lt;li>Plugins are built-in at compile time&lt;/li>
&lt;li>Controlled by plugin.cfg&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Plugin ordering is fixed at compile time&lt;/li>
&lt;/ul>
&lt;h3 id="external-plugins">External Plugins&lt;/h3>
&lt;ul>
&lt;li>Prerequisites: Docker and a shell:
&lt;ol>
&lt;li>Clone CoreDNS&lt;/li>
&lt;li>Modify plugin.cfg&lt;/li>
&lt;li>Build CoreDNS&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h3 id="coredns-as-a-library">CoreDNS as a Library&lt;/h3>
&lt;ul>
&lt;li>Replace the CoreDNS main.go&lt;/li>
&lt;li>Allows you to:
&lt;ul>
&lt;li>Reduced the size and memory footprint of the binary&lt;/li>
&lt;li>Limit the functionality and CLI flags&lt;/li>
&lt;li>Do extra setup or initialization&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Used, for example, by Node Local DNS in K8s&lt;/li>
&lt;/ul>
&lt;h3 id="example-dnscached">Example: dnscached&lt;/h3>
&lt;ul>
&lt;li>Source is in &lt;a href="https://github.com/coredns/learning-coredns">https://github.com/coredns/learning-coredns&lt;/a>&lt;/li>
&lt;li>Simple caching DNS server&lt;/li>
&lt;li>Embeds only bind, cache, erros, forward and log plugins&lt;/li>
&lt;li>CLI args to generate a Corefile internally&lt;/li>
&lt;/ul>
&lt;h3 id="writing-a-plugin">Writing a Plugin&lt;/h3>
&lt;ul>
&lt;li>Three categories of plugins&lt;/li>
&lt;li>Best practice: stick to one of these in your plugin&lt;/li>
&lt;li>Backends:
&lt;ul>
&lt;li>Source of data&lt;/li>
&lt;li>file, forward, hosts, clouddns, template, kubernetes&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Mutators:
&lt;ul>
&lt;li>Modify the inbound request, the outbound response, or both&lt;/li>
&lt;li>acl, cache, rewrite, nsid&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Configurators:
&lt;ul>
&lt;li>Modify the internal state or functioning of CoreDNS&lt;/li>
&lt;li>bind, log, health, ready&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="four-functions">Four functions&lt;/h2>
&lt;ul>
&lt;li>Name - literally, just returns the name of the plugin&lt;/li>
&lt;li>ServeDNS - request handling&lt;/li>
&lt;li>init - register your plugin with Caddy&lt;/li>
&lt;li>setup - parse your config&lt;/li>
&lt;/ul>
&lt;h3 id="example-there-can-be-only-one">Example: There can be only one!&lt;/h3>
&lt;ul>
&lt;li>onlyone plugin from Learning CoreDNS&lt;/li>
&lt;li>Filters out all but one of specific record types&lt;/li>
&lt;/ul>
&lt;h3 id="function-setup">Function: setup&lt;/h3>
&lt;ul>
&lt;li>setup.go&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">setup&lt;/span>(&lt;span style="color:#a6e22e">c&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">caddy&lt;/span>.&lt;span style="color:#a6e22e">Controller&lt;/span>) &lt;span style="color:#66d9ef">error&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">t&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">parse&lt;/span>(&lt;span style="color:#a6e22e">c&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">plugin&lt;/span>.&lt;span style="color:#a6e22e">Error&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;onlyone&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">dnsserver&lt;/span>.&lt;span style="color:#a6e22e">GetConfig&lt;/span>(&lt;span style="color:#a6e22e">c&lt;/span>).&lt;span style="color:#a6e22e">AddPlugin&lt;/span>(&lt;span style="color:#66d9ef">func&lt;/span>(&lt;span style="color:#a6e22e">next&lt;/span> &lt;span style="color:#a6e22e">plugin&lt;/span>.&lt;span style="color:#a6e22e">Handler&lt;/span>) &lt;span style="color:#a6e22e">plugin&lt;/span>.&lt;span style="color:#a6e22e">Handler&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">t&lt;/span>.&lt;span style="color:#a6e22e">Next&lt;/span> = &lt;span style="color:#a6e22e">next&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">t&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="function-servedns">Function: ServeDNS&lt;/h3>
&lt;h2 id="개인-생각">개인 생각&lt;/h2>
&lt;ul>
&lt;li>기본적으로 Google 개발자들이 발표하는게 퀄리티가 좋은듯? 너무 기업에서 &amp;ldquo;우리 최고죠?&amp;rdquo; 이런 느낌으로 홍보 가까운 느낌인데, 구글껀 괜찮은듯, 일단 전체적으로 보편적으로 다들 쓰고 있는걸 말한다.: 구글이 쓰니까 보편적인건가? 뭐든 학습자 입장에서는 쓰지도 않을 Framework, Library 공부하는 것보다는 좋은듯&lt;/li>
&lt;li>CoreDNS Plugin 을 만드는 방법에 대해서 설명해주고 있다. 마지막에 Learning CoreDNS 책을 소개해주는데 github 에도 있어서 읽어봐야겠다.&lt;/li>
&lt;/ul></description></item><item><title>Deep Dive into Minikube</title><link>https://minuk.dev/wiki/deep-dive-into-minikube/</link><pubDate>Wed, 21 Sep 2022 01:59:49 +0900</pubDate><guid>https://minuk.dev/wiki/deep-dive-into-minikube/</guid><description>&lt;ul>
&lt;li>&lt;a href="https://youtu.be/Iyq_MlSku-I">원본&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="it-all-started-in-2016">It all started in 2016&lt;/h2>
&lt;ul>
&lt;li>Minkube was created 6 years ago by Google to alleviate the difficulties that developers had when setting up a Kubernetes environment for local development&lt;/li>
&lt;/ul>
&lt;h3 id="ok-google--assist-the-developers-please">OK Google &amp;hellip; Assist the developers please!&lt;/h3>
&lt;ul>
&lt;li>Google has continued to evolve the Minikube project to grow the Kubernetes ecosystem by making Kubernetes development more attractive and frictionless&lt;/li>
&lt;/ul>
&lt;h2 id="primary-goal">Primary Goal&lt;/h2>
&lt;ul>
&lt;li>make it simple to run Kubernetes locally for learning and day-to-day development, testing &amp;amp; debugging workflows:
&lt;ol>
&lt;li>inclusive&lt;/li>
&lt;li>Community-driven&lt;/li>
&lt;li>User-friendly&lt;/li>
&lt;li>Support all Kubernetes features&lt;/li>
&lt;li>Cross-platform&lt;/li>
&lt;li>Reliable&lt;/li>
&lt;li>High Performance&lt;/li>
&lt;li>Developer Focused&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h3 id="our-first-integration-tests-ran-in-the-office">Our first Integration tests ran in the office.&lt;/h3>
&lt;ul>
&lt;li>Minikube&amp;rsquo;s VM drivers needed Baremetal servers with virtualization enabled.&lt;/li>
&lt;li>Nested Virtualization only available for certain Linux Distros&lt;/li>
&lt;/ul>
&lt;h3 id="it-takes-a-village-to-test-minikube">It takes a village to test Minikube&lt;/h3>
&lt;ul>
&lt;li>Minikube is the most tested local Kubernetes tool.:
&lt;ul>
&lt;li>46 Self-hosted CI VMs in 5 different clouds (GCP, AWS, Equinix Metal, Azure, Macstadium) + Prow and Github Action&lt;/li>
&lt;li>296 end to end tests in integration testing suite&lt;/li>
&lt;li>100 unit tests&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="flake-rate-system">Flake Rate System&lt;/h2>
&lt;ul>
&lt;li>Problem:
&lt;ul>
&lt;li>Running hundreds of test cases on dozen platforms, there are always some flaky test that fail 10-15% of the time on Master.&lt;/li>
&lt;li>Reviewer had to have a lot of context to approve a PR with failed test.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Solution:
&lt;ul>
&lt;li>Run tests on master regularly, generate failed rate on master.&lt;/li>
&lt;li>On each PR comments how many of the Failed tests are a known Flake&lt;/li>
&lt;li>Automatically create Github issue for frequently failing test.&lt;/li>
&lt;li>Generate Visualized&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Minikube&amp;rsquo;s Flake Rate System is built on top of Gopogh&lt;/li>
&lt;/ul>
&lt;h3 id="what-is-gopogh---reducing-squinting-for-go-developers">What is Gopogh? - Reducing Squinting for go developers&lt;/h3>
&lt;ul>
&lt;li>Problem: Failed minikube test logs come with thousands of lines of post mortem logs low-level system logs. (sometimes 10K lines) that makes it very hard to see what log is for what!:
&lt;ul>
&lt;li>Created in a hackathon with a funny name&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="minikube-speaks-your-language">Minikube speaks your language&lt;/h2>
&lt;h2 id="checkout-minikubes-side-project">Checkout Minikube&amp;rsquo;s Side Project!&lt;/h2>
&lt;ul>
&lt;li>Slow jam&lt;/li>
&lt;li>Triage Party&lt;/li>
&lt;li>Gopogh&lt;/li>
&lt;li>Time To k8s&lt;/li>
&lt;li>Minikube-CI&lt;/li>
&lt;li>Pull Sheet&lt;/li>
&lt;/ul>
&lt;h2 id="the-story-of-kbuernetes-124">The story of Kbuernetes 1.24&amp;hellip;&lt;/h2>
&lt;ul>
&lt;li>Kubernetes removed the code for supporting docker runtime&lt;/li>
&lt;li>CNI&amp;hellip;&lt;/li>
&lt;li>Cgroup V2&amp;hellip;&lt;/li>
&lt;/ul>
&lt;h3 id="minikube-continues-to-support-docker-env">Minikube continues to support docker-env&lt;/h3>
&lt;ul>
&lt;li>Users love &amp;ldquo;min8ikube docker-env&amp;rdquo;(building images directly on the cluster) and we can&amp;rsquo;t blame them, it is 36X time faster than Image load.&lt;/li>
&lt;/ul>
&lt;h2 id="minikube-cpu-usage-overtime">Minikube CPU usage overtime&lt;/h2>
&lt;ul>
&lt;li>Save energy by using these Minikube features Features that can save energy:
&lt;ul>
&lt;li>try &amp;ldquo;minikube pause&amp;rdquo;&lt;/li>
&lt;li>Auto-Pause Addon&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="benchmarking">Benchmarking&lt;/h2>
&lt;ul>
&lt;li>Measure Weekly/Daily and per release&lt;/li>
&lt;li>Measure agsinst similar tools&lt;/li>
&lt;/ul>
&lt;h2 id="minikubes-base-image">Minikube&amp;rsquo;s Base Image&lt;/h2>
&lt;h3 id="did-you-know-minikube-maintains-its-own-linux">Did you know minikube maintains its own linux?&lt;/h3>
&lt;ul>
&lt;li>Hand Crafted - Just enough Linux for Kubernetes&lt;/li>
&lt;li>Small ISO - 280MB&lt;/li>
&lt;li>Based on CoreOS BUildroot&lt;/li>
&lt;li>Might Graduate out of Minikube to is own repo&lt;/li>
&lt;li>Advantages:
&lt;ul>
&lt;li>Granular control of enabled kernel modules and packages&lt;/li>
&lt;li>Tailored for Kubernetes&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="types-of-minikube-users">Types of Minikube users&lt;/h3>
&lt;ul>
&lt;li>Learn Kubernetes&lt;/li>
&lt;li>Develop on Kubernetes&lt;/li>
&lt;li>Test/CI&lt;/li>
&lt;/ul>
&lt;h3 id="new-category-of-minikube-users">New category of minikube users!&lt;/h3>
&lt;ul>
&lt;li>Tens of Blog posts, tweets and survey comments shows that a lot of new users are using minikube merely as a Docker Desktop Replacement.&lt;/li>
&lt;/ul>
&lt;h3 id="minikube-start--no-kubernetes">minikube start -no-kubernetes?&lt;/h3>
&lt;ul>
&lt;li>IMO : ?? what?&lt;/li>
&lt;/ul>
&lt;h3 id="top-differentiators-minikube-vs-similar-tools">Top differentiators Minikube vs similar tools&lt;/h3>
&lt;ul>
&lt;li>Multiple container runtimes for Kubernetes&lt;/li>
&lt;li>Direct access to container runtime for faster image build&lt;/li>
&lt;li>Integration tests (most comprehensive)&lt;/li>
&lt;/ul>
&lt;h2 id="advantages-of-vm-drivers">Advantages of VM Drivers&lt;/h2>
&lt;ul>
&lt;li>No need to have Docker Desktop License&lt;/li>
&lt;li>Less CPU usage&lt;/li>
&lt;li>You can hit the port directly (for example if you have a hotspot service running on port 80 you can curl $(minikube ip):80 on your machine vs Docker Drive that by design needs to be assigned a random port.)&lt;/li>
&lt;/ul>
&lt;h2 id="tens-of-survey-requests-for-vm-driver-on-m1arm64">Tens of Survey Requests for VM driver on M1/Arm64&lt;/h2>
&lt;h3 id="1-try-qemu-driver-on-apple-m1">1. Try Qemu Driver on Apple M1&lt;/h3>
&lt;ul>
&lt;li>Qemu driver is finally available for Arm64 and M1&lt;/li>
&lt;li>This means on Arm-based machines like Apple M1 you could have a Kbuernetes experience without having to have Docker Desktop.&lt;/li>
&lt;/ul>
&lt;h3 id="challenges-of-adding-arm64-iso">Challenges of adding ARM64 ISO&lt;/h3>
&lt;ul>
&lt;li>Slow iteration of testing&lt;/li>
&lt;li>BIOS/EFI&lt;/li>
&lt;li>AppArmor&lt;/li>
&lt;li>Lack of team familiarity with Buildroot&lt;/li>
&lt;/ul>
&lt;h3 id="2-try-early-prototype-of-minikube-gui">2. Try Early prototype of Minikube-GUI&lt;/h3>
&lt;ul>
&lt;li>Go to minikube website&lt;/li>
&lt;li>Search for Minikube GUI&lt;/li>
&lt;li>Things to try:
&lt;ul>
&lt;li>Simplified View (one cluster)&lt;/li>
&lt;li>Advanced View (multi cluster)&lt;/li>
&lt;li>Right click tray icon&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="개인-생각">개인 생각&lt;/h2>
&lt;ul>
&lt;li>개인적으로 퀄리티가 굉장히 좋은 강연이였다.&lt;/li>
&lt;li>일단:
&lt;ul>
&lt;li>minikube에 대한 간략한 설명(k8s 에 고통을 덜기위해)&lt;/li>
&lt;li>왜 써야하는지 (안정성)&lt;/li>
&lt;li>겪고 있는 문제(flake, k8s 1.24)&lt;/li>
&lt;li>화제거리(docker 를 사용하기 위해 k8s 없이 minikube 를 돌릴수 있게 해달라)&lt;/li>
&lt;li>새로운 소식(M1 에 대한 지원, GUI 지원)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>위 사항들을 모두 요약한 좋은 강연이였다고 생각된다.&lt;/li>
&lt;li>다만 아쉬운 점은 최근 이야기가 많이 나오고 있는 k3s라던가 minikube 가 내부적으로 어떻게 동작하는지에 대한 설명이 있었으면 좋았을 거 같은데, 사실 이건 시간 관계상 하기 어렵기도 하고 minikube 를 사용하는 사람이 아닌 개발하는 사람에게 필요한 내용이므로 적합하지 않다고 생각해서 뺀것 같다. (혹은 예전에 이미 했어서 중복되서 안하거나?)&lt;/li>
&lt;li>이 강연을 통해서 얻게된 내용:
&lt;ul>
&lt;li>minikube 내부구조를 살짝 공부하고, kind(kubernetes in docker) 와 k3s 와 비교해봐야겠다. (목적성이나 구조적인 문제 둘다)&lt;/li>
&lt;li>중간에 소개해줬던 내부 툴 repo를 살펴봤는데, 현재 사용하고 있지만 툴 자체가 발전하지는 않는 상황인것 같다. 살펴봐야겠다.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Making Your Apps and Infrastructure Services Failure-Resilient with Dapr</title><link>https://minuk.dev/wiki/making-your-apps-and-infrastructure-services-failure-resilient-with-dapr/</link><pubDate>Wed, 07 Sep 2022 00:57:53 +0900</pubDate><guid>https://minuk.dev/wiki/making-your-apps-and-infrastructure-services-failure-resilient-with-dapr/</guid><description>&lt;ul>
&lt;li>&lt;a href="https://youtu.be/Jw05zFpsPms">원본 링크&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="what-is-dapr">What is Dapr?&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://dapr.io/">Dapr : Distributed Application Runtime&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="sidecar">Sidecar&lt;/h3>
&lt;ul>
&lt;li>Dapr API (HTTP/gRPC)&lt;/li>
&lt;/ul>
&lt;h3 id="dapr-components">Dapr Components&lt;/h3>
&lt;ul>
&lt;li>State Stores&lt;/li>
&lt;li>Pubsub Brokers&lt;/li>
&lt;li>Bindings &amp;amp; Triggers&lt;/li>
&lt;li>Secret Stores&lt;/li>
&lt;li>Observability&lt;/li>
&lt;li>Configuration&lt;/li>
&lt;/ul>
&lt;h2 id="resiliency">Resiliency&lt;/h2>
&lt;ul>
&lt;li>State Management&lt;/li>
&lt;li>Application Configuration&lt;/li>
&lt;li>Input Binding&lt;/li>
&lt;li>Service Invocation&lt;/li>
&lt;li>Secret Management&lt;/li>
&lt;li>Publish &amp;amp; Subscribe&lt;/li>
&lt;li>Output Binding&lt;/li>
&lt;/ul>
&lt;h2 id="resiliency-configuration-yaml">Resiliency Configuration YAML&lt;/h2>
&lt;h3 id="resiliency-as-crd">Resiliency as CRD&lt;/h3>
&lt;ul>
&lt;li>In kubernetes Resiliency is defined as a CRD&lt;/li>
&lt;li>Allows for multiple policies to be defined&lt;/li>
&lt;li>Dapr merges all found policies into singl configuration&lt;/li>
&lt;/ul>
&lt;h3 id="resiliency-policies">Resiliency Policies&lt;/h3>
&lt;ul>
&lt;li>Timeouts:
&lt;ul>
&lt;li>Allows for the cancellation of requets after a given duration&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Regries:
&lt;ul>
&lt;li>Allows for the generic retrying of a request or operation&lt;/li>
&lt;li>Two supported retry types. constant and exponential&lt;/li>
&lt;li>Can specify erros which are retryable and permanent&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Circuit Breakers:
&lt;ul>
&lt;li>Allows for broken/breaking systems to be cut-off from requests&lt;/li>
&lt;li>Helps reduce traffic and requests to allow for recovery time&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="resiliency-policies---retries">Resiliency Policies - Retries&lt;/h4>
&lt;ul>
&lt;li>Constant Policieis:
&lt;ul>
&lt;li>maxRetries - The maximum number of attempts to make for a request&lt;/li>
&lt;li>duration - The time in-between retries&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Exponential Policies:
&lt;ul>
&lt;li>maxRetries - The maximum number of attempts to make for a request&lt;/li>
&lt;li>initialinterval - The starting time between retries&lt;/li>
&lt;li>randomizationFactor - Jitter used to offset requests&lt;/li>
&lt;li>multiplier - Growth rate of the retry interval&lt;/li>
&lt;li>maxInterval - The maximum duration between retries&lt;/li>
&lt;li>maxElapsedTime - The maximum time spent over all retries&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="resiliency-policies---circuit-breakers">Resiliency Policies - Circuit Breakers&lt;/h4>
&lt;ul>
&lt;li>maxRequets - The maximum number of requets allowed while the breakers is in the half-open state&lt;/li>
&lt;li>interval - The cyclical period that errors are evaluated in, if not specified the evaluation period is continuous&lt;/li>
&lt;li>timeout - the Time in which the circuit breaker will remain open after breaking&lt;/li>
&lt;li>trip - The criteria that errors are evaluated against to trigger state changes&lt;/li>
&lt;/ul>
&lt;h3 id="resiliency-targets">Resiliency Targets&lt;/h3>
&lt;ul>
&lt;li>Can be defined as Applications, Actors, and Components&lt;/li>
&lt;li>A target maps the policies to be used when calling into a system&lt;/li>
&lt;/ul>
&lt;h2 id="개인-생각">개인 생각&lt;/h2>
&lt;ul>
&lt;li>전반적으로 kubecon 이 자기가 만들거나 사용하고 있는 프로그램을 소개할수 밖에 없는 구조 이긴한데, 이건 좀 그냥 그랬다.&lt;/li>
&lt;li>그냥 아.. 있나보다 싶은 생각?&lt;/li>
&lt;li>굳이 공부한 거라고는 Policy 마다 저런 요소들을 고민해야한다 정도?&lt;/li>
&lt;li>실제로 내가 Dapr 을 쓸께아니라면, 그닥 매력적인 강연은 아닌것 같다.&lt;/li>
&lt;/ul></description></item><item><title>Make Cloud Native Chaos Engineering Easier Deep Dive into Chaos Mesh</title><link>https://minuk.dev/wiki/make-cloud-native-chaos-engineering-easier-deep-dive-into-chaos-mesh/</link><pubDate>Sun, 04 Sep 2022 02:42:20 +0900</pubDate><guid>https://minuk.dev/wiki/make-cloud-native-chaos-engineering-easier-deep-dive-into-chaos-mesh/</guid><description>&lt;ul>
&lt;li>&lt;a href="https://youtu.be/bZnI5omUKe4">원본&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="testing-a-distributed-system-is-difficult">Testing a distributed system is difficult&lt;/h2>
&lt;ul>
&lt;li>Distributed systems are more and more complex nowadays:
&lt;ul>
&lt;li>Faults can happen anytime, anywhere, in any ways&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Writing tests and debugging is hard:
&lt;ul>
&lt;li>Deterministic test is very hard and impossible to cover all faults&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>But, No crash, No data loss, No wrong results&lt;/li>
&lt;/ul>
&lt;h2 id="chaos-engieering-to-the-rescue">Chaos Engieering to the rescue&lt;/h2>
&lt;ul>
&lt;li>Chaos engineering is about breaking things in a controlled environment and through well-planned experiments in order to build confidence in your application to withstand turbulent conditions.&lt;/li>
&lt;li>Chaos engineering is NOT about breaking things randomly without a perpose.&lt;/li>
&lt;li>Program Cycle:
&lt;ul>
&lt;li>Improve -&amp;gt; Steady State -&amp;gt; Hypothesis -&amp;gt; Run Experiment -&amp;gt; Verify -&amp;gt; Improve&amp;hellip;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="why-chaos-mesh">Why Chaos Mesh&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>On Kubernetes:&lt;/p></description></item><item><title>Volcano - Intro &amp; Deep Dive</title><link>https://minuk.dev/wiki/volcano-intro-and-deep-dive/</link><pubDate>Tue, 30 Aug 2022 23:20:47 +0900</pubDate><guid>https://minuk.dev/wiki/volcano-intro-and-deep-dive/</guid><description>&lt;ul>
&lt;li>&lt;a href="https://youtu.be/a76CajRhsX0">원본링크&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="intro--deep-dive---volcano-a-cloud-native-batch-system">Intro &amp;amp; Deep dive - Volcano: A Cloud Native Batch System&lt;/h1>
&lt;h2 id="cloud-native-for-intelligent-workload">Cloud Native for Intelligent Workload&lt;/h2>
&lt;ul>
&lt;li>More and more organization are leveraging cloud native technology to avoid fragmental ecosystem, isolated stack, low resource utilization&lt;/li>
&lt;/ul>
&lt;h2 id="batch-on-k8s-challenges">Batch on K8s: Challenges&lt;/h2>
&lt;ul>
&lt;li>Job meanagement:
&lt;ul>
&lt;li>Pod level scheduling, no awareness of upper-level applications.&lt;/li>
&lt;li>Lack of fine-grained lifecycle management.&lt;/li>
&lt;li>Lack of task dependencies, Job dependencies.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Scheduling:
&lt;ul>
&lt;li>Lack of job based scheduling, e.g. job ordering, job priority, job preemption, job fair-share, job reservation.&lt;/li>
&lt;li>Not enough advanced scheduling algortihms, E.g. CPU topology, task-topology, IO-Awareness, backfill.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Multi-framework support:
&lt;ul>
&lt;li>Insufficient suport for mainstream computing frameowrks like MPI, Tensorflow, Mxnet, Pytorch.&lt;/li>
&lt;li>Complex deployment and O&amp;amp;M because each frameowrk corresponding to a different operator.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Resource planning, sharing, heterogeneous computing:
&lt;ul>
&lt;li>Lack of support to resource sharing mechanism between jobs, queues, namespaces.&lt;/li>
&lt;li>Lack of Deeper support on heterogenous resources.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Performance:
&lt;ul>
&lt;li>Not enough throughput, roundtrip for batch workload.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="volcano-overview">Volcano Overview&lt;/h2>
&lt;ul>
&lt;li>Created at March 2019; Sandbox at April 2020; Incubator at April 2022&lt;/li>
&lt;li>2.3k star, 360+ contributors, latest version v1.5.1&lt;/li>
&lt;li>50+ enterprises adopt Volcano in production environments.&lt;/li>
&lt;/ul>
&lt;h3 id="key-concept">Key Concept&lt;/h3>
&lt;ul>
&lt;li>Job:
&lt;ul>
&lt;li>Multiple Pod Template&lt;/li>
&lt;li>Lifecycle management/Erro handling&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>User/namespace/resource quota:
&lt;ul>
&lt;li>namespace is regarded as user&lt;/li>
&lt;li>resource quota is regarded as the upper limit resource that users in the namespace are able to use at most. Like the QPS in Kube-apiserver.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Resource share:
&lt;ul>
&lt;li>Use Queue for resource sharing&lt;/li>
&lt;li>Share resources between different &amp;ldquo;tenants&amp;rdquo; or resource pools.&lt;/li>
&lt;li>Support different scheduling policies or algorithms for different &amp;ldquo;tenants&amp;rdquo; or resource pools.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="job-mangement">Job mangement&lt;/h3>
&lt;ul>
&lt;li>Volcano Job:
&lt;ul>
&lt;li>Unified Job interface for most of batch job like mpi, pytorch, tensorflow, mxnet, etc.&lt;/li>
&lt;li>Fine-grained Job Lifecycle mangement&lt;/li>
&lt;li>Extendable job plugin:
&lt;ul>
&lt;li>env, svc, ssh, tensorflow&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Coordinate with Scheduler&lt;/li>
&lt;li>Job dependency&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="resource-mangement--queue">Resource mangement- Queue&lt;/h3>
&lt;ul>
&lt;li>Queue is cluster scoped, decoupled with user/namespace&lt;/li>
&lt;li>Queue is used to share resources between &amp;ldquo;multi-tenants&amp;rdquo; or resource pool.&lt;/li>
&lt;li>Configure policy for each queue, e.g. FIFO, fair share, priority, SLA.&lt;/li>
&lt;/ul>
&lt;h3 id="dynamic-resource-sharing-between-queues">Dynamic resource sharing between queues&lt;/h3>
&lt;ul>
&lt;li>Queue Guarantee/Capacity&lt;/li>
&lt;li>Share resource between Queues proportionally by weight&lt;/li>
&lt;/ul>
&lt;h4 id="개인-생각">개인 생각&lt;/h4>
&lt;ul>
&lt;li>?? 왜 Queue 들끼리 균등하게 분할할 생각이 아니라 Queue 끼리 자원을 서로 대여하는 구조인거지??&lt;/li>
&lt;li>Queue 끼리 우선순위를 조정하는 Policy 도 있겠지?? 일단 Queue 내부 Policy 는 있는거 같은데 Queue 간 제어하는 로직이 확실치 않네&lt;/li>
&lt;/ul>
&lt;h3 id="fair-share-within-queue">Fair share within Queue&lt;/h3>
&lt;ul>
&lt;li>Sharing resource between jobs&lt;/li>
&lt;li>Sharing resource between namespaces&lt;/li>
&lt;li>Per-Queue policy (FIFO, Priority, Fair share, &amp;hellip;)&lt;/li>
&lt;/ul>
&lt;h4 id="case--hierarchical-queue">Case : hierarchical queue&lt;/h4>
&lt;ul>
&lt;li>How to share resource in a multi-level org more easily?&lt;/li>
&lt;li>Problem: flat queue cannot meet complex resource share and isolation easily for big org.&lt;/li>
&lt;li>Solution:
&lt;ul>
&lt;li>Multiple level queue constructs a tree which is mapped to the org.&lt;/li>
&lt;li>Each level queue has min, max, weight. Use max to isolate resource, use queue weight to balance resource betweeen queues.&lt;/li>
&lt;li>Share resources between queues and reclaim by weight&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Benefit:
&lt;ul>
&lt;li>Flexible resource mangement, easy to map the organization&lt;/li>
&lt;li>fine-grained control resource share and isolation for a big multi-tenants organization&lt;/li>
&lt;li>The queue min capacity ensures guaranteed resource, the proportion by weight offers flexible sharing&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="scenario-elastic-scheduling">Scenario: Elastic scheduling&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>What is elastic job&lt;/p></description></item><item><title>Intro to Kubernetes, GitOps, and Observability Hands-On Tutorial</title><link>https://minuk.dev/wiki/intro-to-kubernetes-gitops-and-observability-hands-on-tutorial/</link><pubDate>Thu, 25 Aug 2022 00:06:14 +0900</pubDate><guid>https://minuk.dev/wiki/intro-to-kubernetes-gitops-and-observability-hands-on-tutorial/</guid><description>&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=WKvogzTg2iM">원본&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>기본적으로 따라서 하는 Hands On Tutorial 강의이다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="intro-to-kubernetes">Intro to Kubernetes&lt;/h2>
&lt;ul>
&lt;li>Oss CNCF graduated project for container orchestration&lt;/li>
&lt;li>Declarative configuration to manage containerized workloads and services&lt;/li>
&lt;li>Cloud Native, provides:
&lt;ul>
&lt;li>Automation and observability&lt;/li>
&lt;li>Self-healing and horizontal scaling&lt;/li>
&lt;li>Service discovery and load balancing&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Scalable, runs on-premises, in public cloud, and hybrid environments&lt;/li>
&lt;/ul>
&lt;h3 id="kubernetes-cluster-overview">Kubernetes Cluster Overview&lt;/h3>
&lt;ul>
&lt;li>API server&lt;/li>
&lt;li>Cloud controller manager(optional)&lt;/li>
&lt;li>Controller manager&lt;/li>
&lt;li>etcd(persistence store)&lt;/li>
&lt;li>kubelet&lt;/li>
&lt;li>kube-proxy&lt;/li>
&lt;li>Scheduler&lt;/li>
&lt;li>Control plane&lt;/li>
&lt;li>Node&lt;/li>
&lt;/ul>
&lt;h3 id="kubernetes-resources-review">Kubernetes Resources Review&lt;/h3>
&lt;ul>
&lt;li>Kubernetes REST API and declarative resources manage operations and communications between components&lt;/li>
&lt;li>Kubernetes API Groups (resources grouped based on their primary functions):
&lt;ul>
&lt;li>RBAC, scheduling, admission registration, autoscaling, events, apps, core&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Core API group objects (core/v1 + apps/v1 API Groups):
&lt;ul>
&lt;li>Namespaces, Deployments, Services, Secrets&lt;/li>
&lt;li>CRUD operations&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>API extensions via Custom Resource Definitions + Controllers&lt;/li>
&lt;li>Declaraitve (YAML)&lt;/li>
&lt;/ul>
&lt;h3 id="kubernetes-resources-at-a-galance">Kubernetes Resources At A Galance&lt;/h3>
&lt;ul>
&lt;li>Container : Runs an image (immutable copy of your application code and all code dependencies in an isolated environment&lt;/li>
&lt;li>Pod : A set of containers, co-scheduled on one machien. Mortal. Has pod IP. Has labels.&lt;/li>
&lt;li>Deployment : Ensures a certain number of replicas of a pod are running across the cluster&lt;/li>
&lt;li>Service : Gets virtual IP, mapped to endpoints via labels, Named in DNS&lt;/li>
&lt;li>Namespace : Resource names are scoped to a Namespace. Logical boundary&lt;/li>
&lt;/ul>
&lt;h2 id="intro-to-gitops">Intro to GitOps&lt;/h2>
&lt;h3 id="gitops-principles">GitOps Principles&lt;/h3>
&lt;ul>
&lt;li>Declarative: A system managed by GitOPs must have its desired state expressed declaratively&lt;/li>
&lt;li>Versioned and Immutable: Desired state is stored in a way that enforces immutatibility, versioning and retains a complete version history.&lt;/li>
&lt;li>Pulled Automatically: Software agents automatically pull the desired state declarations from the source&lt;/li>
&lt;li>Continuously Reconciled: Software agents continuously observe actual system state and attempt to apply the desired state.&lt;/li>
&lt;/ul>
&lt;h3 id="gitops-a-cloud-native-operating-model">GitOps: A Cloud Native Operating Model&lt;/h3>
&lt;ul>
&lt;li>Unifying Deployment, Monitoring and Management:
&lt;ul>
&lt;li>Git as the single source of truth of a system&amp;rsquo;s desired state&lt;/li>
&lt;li>ALL intended operations are committed by pull request&lt;/li>
&lt;li>ALL diffs between intended and observed state with automatic convergence&lt;/li>
&lt;li>ALL changes are observable, verifiable, and auditable.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="intro-to-gitops-1">Intro to GitOps&lt;/h3>
&lt;ul>
&lt;li>GitOps is the practice of using Git to store declaratively defined desired state and Continuous Delivery agents (e.g. Flux) to automate the reconcilation of current state to desired state. With GitOps, CI and CD are effectively decoupled.&lt;/li>
&lt;/ul>
&lt;h3 id="intro-to-flux">Intro to Flux&lt;/h3>
&lt;ul>
&lt;li>OSS CNCF Project&lt;/li>
&lt;li>Created at Weaveworks&lt;/li>
&lt;li>Runtime composed of Kubernetes Controllers + CRDs&lt;/li>
&lt;li>Flux keeps kubernetes clusters in sync with sources of configuration and automatically + continuously reconciles running state to desired state.&lt;/li>
&lt;/ul>
&lt;h2 id="intro-to-observability">Intro to Observability&lt;/h2>
&lt;ul>
&lt;li>Monitoring vs. Observability:
&lt;ul>
&lt;li>Monitoring: Metrics, alerts, actionable, dashboards, canned quieries&lt;/li>
&lt;li>Observabiilty: Inspect, observe, explore, trace, custom queries&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="observability-instrumentation">Observability Instrumentation&lt;/h3>
&lt;ul>
&lt;li>Metrics:
&lt;ul>
&lt;li>Prometheus:
&lt;ul>
&lt;li>OSS CNCF monitoring and alerting toolkit&lt;/li>
&lt;li>Time series database for metrics collection created by SoundCloud&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>DataVisualization:
&lt;ul>
&lt;li>Grafana:
&lt;ul>
&lt;li>OSS metrics visualization dashboards&lt;/li>
&lt;li>Created by Grafana Labs, CNCF Platinum Partner&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Logging:
&lt;ul>
&lt;li>Fluent Bit:
&lt;ul>
&lt;li>OSS CNCF project for lightweight logs and metrics processing + forwarding&lt;/li>
&lt;li>Sub-project under Fluentd umbrella created by Treasure Data&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="개인생각">개인생각&lt;/h2>
&lt;ul>
&lt;li>강연이 좀 혼란했다. 개념적인걸 설명하고, Flux 를 활용한 gitops 실습이 목표였던 것 같은데 codespace가 잘 동작하지 않거나, port binding 이 잘 동작하지 않아서 영상 중 상당 시간이 지연되었다.&lt;/li>
&lt;li>실습 자료가 나쁜편은 아닌데, 어짜피 대부분은 경험해본적이 있어서 굳이 실습을 하진 않았다.&lt;/li>
&lt;li>영상만을 보고 실습을 하기에는 조금 무릭 ㅏ있다. 기본적으로 로그인도 안되고, 실습 환경도 직접 구축해야하니까&amp;hellip;&lt;/li>
&lt;li>얻어간건 위에 요약된 정보들과 k9s 정도 이다.&lt;/li>
&lt;/ul></description></item><item><title>kubespray</title><link>https://minuk.dev/wiki/kubespray/</link><pubDate>Wed, 24 Aug 2022 17:20:22 +0900</pubDate><guid>https://minuk.dev/wiki/kubespray/</guid><description>&lt;h3 id="문서-목적">문서 목적&lt;/h3>
&lt;ul>
&lt;li>kubespray 를 하면서 삽질한 내용 기록&lt;/li>
&lt;/ul>
&lt;h3 id="참고자료">참고자료&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://www.whatwant.com/entry/Kubespray">kubespray 이용하여 kubenetes 설치하기&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/kubespray">공식사이트&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="내가-한-일">내가 한 일&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>swap off:&lt;/p>
&lt;ul>
&lt;li>각 노드에서 아래 명령어를 실행해준다.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo swapoff -a
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>kubespray 를 clone 해준다.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>git clone https://github.com/kubernetes-sigs/kubespray.git
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>controlplane 이 될 곳에서 ansible 설정을 해준다.:&lt;/p>
&lt;ul>
&lt;li>그냥 linux 이기만 하면 된다. controlplane이 아니여도 된다.&lt;/li>
&lt;li>mac 에서 실행하려고 하니 공식문서에 나와있는 ansible version 이 잘 설치가 안되서, 그냥 controlplane 에서 작업했다.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>VENVDIR&lt;span style="color:#f92672">=&lt;/span>kubespray-venv
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>KUBESPRAYDIR&lt;span style="color:#f92672">=&lt;/span>kubespray
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ANSIBLE_VERSION&lt;span style="color:#f92672">=&lt;/span>2.12
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>virtualenv --python&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>which python3&lt;span style="color:#66d9ef">)&lt;/span> $VENVDIR
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>source $VENVDIR/bin/activate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cd $KUBESPRAYDIR
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pip install -U -r requirements-$ANSIBLE_VERSION.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test -f requirements-$ANSIBLE_VERSION.yml &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> ansible-galaxy role install -r requirements-$ANSIBLE_VERSION.yml &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> ansible-galaxy collection -r requirements-$ANSIBLE_VERSION.yml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul></description></item><item><title>Spark on Kubernetes - The Elastic Story</title><link>https://minuk.dev/wiki/spark-on-kubernetes-the-elastic-story/</link><pubDate>Tue, 23 Aug 2022 02:46:35 +0900</pubDate><guid>https://minuk.dev/wiki/spark-on-kubernetes-the-elastic-story/</guid><description>&lt;ul>
&lt;li>&lt;a href="https://youtu.be/n7WeoTJq-40">원본링크&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="benefits-of-cloud">Benefits of Cloud&lt;/h2>
&lt;ul>
&lt;li>Agile : Resources are on-dmand, pay as you go&lt;/li>
&lt;li>Elastic &amp;amp; Scalable : Almost infinite scale of compute and storage&lt;/li>
&lt;li>Strong Resource Isolation : Container-native on K8S&lt;/li>
&lt;li>Privacy-First : Leverage cloud security techniques to enforce security&lt;/li>
&lt;li>Operation Friendly : Our developers can focus on building and improving services to achieve higher ROI&lt;/li>
&lt;/ul>
&lt;h2 id="design-principles">Design Principles&lt;/h2>
&lt;ul>
&lt;li>Leverage Cloud Infra&lt;/li>
&lt;li>Full Containerizng - Elastic, Agile, Lightweight&lt;/li>
&lt;li>Decouple Compute/Storage, Scale Independently&lt;/li>
&lt;li>Developer-Friendly, API Centric&lt;/li>
&lt;li>Security &amp;amp; Privacy as First Class Citizen&lt;/li>
&lt;li>Use Apple Internal Spark distribution&lt;/li>
&lt;/ul>
&lt;h2 id="architecture-of-cloud-native-spark-service">Architecture of Cloud-Native Spark Service&lt;/h2>
&lt;ul>
&lt;li>원본 링크 PPT 참고&lt;/li>
&lt;li>Spark K8s Operator 가 Resource Queues 를 가르키고 있고 Resource Queue 는 Node 를 관리한다.&lt;/li>
&lt;li>이를 Skate(Spark service gateway) 로 호출하면서 관리하며, 이는 API, CLI, Airflow 등 다양한 Batch Processing 으로 처리한다.&lt;/li>
&lt;li>또한 Jupyter Notebook 을 Interactive Spark Gateway랑 연결해서 달아둘수도 있다.&lt;/li>
&lt;li>이러한 환경에서 Observability Infra, Security &amp;amp; Privacy Infra 가 당연하게도 깔려있어야한다.&lt;/li>
&lt;/ul>
&lt;h2 id="cost-saving-and-elasticity-needs">Cost saving and Elasticity Needs&lt;/h2>
&lt;ul>
&lt;li>Varing workload pattern: fluctuating within a day and/or a week&lt;/li>
&lt;li>Different use cases: daily/weekly scheduled jobs, ad hoc jobs, scheduled + adhoc, backfill&lt;/li>
&lt;li>Fixed amount of resources must account for max usage, which causes resource waste&lt;/li>
&lt;/ul>
&lt;h2 id="design-of-reactive-autoscaling">Design of Reactive AUtoscaling&lt;/h2>
&lt;h3 id="reactive-autoscaling-cluster-nodegroups-layout">Reactive AutoScaling Cluster NodeGroups Layout&lt;/h3>
&lt;ul>
&lt;li>자세한건 원본 링크 PPT 참고&lt;/li>
&lt;li>Physical isolation : Minimize potential impact&lt;/li>
&lt;li>Minimum capacity: Guaranteed at any time&lt;/li>
&lt;li>maximum capacity: Jobs will be queued if executed&lt;/li>
&lt;li>Multi-tenant Autoscaling K8S Cluster
&lt;ul>
&lt;li>spark-system(node group) : static size; shared by all queues&lt;/li>
&lt;li>spark-drive(node group): scale-out; shared by all queues&lt;/li>
&lt;li>spark-executor(node group): scale in/out&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="reactive-autoscaling-workflow">Reactive AutoScaling Workflow&lt;/h3>
&lt;ul>
&lt;li>자세한건 원본 링크 참조&lt;/li>
&lt;li>CLI/SDK/Airflow Operator of Skate clients - 1. Submitting Spark job to Spark Platform&lt;/li>
&lt;li>Skate(Spark service gateway) - 2. Creating SparkAPP CRD on cluster&lt;/li>
&lt;li>Spark K8S Operator - 3. Creating driver and executor pods in YK resources queue&lt;/li>
&lt;li>YuniKorn - 4. Creating Spark Driver pod in driver node group&lt;/li>
&lt;li>YuniKorn - 5. Creating Spark executor pods in executor node group&lt;/li>
&lt;li>6.1 Sned pending pods signal in each node groups for scale-out&lt;/li>
&lt;li>6.2 Send idel node in each node groups for scale in&lt;/li>
&lt;li>
&lt;ol start="7">
&lt;li>Send scale in/out request to cloud provider per node group&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h3 id="reactive-autoscaling-scale-inout-controls">Reactive AutoScaling Scale in/out Controls&lt;/h3>
&lt;ul>
&lt;li>Scale in Controls:
&lt;ul>
&lt;li>Only when no running executor pods on the node&lt;/li>
&lt;li>Enabled Apache YuniKorn bin-packing in resource scheduling&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Scale out Controls:
&lt;ul>
&lt;li>Spark-driver node group scale out only&lt;/li>
&lt;li>Speed up executor pods allocation size config of Spark&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="production-status">Production Status&lt;/h2>
&lt;ul>
&lt;li>In production for 3+ months&lt;/li>
&lt;li>Cost saving report: Cost saving percentage per queue is located in 20% - 70%&lt;/li>
&lt;/ul>
&lt;h3 id="migration-findings-1">Migration findings 1&lt;/h3>
&lt;ul>
&lt;li>Scale in/out events are stable:
&lt;ul>
&lt;li>Tens of thousands of job per week running successfully&lt;/li>
&lt;li>All scale-in events works as expected&lt;/li>
&lt;li>Scale out latency is consistent(&amp;lt;= 5 mins from 2 to 200)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="migration-findings-2">Migration findings 2&lt;/h3>
&lt;ul>
&lt;li>Compared to massive over-provisioning approach before, runtime of workloads with autoscaling enbaled may increase:
&lt;ul>
&lt;li>Most negligible, a couple jobs increased ~20%&lt;/li>
&lt;li>Users need to take this into consideration and optimize jobs if there&amp;rsquo;s strict data delivery time SLO&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="challenges-solutions-learnings">Challenges, Solutions, Learnings&lt;/h3>
&lt;ul>
&lt;li>Physical Isolation and min/max capacity setting&lt;/li>
&lt;li>How to guarantee no impact to existing Spark jobs when scale-in&lt;/li>
&lt;li>How to speed up scale-out latency and always allow Spark driver getting start&lt;/li>
&lt;li>Monitoring autoscaling performance&lt;/li>
&lt;/ul>
&lt;h3 id="top-community-feature-requests">Top Community Feature Requests&lt;/h3>
&lt;ul>
&lt;li>Mixed instance type support&lt;/li>
&lt;li>Dynamic Allocation support&lt;/li>
&lt;li>Spot instance support with Remote Shuffle Service&lt;/li>
&lt;li>Predictive Autoscaling leveraging the platform&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="개인-생각">개인 생각&lt;/h3>
&lt;ul>
&lt;li>Apple 에서 k8s에 spark 를 돌리는 것에 대해서 나와있다.&lt;/li>
&lt;li>Apache YuniKorn 에 대해서 처음 알게 되었다. 좀더 정확히는 Spark 가 Kbuernetes 위에서 동작하는 구조 자체를 처음 공부했다.&lt;/li>
&lt;li>단순히 Spark Pod 이 많이 띄워져 있다. 이런 식으로 끝나는 것이 아니라 Layer 를 나눠서 설명해준 점이 좋았다.&lt;/li>
&lt;li>뭐 당연하게도 CRD 로 관리하고 있었다.&lt;/li>
&lt;li>대충 요약하자면, Spark 를 그냥 동작시키게 되면 skew 현상이 너무 심하고 이는 리소스가 남는 상황에서도 이를 잘 활용하지 못하는 것을 의미한다.&lt;/li>
&lt;li>따라서 중간에 Queue Layer를 두고 API/CLI/Airflow/Jupyter Notebook 등등은 Skate 에 요청한다.:
&lt;ul>
&lt;li>skate 는 처음 나오는 단어이고 특별한 프레임워크를 의미하는게 아니라 Spark service gateway를 줄여서 저렇게 부르는것 같다.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>여기서 YuniKorn Resource Queue 로 작업을 분배하게 된다. 이때 설정에 따라서 여러 cluster를 사용할 수 있는 것으로 보인다.&lt;/li>
&lt;li>특히나 인상 깊은건 Scale in/out Control에 따른 Memory, CPU Utilization Graph 인데 봐보면 너무나도 이상적으로 나왔다. 특정 노드가 엄청나게 뛰는게 아니라 전체적으로 스케줄링이 잘되는 것을 볼 수 있다. 물론 이 그래프만 가지고 판단하기에는 대조군도 없고, 그래프가 너무 컬러풀해서 잘 식별되지는 않지만 충분히 유의미한 그래프이며 이 구조가 어느정도의 안정성을 보임을 확인할수 있다.&lt;/li>
&lt;/ul></description></item><item><title>쿠버네티스 패턴</title><link>https://minuk.dev/wiki/kubernetes-patterns/</link><pubDate>Tue, 16 Aug 2022 10:56:05 +0900</pubDate><guid>https://minuk.dev/wiki/kubernetes-patterns/</guid><description>&lt;ul>
&lt;li>책 내용 정리 및 공식 문서와 비교하며 버전 확인&lt;/li>
&lt;/ul>
&lt;h3 id="1장-개요">1장 개요&lt;/h3>
&lt;h4 id="클라우드-네이티브로-가는길">클라우드 네이티브로 가는길&lt;/h4>
&lt;ul>
&lt;li>클린코드&lt;/li>
&lt;li>도메인 주도 설계&lt;/li>
&lt;li>마이크로서비스 아키텍처 방식&lt;/li>
&lt;li>컨테이너&lt;/li>
&lt;/ul>
&lt;h4 id="분산-기본-요소">분산 기본 요소&lt;/h4>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>개념&lt;/th>
 &lt;th>로컬 기본 요소&lt;/th>
 &lt;th>분산 기본 요소&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>캡슐화 동작&lt;/td>
 &lt;td>클래스&lt;/td>
 &lt;td>컨테이너 이미지&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>인스턴스화 동작&lt;/td>
 &lt;td>객체&lt;/td>
 &lt;td>컨테이너&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>재사용 단위&lt;/td>
 &lt;td>Jar 파일&lt;/td>
 &lt;td>컨테이너 이미지&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>컴포지션&lt;/td>
 &lt;td>포함 관계&lt;/td>
 &lt;td>사이드카 패턴&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>상속&lt;/td>
 &lt;td>확장 관계&lt;/td>
 &lt;td>FROM 으로 부모 이미지 상속&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>배포 단위&lt;/td>
 &lt;td>.jar/.war/.ear&lt;/td>
 &lt;td>pod&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>빌드타임/런타임 격리&lt;/td>
 &lt;td>모듈, 패키지, 클래스&lt;/td>
 &lt;td>namespace, pod, container&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>초기화 필요조건&lt;/td>
 &lt;td>Constructor&lt;/td>
 &lt;td>초기화 컨테이너&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>초기화 직 후 트리거&lt;/td>
 &lt;td>Init method&lt;/td>
 &lt;td>postStart&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>삭제 직전 트리거&lt;/td>
 &lt;td>Destroy method&lt;/td>
 &lt;td>preStop&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>정리 절차&lt;/td>
 &lt;td>finalize(), shutdown hook&lt;/td>
 &lt;td>Defer 컨테이너&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>비동기 &amp;amp; 병렬 칫행&lt;/td>
 &lt;td>ThreadPoolExecutor, ForkJoinPool&lt;/td>
 &lt;td>Job&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>주기적 작업&lt;/td>
 &lt;td>Timer, ScheduleExecutorService&lt;/td>
 &lt;td>CronJob&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>백그라운드 작업&lt;/td>
 &lt;td>Deamon Thread&lt;/td>
 &lt;td>DeamonSets&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>설정관리&lt;/td>
 &lt;td>System.getenv(), Properties&lt;/td>
 &lt;td>ConfigMap, Secret&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h4 id="컨테이너">컨테이너&lt;/h4>
&lt;ul>
&lt;li>컨테이너 이미지는 하나의 문제를 해결하는 기능 단위다.&lt;/li>
&lt;li>컨테이너 이미지는 하나의 팀에 의해 소유되며, 릴리즈 주기가 있다.&lt;/li>
&lt;li>컨테이너 이미지는 자기 완비적이며, 런타임 의존 성을 정의하고 수행한다.&lt;/li>
&lt;li>컨테이너 이미지는 불변적이며, 한번 만들어지면 변경되지 않는다. 즉 이미 설정 값이 정해져 있다.&lt;/li>
&lt;li>컨테이너 이미지는 런타임 의존성과 자원 요구사항이 정의되어 있다.&lt;/li>
&lt;li>컨테이너 이미지는 기능을 노출시키기 위해 잘 정의된 API가 있다.&lt;/li>
&lt;li>컨테이너는 일반적으로 하나의 유닉스 프로세스로 실행된다.&lt;/li>
&lt;li>컨테이너는 일회용이며 언제든지 스케일 업과 스케일 다운을 안전하게 수행할 수 있다.&lt;/li>
&lt;/ul>
&lt;h4 id="파드">파드&lt;/h4>
&lt;ul>
&lt;li>파드는 스케줄링의 최소 단위이다.&lt;/li>
&lt;li>파드는 파드에 속한 컨테이너들의 동일 장소 배치를 보장한다.&lt;/li>
&lt;li>한 파드는 파드 안의 모든 컨테이너가 공유하는 하나의 IP 주소와 이름, 포트 범위를 갖는다.&lt;/li>
&lt;/ul>
&lt;h4 id="서비스">서비스&lt;/h4>
&lt;ul>
&lt;li>서비스는 애플리케이션에 접근하기 위한 이름으로 된 진입점이다.&lt;/li>
&lt;/ul>
&lt;h4 id="레이블">레이블&lt;/h4>
&lt;ul>
&lt;li>레이블은 실행 중인 특정 파드의 인스턴스들을 가리키기 위해 사용된다.&lt;/li>
&lt;li>레이블은 스케줄러에서 많이 사용된다.&lt;/li>
&lt;li>레이블은 파드를 논리적 그룹으로 묶어 가리킬 수 있다.&lt;/li>
&lt;li>미리 앞서서 레이블을 추가하지 않아야한다. 레이블 삭제가 어떤 영향을 일으키는지 알아낼 방법이 없다.&lt;/li>
&lt;/ul>
&lt;h4 id="어노테이션">어노테이션&lt;/h4>
&lt;ul>
&lt;li>레이블과 유사한 기능을 하지만, 사람보다는 봇을 위한 용도로 사용된다.&lt;/li>
&lt;li>검색 불가능한 메타데이터를 지정하는데 사용한다.&lt;/li>
&lt;/ul>
&lt;h4 id="네임스페이스">네임스페이스&lt;/h4>
&lt;ul>
&lt;li>네임스페이스는 쿠버네티스 자원으로서 관리된다.&lt;/li>
&lt;li>네임스페이스는 컨테이너, 파드, 서비스, 레플리카세트 등의 자원에 대한 영역을 제공한다.&lt;/li>
&lt;li>네임스페이스 내에서 자원명은 고유해야한다.&lt;/li>
&lt;li>네임스페이스는 격리시키는 것이 아니므로 자원간 접근을 막을수는 없다.&lt;/li>
&lt;li>노드, PersistentVolume 등은 네임스페이스 내에 속하지 않는다.&lt;/li>
&lt;li>서비스는 &lt;code>&amp;lt;service-name&amp;gt;.&amp;lt;namespace-name&amp;gt;.svc.cluster.local&lt;/code> 형식의 dns address 를 갖는다.&lt;/li>
&lt;li>ResourceQuota는 네임스페이스 별로 제약조건을 걸 수 있다.&lt;/li>
&lt;/ul>
&lt;h2 id="1부-기본-패턴">1부 기본 패턴&lt;/h2>
&lt;h3 id="2장-예측-범위-내의-요구사항">2장 예측 범위 내의 요구사항&lt;/h3>
&lt;ul>
&lt;li>애플리케이션의 요구사항에 따라서 필요한 자원량은 달라지며, 이를 예측하는 것은 어려운 일이다.&lt;/li>
&lt;li>쿠버네티스를 사용하면서 런타임 요구사항을 알아야하는 이유:
&lt;ul>
&lt;li>효율적인 하드웨어 사용을 위한 배치&lt;/li>
&lt;li>전체 클러스터 설계 및 관리&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="런타임-의존성">런타임 의존성&lt;/h4>
&lt;ul>
&lt;li>PersistentVolume&lt;/li>
&lt;li>hostPort&lt;/li>
&lt;li>configMap, secret&lt;/li>
&lt;/ul>
&lt;h4 id="자원-프로파일">자원 프로파일&lt;/h4>
&lt;ul>
&lt;li>compressible resource : cpu, network&lt;/li>
&lt;li>incompressible resource : memory&lt;/li>
&lt;li>incompressible resource를 너무 많이 사용할 경우 컨테이너가 죽게 된다.&lt;/li>
&lt;li>requests, limits 에 따른 서비스 구분:
&lt;ul>
&lt;li>Best-Effort:
&lt;ul>
&lt;li>requests, limits 를 갖고 있지 않다.&lt;/li>
&lt;li>incompressible resource가 모자랄때, 가장 먼저 죽는다.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Burstable:
&lt;ul>
&lt;li>requests와 limits 가 다르다. (일반적으로 limits 가 requests 보다 크다.)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Guaranteed:
&lt;ul>
&lt;li>requests와 limts가 같다.&lt;/li>
&lt;li>가장 나중에 죽는다.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="파드-우선순위">파드 우선순위&lt;/h4>
&lt;ul>
&lt;li>책의 내용과 살짝 다르다. k8s v1.24 문서를 기준으로 작성되었다.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">scheduling.k8s.io/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">PrioirtyClass&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">high-priority&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">value&lt;/span>: &lt;span style="color:#ae81ff">1000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">globalDefault&lt;/span>: &lt;span style="color:#ae81ff">flase&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">description&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;This is a very high priority Pod class&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">random-generator&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">labels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">env&lt;/span>: &lt;span style="color:#ae81ff">random-generator&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">k8spatterns/random-generator:1.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">random-generator&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">priorityClassName&lt;/span>: &lt;span style="color:#ae81ff">high-prioirty&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="프로젝트-자원">프로젝트 자원&lt;/h4>
&lt;ul>
&lt;li>추가 참고자료 : &lt;a href="https://hakkyoonjung31.github.io/linux/memory-overcommit/">메모리 상승과 오버커밋&lt;/a>&lt;/li>
&lt;li>메모리 오버커밋 : 요구된 메모리를 그대로 할당하는 것이 아닌 실제 사용되는 시점에서 필요한 만큼의 메모리를 할당하는 방식에 의해 요구되는 메모리의 총량이 100%를 넘기는 경우&lt;/li>
&lt;li>오버 커밋 상태에서 실제 메모리 사용 총량이 메모리 총량을 넘기게 될 수도 있는데, 이때 OOM-Killer에 의해 프로세스들을 죽여서 용량을 확보하게 된다.&lt;/li>
&lt;li>개인 해석:
&lt;ul>
&lt;li>오버커밋에 의해 요청된 메모리와 사용하는 메모리는 차이가 날 수 있다.&lt;/li>
&lt;li>즉 requests는 250M를 하는데, pod에서 오버커밋을 이용해 500M를 할당하고, 사용은 200M를 하고 있는 상황같은게 발생 할 수 있다는 것이다.&lt;/li>
&lt;li>kubernetes 는 기본적으로 requests 를 기준으로 스케줄링한다.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="3장-선언적-배포">3장 선언적 배포&lt;/h3>
&lt;ul>
&lt;li>선언적 업데이트를 작동시키기 위한 옵션:
&lt;ul>
&lt;li>&lt;code>kubectl replace&lt;/code>로 새로운 버전의 deployment로 전체 deployment를 교체한다.&lt;/li>
&lt;li>deployment를 &lt;code>kubectl patch&lt;/code> 나 &lt;code>kubectl edit&lt;/code>으로 새로운 버전을 넣는다.&lt;/li>
&lt;li>&lt;code>kubectl set image&lt;/code> 을 통해서 deployment에 새로운 이미지를 넣는다.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>deployment 의 장점:
&lt;ul>
&lt;li>deployment는 상태가 내부적으로 관리되는 객체이므로 클라이언트와 상호작용 없이, 서버측에서 실행된다.&lt;/li>
&lt;li>deployment 의 선언적 특성은 배포에 필요한 단계보다는 배포된 상태가 어떻게 보여야하는지를 알 수 있다.&lt;/li>
&lt;li>deployment의 정의는 운영 환경에 배포되기 전에 다양한 환경에서 테스트된 실행 가능한 객체이다.&lt;/li>
&lt;li>업데이트 프로세스는 모두 기록되며, 일시 중지 및 계속을 위한 옵션, 이전 버전으로 롤백을 위한 옵션으로 버전이 지정된다.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="고정-배포">고정 배포&lt;/h4>
&lt;ul>
&lt;li>Recreate 전략:
&lt;ul>
&lt;li>우선적으로 현재 버전의 모든 컨테이너를 죽이고, 이전 버전의 컨테이너가 축출될때 모든 신규 컨테이너를 동시에 시작한다.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="블루-그린">블루-그린&lt;/h4>
&lt;ul>
&lt;li>블루(이전 버전), 그린(현재 버전)&lt;/li>
&lt;li>블루와 그린을 모두 띄운뒤 신규 트래픽을 그린으로 보낸뒤, 기존 트래픽을 다 처리하면 블루를 삭제한다.&lt;/li>
&lt;li>블루와 그린이 순간적으로 동시에 뜨게 된다.&lt;/li>
&lt;li>즉, 자원이 2배로 필요하다.&lt;/li>
&lt;/ul>
&lt;h4 id="카나리아">카나리아&lt;/h4>
&lt;ul>
&lt;li>소수의 인스턴스를 교체하면서 동작한다.&lt;/li>
&lt;/ul>
&lt;h3 id="4장-정상상태-점검">4장 정상상태 점검&lt;/h3>
&lt;ul>
&lt;li>프로세스 상태는 애플리케이션의 정상상태를 결정하기에는 충분하지 않다.&lt;/li>
&lt;/ul>
&lt;h4 id="liveness-probe">Liveness probe&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>HTTP : 200~399 사이 응답코드&lt;/p></description></item><item><title>Horizontal Pod AutoScaler</title><link>https://minuk.dev/wiki/horizontalpodautoscaler/</link><pubDate>Thu, 11 Aug 2022 13:36:02 +0900</pubDate><guid>https://minuk.dev/wiki/horizontalpodautoscaler/</guid><description>&lt;h2 id="공부하게된-이유">공부하게된 이유&lt;/h2>
&lt;ul>
&lt;li>면접때 autoscaling 에 대한 질문이 나왔는데 한번도 k8s 에서 auto scaling 을 해본적이 없었다.&lt;/li>
&lt;li>모든 auto scaling 에 대한 답이 horizontal pod autoscaling 인 건 아니긴 하지만, 면접에서 요구한 건 이 지식이였다.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/ko/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">공식 문서&lt;/a>를 따라하면서 한번 공부해보자.&lt;/li>
&lt;/ul>
&lt;h2 id="개념">개념&lt;/h2>
&lt;ul>
&lt;li>메트릭을 관찰해서 Deployment 의 scale을 변화시켜주는 구조.&lt;/li>
&lt;li>명령을 실행하는 시점에서 최대, 최소의 scale 을 입력해준다.&lt;/li>
&lt;li>추가적으로 메트릭을 임의로 정의해서 조절시킬수 있다.&lt;/li>
&lt;li>공식 문서에 나와있는 기본 예제는 CPU 사용량을 기준으로 하고 있으며, 다양한 resource 에 대해서 기본적으로 지원한다.&lt;/li>
&lt;/ul></description></item><item><title>kubernetes-graceful-shutdown</title><link>https://minuk.dev/wiki/kubernetes-graceful-shutdown/</link><pubDate>Thu, 04 Aug 2022 17:25:09 +0900</pubDate><guid>https://minuk.dev/wiki/kubernetes-graceful-shutdown/</guid><description>글을 보고 궁금한걸 코드로 정리한 문서</description></item></channel></rss>
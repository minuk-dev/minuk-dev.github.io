<!doctype html><html lang=ko-kr><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet href=https://unpkg.com/simpledotcss/simple.css><link rel=stylesheet href=/css/main.css><meta name=generator content="Hugo 0.140.0"><meta name=description content="minuk.dev wiki"><meta name=keywords content="hugo,site,new"><meta name=author content="Min-Uk.Lee"><title>Volcano - Intro & Deep Dive |
minuk dev wiki
</title><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></head><body><header class=header><div class=header_left><a href=/><img class=logo src=/images/Rb.png alt=logo>
MinUk.Dev</a></div><div class=header_middle>Volcano - Intro & Deep Dive</div></header><main><article class=main><div class=title><h1 class=title-header>Volcano - Intro & Deep Dive</h1></div><a href=https://github.com/minuk-dev/minuk-dev.github.io/blame/master/content/wiki/volcano-intro-and-deep-dive.md><h5>created : Tue, 30 Aug 2022 23:20:47 +0900</h5><h5>modified : Wed, 31 Aug 2022 00:24:46 +0900</h5></a><div class=article-meta><div class="breadcumb content"><i class="bi bi-folder"></i>
[[kubecon]]</div></div><div class=list-terms><ul><i class="bi bi-tags" title=Tags></i>
<a href=/tags/kubecon class=tag-btn>kubecon</a>
<a href=/tags/volcano class=tag-btn>volcano</a>
<a href=/tags/k8s class=tag-btn>k8s</a>
<a href=/tags/kubernetes class=tag-btn>kubernetes</a></ul></div><aside class=navbar id=nav-toc style=text-align:left><nav id=TableOfContents><ul><li><a href=#cloud-native-for-intelligent-workload>Cloud Native for Intelligent Workload</a></li><li><a href=#batch-on-k8s-challenges>Batch on K8s: Challenges</a></li><li><a href=#volcano-overview>Volcano Overview</a><ul><li><a href=#key-concept>Key Concept</a></li><li><a href=#job-mangement>Job mangement</a></li><li><a href=#resource-mangement--queue>Resource mangement- Queue</a></li><li><a href=#dynamic-resource-sharing-between-queues>Dynamic resource sharing between queues</a></li><li><a href=#fair-share-within-queue>Fair share within Queue</a></li></ul></li></ul></nav></aside><div class=content><ul><li><a href=https://youtu.be/a76CajRhsX0>원본링크</a></li></ul><h1 id=intro--deep-dive---volcano-a-cloud-native-batch-system>Intro & Deep dive - Volcano: A Cloud Native Batch System</h1><h2 id=cloud-native-for-intelligent-workload>Cloud Native for Intelligent Workload</h2><ul><li>More and more organization are leveraging cloud native technology to avoid fragmental ecosystem, isolated stack, low resource utilization</li></ul><h2 id=batch-on-k8s-challenges>Batch on K8s: Challenges</h2><ul><li>Job meanagement:<ul><li>Pod level scheduling, no awareness of upper-level applications.</li><li>Lack of fine-grained lifecycle management.</li><li>Lack of task dependencies, Job dependencies.</li></ul></li><li>Scheduling:<ul><li>Lack of job based scheduling, e.g. job ordering, job priority, job preemption, job fair-share, job reservation.</li><li>Not enough advanced scheduling algortihms, E.g. CPU topology, task-topology, IO-Awareness, backfill.</li></ul></li><li>Multi-framework support:<ul><li>Insufficient suport for mainstream computing frameowrks like MPI, Tensorflow, Mxnet, Pytorch.</li><li>Complex deployment and O&amp;M because each frameowrk corresponding to a different operator.</li></ul></li><li>Resource planning, sharing, heterogeneous computing:<ul><li>Lack of support to resource sharing mechanism between jobs, queues, namespaces.</li><li>Lack of Deeper support on heterogenous resources.</li></ul></li><li>Performance:<ul><li>Not enough throughput, roundtrip for batch workload.</li></ul></li></ul><h2 id=volcano-overview>Volcano Overview</h2><ul><li>Created at March 2019; Sandbox at April 2020; Incubator at April 2022</li><li>2.3k star, 360+ contributors, latest version v1.5.1</li><li>50+ enterprises adopt Volcano in production environments.</li></ul><h3 id=key-concept>Key Concept</h3><ul><li>Job:<ul><li>Multiple Pod Template</li><li>Lifecycle management/Erro handling</li></ul></li><li>User/namespace/resource quota:<ul><li>namespace is regarded as user</li><li>resource quota is regarded as the upper limit resource that users in the namespace are able to use at most. Like the QPS in Kube-apiserver.</li></ul></li><li>Resource share:<ul><li>Use Queue for resource sharing</li><li>Share resources between different &ldquo;tenants&rdquo; or resource pools.</li><li>Support different scheduling policies or algorithms for different &ldquo;tenants&rdquo; or resource pools.</li></ul></li></ul><h3 id=job-mangement>Job mangement</h3><ul><li>Volcano Job:<ul><li>Unified Job interface for most of batch job like mpi, pytorch, tensorflow, mxnet, etc.</li><li>Fine-grained Job Lifecycle mangement</li><li>Extendable job plugin:<ul><li>env, svc, ssh, tensorflow</li></ul></li><li>Coordinate with Scheduler</li><li>Job dependency</li></ul></li></ul><h3 id=resource-mangement--queue>Resource mangement- Queue</h3><ul><li>Queue is cluster scoped, decoupled with user/namespace</li><li>Queue is used to share resources between &ldquo;multi-tenants&rdquo; or resource pool.</li><li>Configure policy for each queue, e.g. FIFO, fair share, priority, SLA.</li></ul><h3 id=dynamic-resource-sharing-between-queues>Dynamic resource sharing between queues</h3><ul><li>Queue Guarantee/Capacity</li><li>Share resource between Queues proportionally by weight</li></ul><h4 id=개인-생각>개인 생각</h4><ul><li>?? 왜 Queue 들끼리 균등하게 분할할 생각이 아니라 Queue 끼리 자원을 서로 대여하는 구조인거지??</li><li>Queue 끼리 우선순위를 조정하는 Policy 도 있겠지?? 일단 Queue 내부 Policy 는 있는거 같은데 Queue 간 제어하는 로직이 확실치 않네</li></ul><h3 id=fair-share-within-queue>Fair share within Queue</h3><ul><li>Sharing resource between jobs</li><li>Sharing resource between namespaces</li><li>Per-Queue policy (FIFO, Priority, Fair share, &mldr;)</li></ul><h4 id=case--hierarchical-queue>Case : hierarchical queue</h4><ul><li>How to share resource in a multi-level org more easily?</li><li>Problem: flat queue cannot meet complex resource share and isolation easily for big org.</li><li>Solution:<ul><li>Multiple level queue constructs a tree which is mapped to the org.</li><li>Each level queue has min, max, weight. Use max to isolate resource, use queue weight to balance resource betweeen queues.</li><li>Share resources between queues and reclaim by weight</li></ul></li><li>Benefit:<ul><li>Flexible resource mangement, easy to map the organization</li><li>fine-grained control resource share and isolation for a big multi-tenants organization</li><li>The queue min capacity ensures guaranteed resource, the proportion by weight offers flexible sharing</li></ul></li></ul><h4 id=scenario-elastic-scheduling>Scenario: Elastic scheduling</h4><ul><li><p>What is elastic job</p><div class=codeblock><div class=copy-button-box><button class=copy-button state=copy data=#ZgotmplZ>
<i class="bi bi-copy"></i></button></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>batch.volcano.sh/v1alpha1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Job</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>test-job</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>minAvailable</span>: <span style=color:#ae81ff>5</span> <span style=color:#75715e>#min</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>tasks</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>10</span> <span style=color:#75715e>#max</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>image</span>: <span style=color:#ae81ff>train_script</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>nvidia.com/gpu</span>: <span style=color:#ae81ff>1</span></span></span></code></pre></div></div></li></ul><h4 id=scenario-sla-scheduling>Scenario: SLA scheduling</h4><ul><li>Big job get starving while co-existing with small job.</li><li>SLA scheduling allow to configure the job so that it is completed on time and reduce the risk on missed deadlines.</li><li>SLA support argument <code>sla-waiting-time</code> to realize job resource reservation: <code>sla-wating-time</code> is the maximum time that one job should stay in pending. When <code>sla-waiting-time</code> reached, SLA plugin move the pending job to next state. And start to reserve resources for this job until the job&rsquo;s request is satisfied.</li></ul><h4 id=개인-생각-1>개인 생각</h4><ul><li>아 역시 걱정하는 시나리오는 설명해주네. 일단 잘 동작한다라고 설명은 하는데, 솔직히 아직은 잘 모르겠다. 뭔가 깨끗하지가 않다.</li><li>시간적인 관점에서 일종의 threshold 로 제어하는 건데 이런 접근이 최선인가 싶다. utilization 에 초점이 지나치게 맞춰지지 않았나 싶다.</li></ul><h4 id=scenario-cpu-topology-awareness>Scenario: CPU Topology awareness</h4><ul><li>NumaAware:<ul><li>volcano watch CPU topology and schedule pods to the nodes wich NUMA topology.</li></ul></li><li>Senario:<ul><li>Scientific calcuation, video decoding &mldr; etc. big data offline processing and other specific scenes which are computation-intensive jobs that are sensitive to CPU parameters, scheduling delays.</li></ul></li></ul><h4 id=scenario-batch-scheduler-for-spark>Scenario: batch scheduler for Spark</h4><h4 id=개인-생각-2>개인 생각</h4><ul><li>Apache YuniKorn 도 있고 Spark 를 지원하는 도구들은 지금 꽤 나오고 있는데.. 잘 모르겠다.</li><li>일괄적인 일종의 표준으로서 동작하는 거에는 의미가 있긴 하지만, 먼가 너무 뭐든지 우리로 해.. 같은 느낌이 좀 있다.</li><li>결국 이런 프로젝트는 사용자들이 실제로 편하게 사용하거나 성능상의 엄청난 이득이 있어 de-facto standard 처럼 동작해야지 유의미하지 않나 싶다.</li><li>전체적으로 많은 Scenario 들을 나열하면서 설명하지만, 전부 &ldquo;~ 한 상황에서 ~을 지원하기 때문에 ~하게 사용할수 있다.&rdquo; 이런 느낌으로 설명하고 있는데, 왜 volcano 만이 가능한지, 왜 써야하는지, 도입하는데에 대한 노력과 같은 부분을 잘 모르겠다. 사실 k8s 가 도입된지가 어느 정도 지났기 때문에 대부분은 batch 에 대한 노하우가 있고, 당장 kubeflow 만 봐도 batch 에 대한 이야기가 넘처난다. 전체적으로 왜 써야만 하는지에 대해서는 이 강연에서는 잘 캐치하지 못했다.</li><li>오히려 이런 내용은 <a href=https://www.cncf.io/blog/2021/02/10/three-reasons-why-you-need-volcano/>CNCF: three-reasons-why-you-need-volcano</a> 가 더 설명을 잘하고 있다고 생각한다. 하지만 아직까진 scheduling 에 대해서 확 와닿지는 않긴한다.:<ul><li>Group Scheduling</li><li>Automatic Optimization of Resource Allocation</li><li>Support for a Range of Advanced Scheduling Scenarios</li></ul></li><li>뭔가 kubeflow나 kube-batch 가 안되는 부분을 비교해서 설명해주거나 performance 그래프를 비교하며 따라서 이걸 써야한다는 식으로 강연이 전개되면 좋았을텐데 싶다. 자원 관리 측면에서 장점이 있는거 같긴한데&mldr; 압도적인 성능을 보여주는게 아니라면 Production level 에서 이걸 도입하자고 목소리를 낼수 있나?</li><li>Community 가 중국 쪽에 크게 형성되어 있는 거 같은데, 아&mldr; 잘 모르겠다. 뭔가 의문만 남는 강연이다. Deep Dive 라는 제목을 달고서 구체적인 알고리즘이나 수치 없이 이게 맞나? 많은 case 를 들고오기보다는 그냥 1~2 개의 케이스에서 성능과 알고리즘 분석을 해주는게 훨씬 좋았을텐데</li><li>그냥 전체적으로 시간 날린거 같은 기분이다. <a href=https://github.com/kubernetes-sigs/kube-batch>kube-batch</a> 에 리스트에 있는거 봐서는 그래도 어느정도 쓰는 프레임워크 같은데 강연 듣고 남은게 없다.</li><li>뭔가 자료를 찾아보려고 해도 공식 사이트에서 중국어 자료가 나와버리면, 뭐하라는걸까? 일단 대충 batch 작업을 기존에는 scheduling 하는데에 어려움이 있었는데 이를 위한 도구이다 정도만 기억하고 넘겨야겠다. CNCF incubating project 니까 나중에 mainstream 이 되면 알게되겠지</li></ul></div><hr><div class=list-files><ul class=section-tree></ul></div></article><script src=/js/wikilink.js></script><div><script src=https://utteranc.es/client.js repo=minuk-dev/minuk-dev.github.io issue-term=pathname theme=github-dark crossorigin=anonymous async></script></div></main><footer class=footer><div class=footer-left></div><div class=footer-right><ul class=social><li><a href=/about>About</a></li><li><a href=https://github.com/minuk-dev>Github</a></li></ul></div></footer></body><script src=/js/dir_toggle.js></script><script src=/js/codeblock_copy.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.1/font/bootstrap-icons.css><script type=module>
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs";
mermaid.initialize({
  startOnLoad: true,
  theme: "dark",
});
</script></html>
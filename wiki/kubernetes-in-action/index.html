<!doctype html><html lang=ko-kr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Kubernetes in action</title><style>html body{font-family:raleway,sans-serif;background-color:#fff}:root{--accent:#00a3d2;--border-width:5px}</style><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Raleway"><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css integrity=sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u crossorigin=anonymous><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css integrity=sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN crossorigin=anonymous><link rel=stylesheet href=/css/main.css><link rel=stylesheet href=/css/copy-btn.css><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script src=/js/copy-btn.js></script><script src=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js integrity=sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa crossorigin=anonymous></script><script>$(document).on("click",function(){$(".collapse").collapse("hide")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-98056974-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","UA-98056974-1")</script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><meta name=google-site-verification content="g_3tJyj-KkW-_wKx7Ij5GimHV1nKPZXetCz8ydbBAfA"></head><body><nav class="navbar navbar-default navbar-fixed-top"><div class=container><div class=navbar-header><a class="navbar-brand visible-xs" href=#>Kubernetes in action
</a><button class=navbar-toggle data-target=.navbar-collapse data-toggle=collapse>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button></div><div class="collapse navbar-collapse"><ul class="nav navbar-nav"><li><a href=/>Home</a></li><li><a href=/wiki/>Wiki</a></li><li><a href=/posts/>Posts</a></li><li><a href=/about/>About</a></li></ul></div></div></nav><main><div class=navigator style=display:flex><div style=display:flex><div class=parent-doc style=flex:none><button class="btn btn-link" onclick='(function(e){e.querySelector("a").click()})(this)'>
<i class="fa fa-arrow-left"></i>
[[Book reviews]]</button></div></div></div><div><h2>Kubernetes in action</h2><a href=https://github.com/minuk-dev/minuk-dev.github.io/blame/master/content/wiki/kubernetes-in-action.md><h5>created : Mon, 31 Jan 2022 04:38:12 +0900</h5><h5>modified : Sun, 12 Mar 2023 21:16:57 +0900</h5></a><a href=https://minuk.dev/tags/k8s><kbd class=item-tag>k8s</kbd></a></div><aside class=navbar id=nav-toc style=text-align:left><nav id=TableOfContents><ul><li><a href=#chapter-1-introducing-kubernetes>Chapter 1. Introducing Kubernetes</a><ul><li><a href=#changes-of-software-development-and-deployments>Changes of software development and deployments.</a></li><li><a href=#11-understanding-the-need-for-a-system-like-kubernetes>1.1. Understanding the need for a system like Kubernetes</a></li><li><a href=#12-introducing-container-technologies>1.2. Introducing container technologies</a></li><li><a href=#13-introducing-kubernetes>1.3. Introducing Kubernetes</a></li><li><a href=#14-summary>1.4. Summary</a></li></ul></li><li><a href=#chapter-2-first-steps-with-docker-and-kubernetes>Chapter 2. First steps with Docker and Kubernetes</a><ul><li><a href=#21-creating-running-and-sharing-a-container-image>2.1. Creating, running, and sharing a container image</a></li><li><a href=#22-setting-up-a-kubernetes-cluster>2.2. Setting up a Kubernetes cluster</a></li><li><a href=#23-running-your-first-app-on-kubernetes>2.3. Running your first app on Kubernetes</a></li><li><a href=#24-summary>2.4. Summary</a></li></ul></li></ul><ul><li><a href=#chapter-3-pods-running-containers-in-kubernetes>Chapter 3. Pods: running containers in Kubernetes</a><ul><li><a href=#31-intorducing-pods>3.1. Intorducing pods</a></li><li><a href=#32-creating-pods-from-yaml-or-json-descriptors>3.2. Creating pods from YAML or JSON descriptors</a></li><li><a href=#33-organizing-pods-with-labels>3.3. Organizing pods with labels</a></li><li><a href=#34-listing-subsets-of-pods-through-label-selectors>3.4. Listing subsets of pods through label selectors</a></li><li><a href=#35-using-labels-and-selectors-to-constrain-pod-scheduling>3.5. Using labels and selectors to constrain pod scheduling</a></li><li><a href=#36-annotating-pods>3.6. Annotating pods</a></li><li><a href=#37-using-namespaces-to-group-resources>3.7. Using namespaces to group resources</a></li><li><a href=#38-stopping-and-removing-pods>3.8. Stopping and removing pods</a></li></ul></li><li><a href=#chapter-4-replication-and-other-controllers-deploying-managed-pods>Chapter 4. Replication and other controllers: deploying managed pods</a><ul><li><a href=#41-keeping-pods-healthy>4.1. Keeping pods healthy</a></li><li><a href=#42-introducing-replicationcontrollers>4.2. Introducing ReplicationControllers</a></li><li><a href=#43-using-replicasets-instead-of-replicationcontrollers>4.3. Using ReplicaSets instead of ReplicationControllers</a></li><li><a href=#44-running-exactly-one-pod-on-each-node-with-daemonsets>4.4. Running exactly one pod on each node with DaemonSets</a></li><li><a href=#45-running-pods-that-perform-a-single-completable-task>4.5. Running pods that perform a single completable task</a></li><li><a href=#46-scheduling-jobs-to-run-periodically-or-once-in-the-future>4.6. Scheduling Jobs to run periodically or once in the future</a></li><li><a href=#47-summary>4.7. Summary</a></li></ul></li><li><a href=#chapter-5-services-enabling-clients-to-discover-and-talk-to-pods>Chapter 5. Services: enabling clients to discover and talk to pods</a><ul><li><a href=#51-intoducing-services>5.1. Intoducing services</a></li><li><a href=#52-connecting-to-services-living-outside-the-cluster>5.2. Connecting to services living outside the cluster</a></li><li><a href=#53-expsing-services-to-external-clients>5.3. Expsing services to external clients</a></li><li><a href=#54-exposing-services-externally-through-an-ingress-resource>5.4. Exposing services externally through an Ingress resource</a></li><li><a href=#55-signaling-when-a-pod-is-ready-to-accept-connections>5.5. Signaling when a pod is ready to accept connections</a></li><li><a href=#56-using-a-headless-service-for-discovering-individual-pods>5.6. Using a headless service for discovering individual pods</a></li><li><a href=#57-troubleshooting-services>5.7. Troubleshooting services</a></li><li><a href=#58-summary>5.8. Summary</a></li></ul></li><li><a href=#chapter6-volumes-attaching-disk-storage-to-containers>Chapter6. Volumes: attaching disk storage to containers</a><ul><li><a href=#61-introducing-volumes>6.1. Introducing volumes</a></li><li><a href=#62-using-volumes-to-share-data-between-containers>6.2. Using volumes to share data between containers</a></li><li><a href=#63-accessing-files-on-the-worker-nodes-filesystem>6.3. Accessing files on the worker node&rsquo;s filesystem</a></li><li><a href=#64-using-persistent-storage>6.4. Using persistent storage</a></li><li><a href=#65-decoupling-pods-from-the-underlying-storage-technology>6.5. Decoupling pods from the underlying storage technology</a></li><li><a href=#66-dynamic-provisioning-of-persistentvolumes>6.6. Dynamic provisioning of PersistentVolumes</a></li><li><a href=#67-summary>6.7. Summary</a></li></ul></li><li><a href=#chapter-7-configmaps-and-secrets-configuringg-applications>Chapter 7. ConfigMaps and Secrets: configuringg applications</a><ul><li><a href=#71-configuring-containerized-applications>7.1. Configuring containerized applications</a></li><li><a href=#72-passing-command-line-arguments-to-containers>7.2. Passing command-line arguments to containers</a></li><li><a href=#73-setting-environment-variables-for-a-container>7.3. Setting environment variables for a container</a></li><li><a href=#74-decoupling-configuration-with-a-configmap>7.4. Decoupling configuration with a ConfigMap</a></li><li><a href=#75-using-secrets-to-pass-sensitive-data-to-containers>7.5. Using Secrets to pass sensitive data to containers</a></li><li><a href=#76-summary>7.6. Summary</a></li></ul></li><li><a href=#chapter-8-accessing-pod-metadata-and-other-resources-from-applications>Chapter 8. Accessing pod metadata and other resources from applications</a><ul><li><a href=#81-passing-metadata-through-the-downward-api>8.1. Passing metadata through the Downward API</a></li><li><a href=#82-talking-to-the-kubernetes-api-server>8.2. Talking to the Kubernetes API server</a></li></ul></li><li><a href=#chapter-9-deployments-updating-applications-declaratively>Chapter 9. Deployments: updating applications declaratively</a><ul><li><a href=#91-updating-applications-running-in-pods>9.1. Updating applications running in pods</a></li><li><a href=#92-performing-an-automatic-rolling-update-with-a-replicationcontroller>9.2. Performing an automatic rolling update with a ReplicationController</a></li><li><a href=#93-using-deployments-for-updating-apps-declaratively>9.3. Using Deployments for updating apps declaratively</a></li><li><a href=#94-summary>9.4. Summary</a></li></ul></li><li><a href=#chatper-10-statefulsets-deploying-replicated-statful-applications>Chatper 10. StatefulSets: deploying replicated statful applications</a><ul><li><a href=#106-summary>10.6. Summary</a></li></ul></li></ul><ul><li><a href=#chapter-11-understanding-kubernetes-internals>Chapter 11. Understanding Kubernetes internals</a><ul><li><a href=#111-understanding-the-architecture>11.1 Understanding the architecture</a></li><li><a href=#112-how-controllers-cooperate>11.2. How controllers cooperate</a></li><li><a href=#113-understanding-what-a-running-pod-is>11.3. Understanding what a running pod is</a></li><li><a href=#114-inter-pod-networking>11.4. Inter-pod networking</a></li><li><a href=#115-how-services-are-implemented>11.5. How services are implemented</a></li><li><a href=#116-running-highly-avaiable-clusters>11.6. Running highly avaiable clusters</a></li></ul></li><li><a href=#chapter-12-securing-the-kubernetes-api-server>Chapter 12. Securing the Kubernetes API server</a><ul><li><a href=#121-understanding-authentication>12.1. Understanding authentication</a></li><li><a href=#122-securing-the-cluster-with-role-based-access-control>12.2. Securing the cluster with role-based access control</a></li><li><a href=#123-summary>12.3. Summary</a></li></ul></li><li><a href=#chatper-13-securing-cluster-nodes-and-the-network>Chatper 13. Securing cluster nodes and the network</a><ul><li><a href=#131-using-the-host-nodes-namespaces-in-a-pod>13.1. Using the host node&rsquo;s namespaces in a pod</a></li><li><a href=#132-configuring-the-containers-security-context>13.2. Configuring the container&rsquo;s security context</a></li><li><a href=#133-restricting-the-use-of-security-related-features-in-pods>13.3. Restricting the use of security-related features in pods</a></li><li><a href=#134-isolation-the-pod-network>13.4. Isolation the pod network</a></li><li><a href=#135-summary>13.5. Summary</a></li></ul></li><li><a href=#chapter-14-managing-pods-computational-resources>Chapter 14. Managing pods&rsquo; computational resources</a><ul><li><a href=#141-requesting-resources-for-a-pods-containers>14.1. Requesting resources for a pod&rsquo;s containers</a></li><li><a href=#142-limiting-resources-available-to-a-container>14.2. Limiting resources available to a container</a></li><li><a href=#143-understanding-pod-qos-classes>14.3. Understanding pod QoS classes</a></li><li><a href=#144-setting-default-requests-and-limits-for-pods-per-namespace>14.4. Setting default requests and limits for pods per namespace</a></li><li><a href=#145-limiting-the-total-resources-available-in-a-namespace>14.5. Limiting the total resources available in a namespace</a></li><li><a href=#146-monitoring-pod-resource-usage>14.6. Monitoring pod resource usage</a></li></ul></li><li><a href=#chatper-15-automatic-scaling-of-pods-and-cluseter-nodes>Chatper 15. Automatic scaling of pods and cluseter nodes</a><ul><li><a href=#151-horizontal-pod-autoscaling>15.1. Horizontal pod autoscaling</a></li><li><a href=#152-vertical-pod-autoscaling>15.2. Vertical pod autoscaling</a></li><li><a href=#153-horizontal-scaling-of-cluster-nodes>15.3. Horizontal scaling of cluster nodes</a></li><li><a href=#154-summary>15.4. Summary</a></li></ul></li><li><a href=#chapter-16-advanced-schdeuling>Chapter 16. Advanced schdeuling</a><ul><li><a href=#161-using-taints-and-tolerations-to-repel-pods-from-certain-nodes>16.1. Using taints and tolerations to repel pods from certain nodes</a></li><li><a href=#162-using-node-affinity-to-attract-pods-to-certain-nodes>16.2. Using node affinity to attract pods to certain nodes</a></li><li><a href=#163-co-locating-pods-with-pod-affinity-and-anti-affinity>16.3. Co-locating pods with pod affinity and anti-affinity</a></li><li><a href=#164-summary>16.4. Summary</a></li></ul></li><li><a href=#chapter-17-best-practices-for-developing-apss>Chapter 17. Best practices for developing apss</a><ul><li><a href=#171-bringing-everything-together>17.1. Bringing everything together</a></li><li><a href=#172-understanding-the-pods-lifecycle>17.2. Understanding the pod&rsquo;s lifecycle</a></li><li><a href=#173-ensuring-all-client-requests-are-handled-properly>17.3. Ensuring all client requests are handled properly</a></li><li><a href=#174-making-your-apps-easy-to-run-and-manage-in-kubernetes>17.4. Making your apps easy to run and manage in Kubernetes</a></li><li><a href=#175-best-practices-for-development-and-testing>17.5. Best practices for development and testing</a></li><li><a href=#176-summary>17.6. Summary</a></li></ul></li></ul></nav></aside><div align=start class=content data-spy=scroll data-offset=20 data-target=#nav-toc style=position:relative><h1 id=part-1-overview>Part 1. Overview</h1><h2 id=chapter-1-introducing-kubernetes>Chapter 1. Introducing Kubernetes</h2><h3 id=changes-of-software-development-and-deployments>Changes of software development and deployments.</h3><ul><li>Years ago, most software applications were big monoliths<ul><li>Slow release cycles</li><li>Relatively infrequent updates</li></ul></li><li>Today, these applications are broken down into smaller. : microservices<ul><li>Microservies are decoupled from each other -> Easily developed, deployed, updated, and scaled individually</li></ul></li><li>Kubernetes helps the ops team by automaticaly monitoring and rescheduling those apps in the event of a hardware failure.</li></ul><h3 id=11-understanding-the-need-for-a-system-like-kubernetes>1.1. Understanding the need for a system like Kubernetes</h3><h4 id=111-moving-from-monolithic-apps-to-microservices>1.1.1. Moving from monolithic apps to microservices</h4><ul><li>Microservies communicate through synchronous protocols such as HTTP, over which they usually expose RESTful (REpresentional State Transfer) APIs, or through asynchronous protocols such as AMQP (Advanced Message Queueing Protocol)</li><li>Scaling microservices</li><li>Deploying microservices</li><li>Understanding the divergence of environment requirements</li></ul><h4 id=112-providing-a-consistent-environment-to-applications>1.1.2. Providing a consistent environment to applications</h4><h4 id=113-moving-to-continuous-delivery-devops-and-noops>1.1.3. Moving to continuous delivery: DevOps and NoOps</h4><ul><li>Understanding the benefits</li><li>Letting developers and sysadmins do what they do best</li></ul><h3 id=12-introducing-container-technologies>1.2. Introducing container technologies</h3><h4 id=121-understanding-what-contianers-are>1.2.1. Understanding what contianers are</h4><ul><li>Isolating components with Linux container technologies</li><li>Comparing virtual machines to containers</li><li>Introducing the mechanisms that make container isolation possible</li><li>Isolating processes with Linux Namespaces</li><li>Limiting resources available to a process</li></ul><h4 id=122-intoducing-the-docker-container-platform>1.2.2. Intoducing the Docker container platform</h4><ul><li>Understanding Docker concepts<ul><li>Images</li><li>Regieries</li><li>Containers</li></ul></li><li>Building, distributing, and running a Docker image</li><li>Comparing virtual machines and Docker containers</li><li>Understanding image layers</li><li>Understanding the portability limitations of container images</li></ul><h4 id=123-introducing-rkt---an-alternative-to-docker>1.2.3. Introducing rkt - an alternative to Docker</h4><ul><li>rkt (pronounced &ldquo;rock-it&rdquo;)</li><li>benefits:<ul><li>security</li><li>composability</li><li>conforming to open standards</li></ul></li></ul><h3 id=13-introducing-kubernetes>1.3. Introducing Kubernetes</h3><h4 id=131-understanding-its-origins>1.3.1. Understanding its origins</h4><ul><li>Through the years, Google developed an internal system called Borg</li></ul><h4 id=132-looking-at-kubernetes-from-the-top-of-a-mountain>1.3.2. Looking at Kubernetes from the top of a mountain</h4><ul><li>Understanding the core of what Kubernetes does</li><li>Helping deveopers focus on the core app features</li><li>Helping ops teams achieve better reousrce utilization</li></ul><h4 id=133-understanding-the-architecture-of-a-kubernetes-cluster>1.3.3. Understanding the architecture of a Kubernetes cluster</h4><ul><li><p>The components that make up a Kubernetes cluster</p><ul><li>The master node, which hosts the Kubernetes Control Plane that controls and manages the whole Kubernetes system</li><li>Worker nodes that run the actual applications you deploy</li></ul></li><li><p>The Control Plane</p><ul><li>The Kubernetes API Server, which you and the other Control Plan components communicate with</li><li>The Schduler, which schedules your apps (assigns a worker node to each deployable component of your application)</li><li>The Controller Manager, which performs cluster-level functions, such as replicating components, keeping track of worker nodes, handlign node failures, and so on</li><li>etcd, a reliable distributed data store that persistently stores the cluster configuration.</li></ul></li><li><p>The nodes</p><ul><li>Docker, rkt, or another container runtime, which runs your containers</li><li>The Kubelet, which talks to the API server and manages containers on its node.</li><li>The Kubernets Service Proxy (kube-proxy), which load-balances network traffic between application components</li></ul></li></ul><h4 id=134-running-an-application-in-kubernetes>1.3.4. Running an application in Kubernetes</h4><ul><li>Understanding how the description results in a running container</li><li>Keeping the containers running</li><li>Scaling the number of copies</li><li>Hitting a moving target</li></ul><h4 id=135-understanding-the-benefits-of-using-kubernetes>1.3.5. Understanding the benefits of using Kubernetes</h4><ul><li>Simplifying application deployment</li><li>Achieving better unilization of hardware</li><li>Health checking and self-healing</li><li>Automatic scaling</li><li>Simplifying application development</li></ul><h3 id=14-summary>1.4. Summary</h3><ul><li>Monolithic apps are easier to deploy, but harder to maintain over time and sometimes impossible to scale.</li><li>Microservices-based application architectures allow easier development of each component, but are harder to deploy and configure to work as a single system.</li><li>Linux containers provide much the same benefits as vitual machines, but are far more lightweight and allow for much better hardware utilization.</li><li>Docker improved on existing Linux container technologies by allowing easier and faster provisioning of containerized apps together with their OS environments.</li><li>Kubernetes exposes the whole datacenter as single computational reousrce for running applications.</li><li>Developers can deploy apps through Kubernetes without assistance from sysadmins.</li><li>Sysadmins can sleep better by having Kubernetes deal with failed nodes automatically.</li></ul><h2 id=chapter-2-first-steps-with-docker-and-kubernetes>Chapter 2. First steps with Docker and Kubernetes</h2><h3 id=21-creating-running-and-sharing-a-container-image>2.1. Creating, running, and sharing a container image</h3><ul><li>docker run</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run &lt;image&gt;:&lt;tag&gt;
</span></span></code></pre></div><ul><li>dockerfile</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#66d9ef>FROM</span><span style=color:#e6db74> node:7</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>ADD</span> app.js /app.js<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>ENTRYPOINT</span> [<span style=color:#e6db74>&#34;node&#34;</span>, <span style=color:#e6db74>&#34;app.js&#34;</span>]<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div><ul><li>listing runnign containers</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker ps
</span></span></code></pre></div><ul><li>exploring the inside of a running container</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker exec -it &lt;container-name&gt; &lt;command&gt;
</span></span></code></pre></div><ul><li><p><code>-i</code>, which makes sure STDIN is kept open. You need this for entering commands into the shell.</p></li><li><p><code>-t</code>, which allocates a pseudo terminal (TTY).</p></li><li><p>Getting additional information about a container</p></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker inspect &lt;container-name&gt;
</span></span></code></pre></div><ul><li>The container&rsquo;s filesystem is also isolated<ul><li>Tip: Entering a running container like this is useful when debugging an app running in acontainer. When something&rsquo;s wrong, the first thing you&rsquo;ll want to explore is the actual state of the system your application sees. Keep in mind that an application will not only see its own unique filesystem, but also processes, users, hostname, and network interfaces.</li></ul></li></ul><h4 id=217-stopping-and-removing-a-container>2.1.7. Stopping and removing a container</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker rm &lt;conatinaer-name&gt;
</span></span></code></pre></div><h4 id=218-pushing-the-image-to-an-image-registry>2.1.8. Pushing the image to an image registry</h4><ul><li>Tagging an image under an additional tag</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker tag &lt;image&gt; &lt;docker-hub-id&gt;/&lt;name&gt;
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker images | head
</span></span></code></pre></div><ul><li>Pushing the image to Docker Hub</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker push &lt;docker-hub-id&gt;/&lt;name&gt;
</span></span></code></pre></div><ul><li>Running the image on a different machine</li></ul><h3 id=22-setting-up-a-kubernetes-cluster>2.2. Setting up a Kubernetes cluster</h3><h4 id=221-running-a-local-single-node-kubernetes-cluster-with-minikube>2.2.1. Running a local single-node Kubernetes cluster with Minikube</h4><h3 id=23-running-your-first-app-on-kubernetes>2.3. Running your first app on Kubernetes</h3><h4 id=231-deploying-your-app>2.3.1 Deploying your app</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl run kubia --image<span style=color:#f92672>=</span>&lt;image-name&gt; --port<span style=color:#f92672>=</span>&lt;port&gt;
</span></span></code></pre></div><ul><li>Listing pods</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><h4 id=232-accessing-your-web-application>2.3.2. Accessing your web application</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl expose rc &lt;pod-name&gt; --type<span style=color:#f92672>=</span>LoadBalancer --name &lt;service-name&gt;
</span></span></code></pre></div><ul><li>Listing service</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get services
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get svc
</span></span></code></pre></div><h4 id=233-the-logical-parts-of-your-system>2.3.3. The logical parts of your system</h4><ul><li>Understanding how the ReplicationController, the Pod, and the Service fit together</li><li>Understanding the pod and its container</li><li>Understanding the role of the ReplicationController</li><li>Understanding why you need a service</li></ul><h4 id=234-horizontally-scaling-the-application>2.3.4. Horizontally scaling the application</h4><ul><li>Increasing the desired replica count</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl scale rc &lt;pod-name&gt; --replicas<span style=color:#f92672>=</span>&lt;number&gt;
</span></span></code></pre></div><ul><li>Seeing the results of the scale-out</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get rc
</span></span></code></pre></div><h4 id=235-examining-what-nodes-your-app-is-running-on>2.3.5. Examining what nodes your app is running on</h4><h4 id=236-introducing-the-kubernetes-dashboard>2.3.6. Introducing the Kubernetes dashboard</h4><h3 id=24-summary>2.4. Summary</h3><h1 id=part-2-core-concepts>Part 2. Core concepts</h1><h2 id=chapter-3-pods-running-containers-in-kubernetes>Chapter 3. Pods: running containers in Kubernetes</h2><h3 id=31-intorducing-pods>3.1. Intorducing pods</h3><h4 id=311-understanding-why-we-need-pods>3.1.1. Understanding why we need pods</h4><ul><li>Understanding why multiple containers are better than one container running multiple processes</li></ul><h4 id=312-understanding-pods>3.1.2. Understanding pods</h4><ul><li>Understanding the partial ioslation between containers of the same pod</li><li>Understanding how containers share the same IP and port space</li><li>Introducing the flat inter-pod network</li></ul><h4 id=313-organization-containers-across-pods-properly>3.1.3. Organization containers across pods properly</h4><ul><li>Splitting multi-tier apps into multiple pods</li><li>Splitting into multiple pods to enable individual scaling</li><li>Understanding when to use multiple containers in a pod</li><li>Deciding when to use multiple containers in a pod</li></ul><h3 id=32-creating-pods-from-yaml-or-json-descriptors>3.2. Creating pods from YAML or JSON descriptors</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get po &lt;podname&gt; -o yaml
</span></span></code></pre></div><ol><li>Kubernetes API version used in this YAML descriptor</li><li>Type of Kubernetes object/resource</li><li>Pod metadata(name, labels, annotations, and so on)</li><li>Pod specification/contents(list of pod&rsquo;s containers, volumes, and so on)</li><li>Detailed status of the pod and its containers</li></ol><ul><li>Inducing the main parts of a pod definition:<ul><li>Metadata includes the name, namespace, labels, and other information about the pod.</li><li>Spec contains the actual description of the pod&rsquo;s contents, such as the pod&rsquo;s containers, volumes, and other data.</li><li>Status contains the current information about the runnign pod, such as what condition the pod is in, the description and status of each container, and the pod&rsquo;s internal IP and other basic info.</li></ul></li></ul><h4 id=322-creating-a--simple-yaml-descriptor-for-a-pod>3.2.2. Creating a simple YAML descriptor for a pod</h4><ul><li>Specifying container ports</li><li>Using kubectl explain to discover possible API object fields</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl explain pods
</span></span></code></pre></div><h4 id=323-using-kubectl-create-to-create-the-pod>3.2.3. Using kubectl create to create the pod</h4><h4 id=324-viewing-application-logs>3.2.4. Viewing application logs</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl logs &lt;container id&gt;
</span></span></code></pre></div><ul><li>Retrieving a pod&rsquo;s log with kubectl logs</li><li>Specifying the container name when getting logs of a multi-container pod</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl logs &lt;container id&gt; -c &lt;container-name&gt;
</span></span></code></pre></div><h4 id=325-sending-requests-to-the-pod>3.2.5. Sending requests to the pod</h4><ul><li>Forwarding a local network port to a port in the pod</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl port-forward &lt;container-id&gt; 8888:8080
</span></span></code></pre></div><ul><li>Connecting to the pod through the port forwarder</li></ul><h3 id=33-organizing-pods-with-labels>3.3. Organizing pods with labels</h3><h4 id=331-introducing-labels>3.3.1. Introducing labels</h4><ul><li><p>Each pod is labeled with two labels:</p><ul><li>app, which specifies which app, component, or microservice the pod belongs to.</li><li>rel, which shows whether the application running in the pod is a stable, beta, or a canary release</li></ul></li><li><p>Definition : A canary relase is when you deploy a new version of an application next to the stable version, and only let a small fraction of users hit the new version to see how it behaves before rolling it out to all users. This prevents bad releases from being exposed to too many users.</p></li></ul><h4 id=332-specifying-labels-when-creating-a-pod>3.3.2. Specifying labels when creating a pod</h4><h4 id=333-modifying-labels-of-existing-pods>3.3.3. Modifying labels of existing pods</h4><ul><li>Show labels pod</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get po --show-labels
</span></span><span style=display:flex><span>kubectl get po -L creation_method,env
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl label po &lt;pod-name&gt; creation_method<span style=color:#f92672>=</span>manual
</span></span></code></pre></div><h3 id=34-listing-subsets-of-pods-through-label-selectors>3.4. Listing subsets of pods through label selectors</h3><ul><li>A label selector can select resources based on whether the resource:<ul><li>Contains (or doesn&rsquo;t contain) a label with a certain key</li><li>Contains a label with a certain key and value</li><li>Contains a label with a certain key, but with a value not equal to the one you specify</li></ul></li></ul><h4 id=341-listing-pods-using-a-label-selector>3.4.1. Listing pods using a label selector</h4><h4 id=342-using-multiple-conditions-in-a-label-selector>3.4.2. Using multiple conditions in a label selector</h4><h3 id=35-using-labels-and-selectors-to-constrain-pod-scheduling>3.5. Using labels and selectors to constrain pod scheduling</h3><h4 id=351-using-labels-for-categorizing-worker-nodes>3.5.1. Using labels for categorizing worker nodes</h4><h4 id=352-scheduling-pods-to-specific-nodes>3.5.2. Scheduling pods to specific nodes</h4><h4 id=353-scheduling-to-one-specific-node>3.5.3. Scheduling to one specific node</h4><h3 id=36-annotating-pods>3.6. Annotating pods</h3><h4 id=361-looking-up-an-objects-annotations>3.6.1. Looking up an object&rsquo;s annotations</h4><h4 id=362-adding-and-modifying-annotations>3.6.2. Adding and modifying annotations</h4><h3 id=37-using-namespaces-to-group-resources>3.7. Using namespaces to group resources</h3><h4 id=371-understanding-the-need-for-namespaces>3.7.1. Understanding the need for namespaces</h4><h4 id=372-discovering-other-namespaces-and-their-pods>3.7.2. Discovering other namespaces and their pods</h4><h4 id=373-creating-a-namespace>3.7.3. Creating a namespace</h4><h4 id=374-managing-objects-in-other-namespaces>3.7.4. Managing objects in other namespaces</h4><h4 id=375-understanding-the-isolation-provided-by-namespaces>3.7.5. Understanding the isolation provided by namespaces</h4><h3 id=38-stopping-and-removing-pods>3.8. Stopping and removing pods</h3><h4 id=381-deleting-a-pod-by-name>3.8.1. Deleting a pod by name</h4><h4 id=382-deleting-pods-using-label-selectors>3.8.2. Deleting pods using label selectors</h4><h2 id=chapter-4-replication-and-other-controllers-deploying-managed-pods>Chapter 4. Replication and other controllers: deploying managed pods</h2><h3 id=41-keeping-pods-healthy>4.1. Keeping pods healthy</h3><h4 id=411-introducing-liveness-probes>4.1.1. Introducing liveness probes</h4><ul><li>Kubernetes can probe a container using one of the three mechanisms:<ul><li>An HTTP GET probe performs an HTTP GET request on the container&rsquo;s IP address, a port and path you specify. If the probe receives a response, and the response code doesn&rsquo;t represent an error(in other words, if the HTTP response code is 2xx or 3xx), the probe is considered successful. If the server returns an error response code or if it doesn&rsquo;t respond at all, the probe is considered a failure and the container will be restarted as a result.</li><li>A TCP Socket probe tries to open a TCP connection to the specified port of the container. If the connection is established successfully, the probe is successful. Otherwise, the container is restarted.</li><li>An Exec probe excutes an arbitrary command inside the container and checks the command&rsquo;s exit status code. If the status code is o, the probe is successful. All other codes are considered failures.</li></ul></li></ul><h4 id=412-creating-an-http-based-liveness-probe>4.1.2. Creating an HTTP-based liveness probe</h4><h4 id=413-seeing-a-liveness-probe-in-action>4.1.3. Seeing a liveness probe in action</h4><h4 id=414-configuring-addiontal-properties-of-the-liveness-probe>4.1.4. Configuring addiontal properties of the liveness probe</h4><h4 id=415-creating-effective-liveness-probes>4.1.5. Creating effective liveness probes</h4><ul><li>What a liveness probe should check:<ul><li>Make sure the <code>/health</code> HTTP endpoint doesn&rsquo;t require authentication</li></ul></li><li>Keeping probes light</li><li>Don&rsquo;t Border implementing retry loops in your probes</li><li>Liveness probe wrap-up</li></ul><h3 id=42-introducing-replicationcontrollers>4.2. Introducing ReplicationControllers</h3><h4 id=421-the-operation-of-a-replicationcontroller>4.2.1. The operation of a ReplicationController</h4><ul><li>A ReplicationController has three essential parts:<ul><li>A label selector, which determines what pods are in the ReplicationController&rsquo;s scope</li><li>A replica count, which specifies the desired number of pods that should be running</li><li>A pod template, which is used when creating new pod replicas</li></ul></li><li>Understanding the benefits of using a ReplicationController:<ul><li>It makes sure a pod (or multiple pod replicas) is always running by starting a new pod when an existing one goes missing.</li><li>When a cluster node fails, it creates replacement replicas for all the pods that were running on the failed node (those that were under the Replication-Controller&rsquo;s control).</li><li>It enables easy horizontal scaling of pods - both manual and automatic</li></ul></li><li>A pod instance is nver relocated to another node. Instead, the ReplicationController creates a completely new pod instance that has no relation to the instance it&rsquo;s replacing.</li></ul><h4 id=422-creating-a-replicationcontroller>4.2.2. Creating a ReplicationController</h4><ul><li>Don&rsquo;t specify a pod selector when defining a ReplicationController.</li></ul><h4 id=423-seeing-the-replicationcontroller-in-action>4.2.3. Seeing the ReplicationController in action</h4><ul><li><code>rc</code> : replicationcontroller</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get rc
</span></span></code></pre></div><h4 id=424-moving-pods-in-and-out-of-the-scope-of-a-replicationcontroller>4.2.4. Moving pods in and out of the scope of a ReplicationController</h4><ul><li>Although a pod isn&rsquo;t tied to a ReplicationController, the pod does reference it in the metadata.ownerReferences field, which you can use to easily find which ReplicationController a pod belongs to.</li></ul><h4 id=425-changing-the-pod-template>4.2.5. Changing the pod template</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl edit rc &lt;rc-name&gt;
</span></span></code></pre></div><h4 id=426-horizontally-scaling-pods>4.2.6. Horizontally scaling pods</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl scale rc &lt;rc-name&gt; --replicas<span style=color:#f92672>=</span>&lt;num&gt;
</span></span></code></pre></div><h4 id=427-deleting-a-replicationcontroller>4.2.7. Deleting a ReplicationController</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete rc &lt;rc-name&gt;
</span></span></code></pre></div><h3 id=43-using-replicasets-instead-of-replicationcontrollers>4.3. Using ReplicaSets instead of ReplicationControllers</h3><h4 id=431-comparing-a-replicaset-to-a-replicationcontroller>4.3.1. Comparing a ReplicaSet to a ReplicationController</h4><h4 id=432-defining-a-replicaset>4.3.2. Defining a ReplicaSet</h4><h4 id=433-creating-and-examing-a-replicaset>4.3.3. Creating and examing a ReplicaSet</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><h4 id=434-using-the-replicasets-more-expressive-label-selectors>4.3.4. Using the ReplicaSet&rsquo;s more expressive label selectors</h4><ul><li>Selector key matchExpressions&rsquo; operators:<ul><li><code>In</code>-Label&rsquo;s value must match one of the specified values.</li><li><code>NotIn</code>-Label&rsquo;s value must not match any of the specified values.</li><li><code>Exists</code>-Pod must include a label with the specified key (the value isn&rsquo;t important). When using this operator, you shouldn&rsquo;t specify the values field.</li><li><code>DoesNotExist</code>-Pod must not include a label with the spcified key.</li></ul></li></ul><h4 id=435-wrapping-up-replicasets>4.3.5. Wrapping up ReplicaSets</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete rs &lt;rs-name&gt;
</span></span></code></pre></div><h3 id=44-running-exactly-one-pod-on-each-node-with-daemonsets>4.4. Running exactly one pod on each node with DaemonSets</h3><h4 id=441-using-a-daemonset-to-run-a-pod-one-every-node>4.4.1. Using a DaemonSet to run a pod one every node</h4><h4 id=442-using-a-daemonset-to-run-pods-only-on-certain-nodes>4.4.2. Using a DaemonSet to run pods only on certain nodes</h4><ul><li>Creating the DaemonSet</li><li>Adding the required label to your node(s)</li></ul><h3 id=45-running-pods-that-perform-a-single-completable-task>4.5. Running pods that perform a single completable task</h3><h4 id=451-introducing-the-job-resource>4.5.1. Introducing the Job resource</h4><h4 id=452-defining-a-job-resource>4.5.2. Defining a Job resource</h4><h4 id=453-seeing-a-job-run-a-pod>4.5.3. Seeing a Job run a pod</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get jobs
</span></span></code></pre></div><h4 id=454-running-multiple-pod-instances-in-a-job>4.5.4. Running multiple pod instances in a Job</h4><h4 id=455-limiting-the-time-allowed-for-a-job-pod-to-complete>4.5.5. Limiting the time allowed for a Job pod to complete</h4><h3 id=46-scheduling-jobs-to-run-periodically-or-once-in-the-future>4.6. Scheduling Jobs to run periodically or once in the future</h3><h4 id=461-creating-a-cronjob>4.6.1. Creating a CronJob</h4><h4 id=462-understanding-how-scheduled-jobs-are-run>4.6.2. Understanding how scheduled jobs are run</h4><h3 id=47-summary>4.7. Summary</h3><ul><li>You can specify a liveness probe to have Kubernetes restart your containre as soon as it&rsquo;s no longer healthy</li><li>Pods shouldn&rsquo;t be created directly, because they will not be re-created if they&rsquo;re deleted by mistake, if the node they&rsquo;re running on fails, or if they&rsquo;re evicted from the node.</li><li>ReplicationControllers always keep the desired number of pod replicas running</li><li>Scaling pods horizontally is as easy as changing the desired replica count on a ReplicationController</li><li>Pods aren&rsquo;t owned by the ReplicationControllers and can be moved between them if necessary.</li><li>A ReplicationController creates new pods from a pod template. Chaning the template has no effect on existing pods.</li><li>ReplicationControllers should be replaced with ReplicaSets and Deployments, which provide the same functionality, but with additional powerful features.</li><li>ReplicationControllers and ReplicaSets schedule pods to random cluster nodes, whereas DaemonSets make sure every node runs a single instance of a pod defined in the DaemonSet.</li><li>Pods that perform a batach task should be created through a Kubernetes Job resource, not directly or through a ReplicationController or similar object.</li><li>Jobs that need to run sometime in the future can be created through CronJob resources.</li></ul><h2 id=chapter-5-services-enabling-clients-to-discover-and-talk-to-pods>Chapter 5. Services: enabling clients to discover and talk to pods</h2><h3 id=51-intoducing-services>5.1. Intoducing services</h3><ul><li>When creating a service with multiple ports, you must speicify a name for each port.</li><li>FQDN : fully qualified domain name</li></ul><h3 id=52-connecting-to-services-living-outside-the-cluster>5.2. Connecting to services living outside the cluster</h3><h3 id=53-expsing-services-to-external-clients>5.3. Expsing services to external clients</h3><ul><li>Ways to make a service accisible externally:<ul><li>Setting the service type to NodePort</li><li>Setting the service type to LoadBalancer</li><li>Creating an Ingress resource</li></ul></li></ul><h3 id=54-exposing-services-externally-through-an-ingress-resource>5.4. Exposing services externally through an Ingress resource</h3><h3 id=55-signaling-when-a-pod-is-ready-to-accept-connections>5.5. Signaling when a pod is ready to accept connections</h3><ul><li>three types of readiness probes:<ul><li>An Exec probe, where a process is executed.</li><li>An HTTP GET probe</li><li>A TCP Socket probe</li></ul></li><li>If you want to add or remove a pod from a service manually, add enabled=true as a label to your pod and to the label selector of your service.</li><li>You should always define a readiness probe, even if it&rsquo;s as simple as sending an HTTP requeset to the base URL.</li><li>Don&rsquo;t include pod shutdown logic into your readiness probes</li></ul><h3 id=56-using-a-headless-service-for-discovering-individual-pods>5.6. Using a headless service for discovering individual pods</h3><h3 id=57-troubleshooting-services>5.7. Troubleshooting services</h3><ul><li>Make sure you&rsquo;re connecting to the service&rsquo;s cluster IP from within the cluster, not from the outside.</li><li>Don&rsquo;t bother pining the service IP to figure out if the service is accessible (remember, the service&rsquo;s cluster IP is a virtual IP and pining it will never work).</li><li>If you&rsquo;ve defined a readiness probe, make sure it&rsquo;s succeeding; otherwise the pod won&rsquo;t be part of the service.</li><li>To confirm that a pod is part of the service, examine the corresponding Endpoints object with <code>kubectl get endpoints</code>.</li><li>If you&rsquo;re trying to access the service through its FQDN or a part of it and it doesn&rsquo;t work, see if you can access it using its cluster IP instead of the FQDN.</li><li>Check whether you&rsquo;re connecting to the port exposed by the service and not the targtet port.</li><li>Try connecting to the pod IP directly to confirm your pod is accepting connections on the correct port.</li><li>If you can&rsquo;t even access your app through the pod&rsquo;s IP, make sure your app isn&rsquo;t only binding to localhost.</li></ul><h3 id=58-summary>5.8. Summary</h3><ul><li>Expose multiple pods that match a certain label selector under a single, stable IP address and port.</li><li>Make services accessible from inside the cluster by default, but allows your to make the service accessible from outside the cluster by setting its type to either <code>NodePort</code> or <code>LoadBalancer</code></li><li>Enables pods to discover services together with their IP addresses and ports by looking up environment variables</li><li>Allows discovery of and communication with services residing outside the cluster by creating a Service resource without specifying a selector, by creating an associated Endpoints resource instead</li><li>Provides a DNS CNAME alias for external services with the ExternalName service type</li><li>Expose multiple HTTP services through a single Ingress (consuming a single IP)</li><li>Uses a pod container&rsquo;s readiness probe to determine whether a pod should or shouldn&rsquo;t be included as a service endpoint</li><li>Enables discovery of pod IPs through DNS when you create a headless service</li></ul><h2 id=chapter6-volumes-attaching-disk-storage-to-containers>Chapter6. Volumes: attaching disk storage to containers</h2><h3 id=61-introducing-volumes>6.1. Introducing volumes</h3><ul><li>volume types:<ul><li>emptyDir : A simple empty directory used for storing transient data.</li><li>hostPath : Used for mounting directories from the worker node&rsquo;s filesystem into the pod</li><li>gitRepo : A volume initialized by checking out the contents of a Git repository</li><li>nfs : An NFS share mounted into the pod</li><li>gcePersistentDisk : Google Compute Engine Persistent Disk</li><li>cinder, cephfs, iscsi, flocker, glusterfs, guobyte, rdb, flexVolume, vsphere-Volume, photonPersistentDisk, scaleIO : Used for mounting other types of network storage</li><li>configMap, secret, downwardAPI : Special types of volumes used to expose certain Kubernetes resources and cluster information to the pod</li><li>persistentVolumeClaim : A way to use a pre- or dynamically provisioned persistent storage.</li></ul></li></ul><h3 id=62-using-volumes-to-share-data-between-containers>6.2. Using volumes to share data between containers</h3><h4 id=621-using-an-emptydir-volume>6.2.1. Using an emptyDir volume</h4><h4 id=622-using-a-git-repository-as-the-starting-point-for-a-volume>6.2.2. Using a Git repository as the starting point for a volume</h4><h3 id=63-accessing-files-on-the-worker-nodes-filesystem>6.3. Accessing files on the worker node&rsquo;s filesystem</h3><h3 id=64-using-persistent-storage>6.4. Using persistent storage</h3><h3 id=65-decoupling-pods-from-the-underlying-storage-technology>6.5. Decoupling pods from the underlying storage technology</h3><h4 id=651-introducing-persistentvolumes-and-persistentvolumeclaims>6.5.1. Introducing PersistentVolumes and PersistentVolumeClaims</h4><h4 id=652-creating-a-persistentvolume>6.5.2. Creating a PersistentVolume</h4><ul><li>pv : persistent volume</li><li>pvc : persistent volume claim</li></ul><h3 id=66-dynamic-provisioning-of-persistentvolumes>6.6. Dynamic provisioning of PersistentVolumes</h3><ul><li>Similar to PersistentVolumes, StorageClass resources aren&rsquo;t namespaced.</li></ul><h3 id=67-summary>6.7. Summary</h3><ul><li>Create a multi-container pod and have the pod&rsquo;s containers operate on the same files by adding a volume to the pod and mounting it in each container</li><li>Use the emptyDir volume to store temporary, non-persistent data</li><li>Use the gitRepo volume to easily populate a directory with the contents of a Git repository at pod startup</li><li>Use the hostPath volume to access files from the host node</li><li>Mount external storage in a volume to persist pod data across pod restarts</li><li>Decouple the pod from the storage infrastructure by using PersistentVolumes and PersistentVolumeClaims</li><li>Have PersistentVolumes of the desire (or the default) storage class dynamically provisioned for each PersistentVolumeClaim</li><li>Prevent the dyanic provisioner from interfering when you want the PersistentVolumeClaim te be bound to a pre-provisioned PersistentVolume</li></ul><h2 id=chapter-7-configmaps-and-secrets-configuringg-applications>Chapter 7. ConfigMaps and Secrets: configuringg applications</h2><h3 id=71-configuring-containerized-applications>7.1. Configuring containerized applications</h3><h3 id=72-passing-command-line-arguments-to-containers>7.2. Passing command-line arguments to containers</h3><h4 id=721-defining-the-command-and-arguments-in-docker>7.2.1. Defining the command and arguments in Docker</h4><ul><li>ENTRYPOINT defines the executable invoked when the container is started</li><li>CMD specifies the arguments that get passed to the ENTRYPOINT</li></ul><h3 id=73-setting-environment-variables-for-a-container>7.3. Setting environment variables for a container</h3><h3 id=74-decoupling-configuration-with-a-configmap>7.4. Decoupling configuration with a ConfigMap</h3><ul><li>ConfigMap keys must be a valid DNS subdomain (they may only contain alphanumeric characters, dashes, underscores, and dots). They may optionally include a leading dot.</li></ul><h3 id=75-using-secrets-to-pass-sensitive-data-to-containers>7.5. Using Secrets to pass sensitive data to containers</h3><h3 id=76-summary>7.6. Summary</h3><ul><li>Override the default command defined in a container image in the pod definition</li><li>Pass command-line arguments to the main container process</li><li>Set environment variables for a container</li><li>Decouple configuration from a pod specification and put it into a ConfigMap</li><li>Store sensitive data in a Secret and deliver it securely to containers</li><li>Create a docker-registry Scret and use it to pull images from a private image registry</li></ul><h2 id=chapter-8-accessing-pod-metadata-and-other-resources-from-applications>Chapter 8. Accessing pod metadata and other resources from applications</h2><h3 id=81-passing-metadata-through-the-downward-api>8.1. Passing metadata through the Downward API</h3><h3 id=82-talking-to-the-kubernetes-api-server>8.2. Talking to the Kubernetes API server</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl cluster-info
</span></span></code></pre></div><ul><li>Accessing the API server through kubectl proxy</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl proxy
</span></span></code></pre></div><ul><li>How a pod&rsquo;s name, namespace, and other metadata can be exposed to the process either through enviroment variables or files in a downwardAPI volume</li><li>How CPU and memory requests and limits are passed to your app in any unit the app requies</li><li>How a pod can use downwardAPI volumes to get up-to-date metadata, which may change during the lifetime of the pod (such as labels and annotations)</li><li>How you can brwose the Kubernetes REST API through kubectl proxy</li><li>How pods can find the API server&rsquo;s location through environment variables or DNS, similar to any other Service defined in Kubernetes</li><li>How an application running in a pod can verify that it&rsquo;s talking to the API server and how it can authenticate itself</li><li>How using an ambassador container can make talking to the API server from within an app much simpler</li><li>How client libraries can get you interacting with Kubernetes in minutes</li></ul><h2 id=chapter-9-deployments-updating-applications-declaratively>Chapter 9. Deployments: updating applications declaratively</h2><h3 id=91-updating-applications-running-in-pods>9.1. Updating applications running in pods</h3><h3 id=92-performing-an-automatic-rolling-update-with-a-replicationcontroller>9.2. Performing an automatic rolling update with a ReplicationController</h3><h3 id=93-using-deployments-for-updating-apps-declaratively>9.3. Using Deployments for updating apps declaratively</h3><ul><li>Deployment -> ReplicaSet -> Pods</li><li><code>kubectl edit</code></li><li><code>kubectl patch</code></li><li><code>kubectl apply</code></li><li><code>kubectl replace</code></li><li><code>kubectl set image</code></li></ul><h3 id=94-summary>9.4. Summary</h3><ul><li><p>Perform a rolling update of pods managed by a ReplicationController</p></li><li><p>Create Deployments instead of lower-level ReplicationControllers or ReplicaSets</p></li><li><p>Update your pods by editing the pod template in the Deployment specification</p></li><li><p>Roll back a Deployment either to the previous revision or to any earlier revision still listed in the revision history</p></li><li><p>Abort a Deployment mid-way</p></li><li><p>Pause a Deployment to inspect how a single instnace of the new version behaves in production before allowing additional pod instances to replace the old ones</p></li><li><p>Control the rate of the rolling update through maxSurege and maxUnavailable properties</p></li><li><p>Use minReadySeconds and readiness probes to have the rollout of a faulty version blocked automatically</p></li><li><p>Use three dashes as a separator to define multiple resources in a single YAML file</p></li><li><p>Turn on kubectl&rsquo;s verbose loggin to se exacyl what it&rsquo;s doing behind the curtains</p></li></ul><h2 id=chatper-10-statefulsets-deploying-replicated-statful-applications>Chatper 10. StatefulSets: deploying replicated statful applications</h2><ul><li>StatefulSets were initially called PetSets. That name comes from the pets vs. cattle analogy explained here</li><li>SRV record<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl run -it srvlookup --image/tutum/dnsutils --rm
</span></span></code></pre></div></li></ul><h3 id=106-summary>10.6. Summary</h3><ul><li>Give replicated pods individual storage</li><li>Provide a stable identity to a pod</li><li>Create a StatefulSet and a corresponding headless governing Service</li><li>Scale and update a StatefulSet</li><li>Discorver other members of the StatefulSet through DNS</li><li>Connect to other members through their host names</li><li>Forcibly delete stateful pods</li></ul><h1 id=part-3-beyond-the-basics>Part 3. Beyond the basics</h1><h2 id=chapter-11-understanding-kubernetes-internals>Chapter 11. Understanding Kubernetes internals</h2><h3 id=111-understanding-the-architecture>11.1 Understanding the architecture</h3><ul><li>Kubernetes cluster is split into two parts:<ul><li>The Kubernetes Control Plane</li><li>The (worker) nodes</li></ul></li><li>Components of the Control Plane:<ul><li>The etcd distributed persistent storage</li><li>The API server</li><li>The Scheduler</li><li>The Controller Manager</li></ul></li><li>Components running on the worker nodes:<ul><li>The Kubelet</li><li>The Kubernetes Service Proxy (kube-proxy)</li><li>The Container Runtime (Docker, rtc, or others)</li></ul></li><li>Add-on components:<ul><li>The Kubernetes DNS server</li><li>The Dashboard</li><li>An Ingress controller</li><li>Heapster</li><li>The Container Network Interface newtork plugin</li></ul></li></ul><h4 id=1111-the-distribued-natrue-of-kubernetes-components>11.1.1. The distribued natrue of Kubernetes components</h4><ul><li>Kubernetes system components communicate only with the API server.</li><li>The API server is the only component that communicates with etcd.</li><li>The Control Plane components, as well as kube-proxy, can either be deployed on the system directly or they can run as pods.</li><li>The Kubelet is the only component that always runs as a regular system component.</li></ul><h4 id=1112-how-kubernetes-uses-etcd>11.1.2. How kubernetes uses etcd</h4><ul><li>Optimistic concurrency control.</li><li>Ensuring consistency when etcd is clustered</li></ul><h4 id=1113-what-the-api-server-does>11.1.3. What the API server does</h4><ol><li>Authenticating the client with authentication plugins</li><li>Authorizing the client with authorization plugins</li><li>Validating and/or Modifying the resource in the request with admission control plugins</li><li>Validating the resource and storing it persistently</li></ol><h4 id=1114-understanding-how-the-api-server-notifies-clients-of-resource-changes>11.1.4. Understanding how the API server notifies clients of resource changes</h4><ul><li>Clients watch for changes by opening an HTTP connection to the API server</li></ul><h4 id=1115-understanding-the-scheduler>11.1.5. Understanding the Scheduler</h4><ul><li>Understanding the default scheduling algorithm:<ul><li>Filtering the list of all nodes to obtain a list of acceptable nodes the pod can be scheduled to.</li><li>Prioritizing the acceptable nodes and choosing the best one. If multiple nodes have the highest score, round-robin is used to ensure pods are deployed across all of them evenly.</li></ul></li></ul><h4 id=1116-introducing-the-controllers-running-in-the-controller-manager>11.1.6. Introducing the controllers running in the Controller Manager</h4><ul><li>The single Controller Manager process currently combines a multitude of controllers performing various reconciliation tasks:<ul><li>Replication Manager (a controller for ReplicationController resources)</li><li>ReplicaSet, DaemonSet, and Job controllers</li><li>Deployment controller</li><li>StatefulSet controller</li><li>Node controller</li><li>Service controller</li><li>Endpoints controller</li><li>Namespace controller</li><li>PersistentVolume controller</li><li>Others</li></ul></li><li>Controllers never talk to each other directly.</li></ul><h4 id=1117-what-the-kubelet-does>11.1.7. What the Kubelet does</h4><h4 id=1118-the-role-of-the-kubernetes-service-proxy>11.1.8. The role of the Kubernetes Service Proxy</h4><ul><li>The userspace proxy mode</li><li>The iptables proxy mode</li></ul><h4 id=1119-introducing-kubernetes-add-ons>11.1.9. Introducing Kubernetes add-ons</h4><ul><li>How add-ons are deployed</li><li>How the DNS server works</li><li>How (most) ingress controllers work</li></ul><h4 id=11110-bringing-it-all-together>11.1.10. Bringing it all together</h4><h3 id=112-how-controllers-cooperate>11.2. How controllers cooperate</h3><h4 id=1121-understanding-which-components-are-involved>11.2.1. Understanding which components are involved</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get events --watch
</span></span></code></pre></div><h3 id=113-understanding-what-a-running-pod-is>11.3. Understanding what a running pod is</h3><ul><li>The pause container is an infrastructure container whose sole purpose is to hold all these namespaces.</li></ul><h3 id=114-inter-pod-networking>11.4. Inter-pod networking</h3><ul><li>Pods on a node are connected to the same bridge through virtual Ethernet interface paris.</li><li>For pods on different nodes to communicate, the bridges need to be connected somehow.</li></ul><h4 id=1143-introducing-the-container-network-interface>11.4.3. Introducing the Container Network Interface</h4><ul><li>CNI : Container Network Interface:<ul><li>Calico, Fannel, Romana, Weave Net&mldr;</li></ul></li></ul><h3 id=115-how-services-are-implemented>11.5. How services are implemented</h3><h4 id=1151-introducing-the-kube-proxy>11.5.1. Introducing the kube-proxy</h4><ul><li>userspace proxy mode : the kube-proxy was an actual proxy waiting for connections and for each incoming connection, opening a new connection to one of the pods.</li></ul><h4 id=1152-how-kube-proxy-uses-iptables>11.5.2. How kube-proxy uses iptables</h4><ol><li>When a service is created in the API server, the virtual IP address is assigned to it immediately.</li><li>The API server notifies all kube-proxy agents running on the worker nodes that a new Service has been created.</li><li>Each kube-proxy makes that service addressable on the node it&rsquo;s running on.</li></ol><ul><li><p>An Endpoints object holds the IP/port paris of all the pods that back the service (an IP/port pair can also point to something other than a pod).:</p><ul><li>So, all endpoints are watched by kube-proxy.</li></ul></li><li><p>The trip of packet from a pod to another pod of service:</p><ol><li>The packet&rsquo;s destination is initially set to the IP and port of the Service</li><li>The kernel checks if the packet matches any of iptables rule. When matched, the packet&rsquo;s destination IP and port should be replaced with the IP and port of a randomly selected pod.</li></ol></li></ul><h3 id=116-running-highly-avaiable-clusters>11.6. Running highly avaiable clusters</h3><ul><li>Running multiple instances to reduce the likelihood of downtime</li><li>Using leader-election for non-horizontally scalable apps</li></ul><h4 id=1162-making-kubernetes-control-plane-components-highly-available>11.6.2. Making Kubernetes Control Plane components highly available</h4><ul><li><p>To make Kubernetes highly available, you ndeed to run multiple master nodes:</p><ul><li>etcd, which is the distributed data store where all the API objects are kept</li><li>API server</li><li>Controller Manager, which is the process in which all the controllers run</li><li>Scheduler</li></ul></li><li><p>Behind LoadBalancer, all etcd on master nodes (including stand-by) are connected each other:</p><ul><li>Because etcd was designed as a distributed system, one of its key features is the ability to run multiple etcd instances, so making it highly available is no big deal.</li><li>API server is (almost completely) stateless (all the data is stored in etcd)</li></ul></li><li><p>Ensuring high avilability of the controllers and the Scheduler:</p><ul><li>leader election</li></ul></li></ul><h2 id=chapter-12-securing-the-kubernetes-api-server>Chapter 12. Securing the Kubernetes API server</h2><h3 id=121-understanding-authentication>12.1. Understanding authentication</h3><ul><li>Several authentication plugins:<ul><li>From the client certificate, From an authentication token passed in an HTTP header, Basic HTTP authentication</li></ul></li></ul><h4 id=1111-users-and-groups>11.1.1. Users and groups</h4><ul><li>Kubernetes distinguises between two kinds of clients connecting to the API server:<ul><li>users, pods</li></ul></li><li>pods use ServiceAccount</li><li>built-in groups:<ul><li><code>system:unauthenticated</code>, <code>system:authenticated</code>, <code>system:serviceaccounts</code>, <code>system:serviceaccounts:&lt;namespace></code></li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get sa
</span></span></code></pre></div><h4 id=1114-assigning-a-serviceaccount-to-a-pod>11.1.4. Assigning a ServiceAccount to a pod</h4><h3 id=122-securing-the-cluster-with-role-based-access-control>12.2. Securing the cluster with role-based access control</h3><h4 id=1221-introducing-the-rbac-authorization-plugin>12.2.1. Introducing the RBAC authorization plugin</h4><p>| HTTP method | Verb for single resource | Verb for collection |
| GET, HEAD | get(and watch for watching) | list (and watch) |
| POST | create | n/a |
| PUT | update | n/a |
| PATCH | patch | n/a |
| DELETE | delete | deletecollection |</p><h4 id=1222-introducing-rbac-resources>12.2.2. Introducing RBAC resources</h4><ul><li>Roles and ClusterRoles</li><li>RoleBindings and ClusterRoleBindings</li></ul><hr><h3 id=123-summary>12.3. Summary</h3><ul><li>Clients of the API server include both human users and applications running in pods.</li><li>Application in pods are associated with a ServiceAccount.</li><li>Both users and ServiceAccounts are associated with groups.</li><li>By default, pods run under the default ServiceAccount, which is created for each namespace automatically.</li><li>Addiontional ServiceAccounts can be created manually and associated with a pod.</li><li>ServiceAccounts can be configured to allow mouting only a constrained list of Secrets in a given pod.</li><li>A serviceAccount can also be used to attach image pull Secrets to pods, so you don&rsquo;t need to specify the Secrets in every pod.</li><li>Roles and ClusterRoles define what actions can be performed on which resources.</li><li>RoleBindings and ClusterRoleBindings bind Roles and ClusterRoles to users, groups, and ServiceAccounts.</li><li>Each cluster comes with default ClusterRolese and ClusterRoleBindings.</li></ul><h2 id=chatper-13-securing-cluster-nodes-and-the-network>Chatper 13. Securing cluster nodes and the network</h2><h3 id=131-using-the-host-nodes-namespaces-in-a-pod>13.1. Using the host node&rsquo;s namespaces in a pod</h3><h4 id=1311-using-the-nodes-network-namespace-in-a-pod>13.1.1. Using the node&rsquo;s network namespace in a pod</h4><h4 id=1312-binding-to-a-host-port-without-using-the-hosts-network-namespace>13.1.2. Binding to a host port without using the host&rsquo;s network namespace</h4><h3 id=132-configuring-the-containers-security-context>13.2. Configuring the container&rsquo;s security context</h3><h4 id=1321-running-a-container-as-a-specific-user>13.2.1. Running a container as a specific user</h4><h4 id=1322-preventing-a-container-from-running-as-root>13.2.2. Preventing a container from running as root</h4><h4 id=1323-running-pods-in-privileged-mode>13.2.3. Running pods in privileged mode</h4><h4 id=1324-adding-individual-kernel-capabilities-to-a-container>13.2.4. Adding individual kernel capabilities to a container</h4><h3 id=133-restricting-the-use-of-security-related-features-in-pods>13.3. Restricting the use of security-related features in pods</h3><h4 id=1331-introducing-the-podsecuritypolicy-resource>13.3.1. Introducing the PodSecurityPolicy resource</h4><ul><li>Understanding what a Pod SecurityPolicy can do:<ul><li>Whether a pod can use the host&rsquo;s IPC, PID, or Network namespaces</li><li>Which host ports a pod can bind to</li><li>What user IDs a container can run as</li><li>Whether a pod with privileged containers can be created</li><li>Which kernel capabilities are allowed, which are added by default and which are always dropped</li><li>What SELinux labels a container can use</li><li>Whether a container can use a writable root filesytem or not</li><li>Which filesystem groups the container can run as</li><li>Which volume types a pod can use</li></ul></li></ul><h4 id=1332-understanding-runasuser-fsgroup-and-supplementalgroups-policies>13.3.2. Understanding runAsUser, fsGroup, and supplementalGroups policies</h4><ul><li>Changing the policy has no effect on exising pods, because Pod-Security-Policies are enforced only when creating or updating pods.</li></ul><h4 id=1333-configuring-allowed-default-and-disallowed-capabilities>13.3.3. Configuring allowed, default, and disallowed capabilities</h4><h4 id=1334-constraining-the-types-of-volumes-pods-can-use>13.3.4. Constraining the types of volumes pods can use</h4><h4 id=1335-assigning-different-podsecuritypolicies-to-different-users-and-groups>13.3.5. Assigning different PodSecurityPolicies to different users and groups</h4><ul><li>psps : PodSecurityPolicy</li></ul><h3 id=134-isolation-the-pod-network>13.4. Isolation the pod network</h3><ul><li>ingress(out->in), egress(in->out)</li></ul><h4 id=1342-allowing-only-some-pods-in-the-namespace-to-connect-to-a-server-pod>13.4.2. Allowing only some pods in the namespace to connect to a server pod</h4><h4 id=1343-isolating-the-network-between-kubernetes-namespaces>13.4.3. Isolating the network between Kubernetes namespaces</h4><h4 id=1344-isolating-using-cidr-notation>13.4.4. Isolating using CIDR notation</h4><h3 id=135-summary>13.5. Summary</h3><ul><li>Pods can use the node&rsquo;s Linux namespaces instead of using their own.</li><li>Containers can be configured to run as a different user and/or group than the one defined in the container image.</li><li>Containers can also run in priviledged mode, allowing them to access the node&rsquo;s devices that are otherwise not exposed to pods.</li><li>Containers can be run as read-only, preventing processes from writing to the container&rsquo;s filesystem (and only allowing them to write to mounted volumes).</li><li>Cluster-level PodSecurityPolicy resources can be created to prevent users from creating pods that could compromise a node.</li><li>PodSecurityPolicy resources can be associated with specific user using RBAC&rsquo;s ClusterRoles and ClusterRoleBindings.</li><li>NetworkPolicy resources are used to limit a pod&rsquo;s inbound and/or outbound traffic.</li></ul><h2 id=chapter-14-managing-pods-computational-resources>Chapter 14. Managing pods&rsquo; computational resources</h2><h3 id=141-requesting-resources-for-a-pods-containers>14.1. Requesting resources for a pod&rsquo;s containers</h3><ul><li>Understanding how resource requests affect scheduling:<ul><li>Understanding how the Scheduler determines if a pod can fit on a node:<ul><li>Scheduler doesn&rsquo;t look at how much of each individual resource is begin used at the exact time of scheduling</li><li>Scheduleer guarantee given to the already deployed pods</li><li><code>LeastRequestedPriory</code>, <code>MostRequestedPriority</code></li><li>By keeping pods tightly packed, certain nodes are left vacant and can be removed.</li></ul></li><li>Inspecting a node&rsquo;s capacity:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl describe nodes
</span></span></code></pre></div></li></ul></li></ul><h3 id=142-limiting-resources-available-to-a-container>14.2. Limiting resources available to a container</h3><ul><li>The sum of all resource limits of all the pods on a node is allowed to exceed 100% of the node&rsquo;s capacity.</li><li>Understanding that containers always see the node&rsquo;s memory, not the container&rsquo;s</li><li>Understanding that containers also see all the node&rsquo;s CPU cores</li></ul><h3 id=143-understanding-pod-qos-classes>14.3. Understanding pod QoS classes</h3><ul><li><p>Three Quality of Service (QoS) classes:</p><ul><li><code>BestEffort</code> : the lowest priority</li><li><code>Burstable</code></li><li><code>Guarantted</code> : the highest<ul><li>When use Guarantted, the three things need to be true:<ul><li>Requests and limits need to be set for both CPU and memory.</li><li>They need to be set for each container.</li><li>They need to be equal (the limit needs to match the request for each resource in each container)</li></ul></li></ul></li></ul></li><li><p>Understanding which process gets killed when memory is low:</p><ul><li>Basically, depending on priority : Kill order (BestEffort -> Burstable -> Guarantted)</li><li>Understnading how containers with the same Qos class are handled:<ul><li>OOM(OutOfMemory) scores</li></ul></li></ul></li></ul><h3 id=144-setting-default-requests-and-limits-for-pods-per-namespace>14.4. Setting default requests and limits for pods per namespace</h3><h3 id=145-limiting-the-total-resources-available-in-a-namespace>14.5. Limiting the total resources available in a namespace</h3><h4 id=1451-introducing-the-resource-quota-object>14.5.1. Introducing the Resource Quota object</h4><h3 id=146-monitoring-pod-resource-usage>14.6. Monitoring pod resource usage</h3><ul><li>cAdvisor</li><li>Heapster</li><li>Storing and analyzing historical resource consumption statistics:<ul><li>InfluxDB, Grafana</li></ul></li></ul><h2 id=chatper-15-automatic-scaling-of-pods-and-cluseter-nodes>Chatper 15. Automatic scaling of pods and cluseter nodes</h2><h3 id=151-horizontal-pod-autoscaling>15.1. Horizontal pod autoscaling</h3><h4 id=1511-understanding-the-autoscaling-process>15.1.1. Understanding the autoscaling process</h4><ul><li>Obtaining pod metrics:<ul><li>Pod(s) - cAdvisor(s) - Heapster - Horizontal Pod Autoscaler(s)</li><li>Prior to k8s version 1.6, the HorizontalPodAutoscaler obtained the metrics from Heapster directly.</li><li>Heapster was deprecated in version 1.11, and removed in version 1.13</li></ul></li><li>Calculating the required number of pods</li><li>Updating the desired replica count on the scaled resource</li></ul><h4 id=1512-scaling-based-on-cpu-utilization>15.1.2. Scaling based on CPU utilization</h4><ul><li>Creating a HorizontalPodAutoscaler based on CPU usage</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl autoscale deployment &lt;deployment name&gt; --cpu-percent<span style=color:#f92672>=</span><span style=color:#ae81ff>30</span> --min<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> --max<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>
</span></span></code></pre></div><ul><li>Seeing the first automatic rescale event</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get hpa
</span></span></code></pre></div><ul><li>Triggering a scale-up</li><li>Seeing the Autoscaler scale up the deployment</li><li>Understanding the maximum rate of scaling:<ul><li>auto scaling events has minimum gap.</li></ul></li><li>Modifying the target metric value on an existing HPA object</li></ul><h4 id=1513-scaling-based-on-memory-consumption>15.1.3. Scaling based on memory consumption</h4><h4 id=1514-scaling-based-on-other-and-custom-metrics>15.1.4. Scaling based on other and custom metrics</h4><ul><li><p><a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#scaling-on-custom-metrics>https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#scaling-on-custom-metrics</a></p></li><li><p>Three types of metrics which we can use for an HPA object:</p><ul><li>Resource, Pods, object</li></ul></li><li><p>Understanding the Resource metric type</p></li><li><p>Understanding the pods metric type</p></li></ul><h4 id=1515-determining-which-metrics-areappropriate-for-autoscaling>15.1.5. Determining which metrics areappropriate for autoscaling</h4><h4 id=1516-scaling-down-to-zero-replicas>15.1.6. Scaling down to zero replicas</h4><ul><li><a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#scaling-on-custom-metrics>https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#scaling-on-custom-metrics</a></li></ul><h3 id=152-vertical-pod-autoscaling>15.2. Vertical pod autoscaling</h3><ul><li><a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler>https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler</a></li></ul><h4 id=1521-automatically-configuring-resource-requests>15.2.1. Automatically configuring resource requests</h4><h4 id=1522-modifying-resource-requests-while-a-pod-is-running>15.2.2. Modifying resource requests while a pod is running</h4><h3 id=153-horizontal-scaling-of-cluster-nodes>15.3. Horizontal scaling of cluster nodes</h3><h4 id=1531-introducing-the-cluster-autoscaler>15.3.1. Introducing the Cluster Autoscaler</h4><ul><li><p>Requesting additional nodes from the cloud infrastructure:</p><ol><li>Autoscaler notices a Pod can&rsquo;t be schdeulded to exsiting nodes</li><li>Autoscaler determines which node type (if any) would be able to fit the pod. If multiple types could fit the pod, it selects one of them.</li><li>Autoscaler scales up the node group selected in previous step.</li></ol></li><li><p>Relinquishing nodes</p></li></ul><h4 id=1532-enabling-the-cluster-autoscaler>15.3.2. Enabling the Cluster Autoscaler</h4><h4 id=1533-limiting-service-disruption-during-cluster-scale-down>15.3.3. Limiting service disruption during cluster scale-down</h4><ul><li>PodDiscruptionBudget</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl create pdb &lt;pdb name&gt; --selector<span style=color:#f92672>=</span>&lt;selector; e.g. app<span style=color:#f92672>=</span>kubia&gt; --min-available<span style=color:#f92672>=</span>&lt;numeric; e.g. 3&gt;
</span></span></code></pre></div><h3 id=154-summary>15.4. Summary</h3><ul><li>Configuring the automatic horizontal scaling of pods is as easy as creating a Horizontal-PodAutoscaler object and pointing it to a Deployment, ReplicaSet, or ReplicationController and specifying the target CPU utilization for the pods.</li><li>Besides having the Horizontal Pod Autoscaler perform scaling operations based on the pods&rsquo; CPU utilization, you can also configure it to scale based on your own application-provided custom metrics or metrics related to other objects deployed in the cluster.</li><li>Vertical pod autoscaling isn&rsquo;t possible yet.</li><li>Even cluster nodes can be scaled automatically if your Kubernetes cluster runs on a supported cloud provider.</li><li>You can run one-off processes in a pod and have the pod stopped and deleted automatically as soon you press CTRL+C by using kubectl run with the -it and &ndash;rm options.</li></ul><h2 id=chapter-16-advanced-schdeuling>Chapter 16. Advanced schdeuling</h2><h3 id=161-using-taints-and-tolerations-to-repel-pods-from-certain-nodes>16.1. Using taints and tolerations to repel pods from certain nodes</h3><h4 id=1611-introducing-taints-and-tolerations>16.1.1. Introducing taints and tolerations</h4><ul><li>Displaying a node&rsquo;s taints</li><li>Displaying a pod&rsquo;s toelrations</li><li>Understanding taint effects:<ul><li>NoSchedule</li><li>PreferNoSchedule</li><li>NoExecute</li></ul></li></ul><h4 id=1612-adding-custom-taints-to-a-node>16.1.2. Adding custom taints to a node</h4><h4 id=1613-adding-toleratiosn-to-pods>16.1.3. Adding toleratiosn to pods</h4><h4 id=1614-understanding-what-taints-and-tolerations-can-be-used-for>16.1.4. Understanding what taints and tolerations can be used for</h4><ul><li>Using taints and tolerations during scheduling:<ul><li>Taints can be used to prevent scheduling of new pods and to define unpreferred nodes and even evict existing pods from a node</li></ul></li><li>Configuring how long after a node failure a pod is rescheduled</li></ul><h3 id=162-using-node-affinity-to-attract-pods-to-certain-nodes>16.2. Using node affinity to attract pods to certain nodes</h3><ul><li>Comparing node affinity to node selectors</li><li>Examining the default node labels</li></ul><h4 id=1621-specifying-hard-node-affinity-rules>16.2.1. Specifying hard node affinity rules</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kubia-gpu</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>affinity</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>nodeAffinity</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>requiredDuringSchedulingIgnoredDuringExecution</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>nodeSelectorTerms</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>matchExpressions</span>:
</span></span><span style=display:flex><span>          - <span style=color:#f92672>key</span>: <span style=color:#ae81ff>gpu</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>operation</span>: <span style=color:#ae81ff>In</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>values</span>:
</span></span><span style=display:flex><span>            - <span style=color:#e6db74>&#34;true&#34;</span>
</span></span></code></pre></div><ul><li><p>Making sense of the long nodeAffinity attribute name:</p><ul><li><code>requiredDuringScheduling...</code> : the node must have for the pod to be scheduled to the node.</li><li><code>...IgnoredDuringExecution</code> : don&rsquo;t affect pods already executing on the node.</li></ul></li><li><p>Understanding nodeSelectorTerms</p></li></ul><h4 id=1622-prioritizing-nodes-when-scheduling-a-pod>16.2.2. Prioritizing nodes when scheduling a pod</h4><ul><li>Labeling nodes</li></ul><h3 id=163-co-locating-pods-with-pod-affinity-and-anti-affinity>16.3. Co-locating pods with pod affinity and anti-affinity</h3><h4 id=1631-using-inter-pod-affinity-to-deploy-pods-on-the-same-node>16.3.1. Using inter-pod affinity to deploy pods on the same node</h4><ul><li>Specifying pod affinity in a pod definition</li><li>Deploying a pod with pod affinity</li><li>Understanding how the scheduler uses pod affinity rules</li></ul><h4 id=1632-deploying-pods-in-the-same-rack-availability-zone-or-geographic-region>16.3.2. Deploying pods in the same rack, availability zone, or geographic region</h4><ul><li><a href=https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/>https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</a></li><li>Co-locating pods in the same availability zone</li><li>Co-locating pods in the same geographical region</li></ul><h4 id=1633-expressing-pod-affinity-preferences-instad-of-hard-requirements>16.3.3. Expressing pod affinity preferences instad of hard requirements</h4><h4 id=1634-schdeduling-pods-away-from-each-other-with-pod-anti-affinity>16.3.4. Schdeduling pods away from each other with pod anti-affinity</h4><h3 id=164-summary>16.4. Summary</h3><ul><li>If you add a taint to a node, pods won&rsquo;t be scheduled to that node unless they tolerate that taint.</li><li>Three types of taints exsit:<ul><li><code>NoSchedule</code>: completely prevents chdeduling</li><li><code>PreferNoSchedule</code>: isn&rsquo;t as strict</li><li><code>NoExecute</code>: even evicts existing pods from a node.</li></ul></li><li>The <code>NoExecute</code> taint is also used to specify how long the Control Plan should wait before rescheduling the pod when the node it runs on becomes unreachable or unready.</li><li>Node affinity allows you to specify which nodes a pod should be scheduled to. It can be used to specify a hard requirement or to only express a node preference.</li><li>Pod affinity is used to make the Scheduler deploy pods to the same node where another pod is running (based on the pod&rsquo;s lables).</li><li>Pod affinity&rsquo;s <code>topologyKey</code> specifies how close the pod should be deployed to the other pod(onto the same node or onto a node in the same rack, availability zone, or avabilability region).</li><li>Pod anti-affinity can be used to keep certain pods away from each other.</li><li>Both pod affinity and anti-affinity, like node affinity, can either specify hard requirements or preferences.</li></ul><h2 id=chapter-17-best-practices-for-developing-apss>Chapter 17. Best practices for developing apss</h2><h3 id=171-bringing-everything-together>17.1. Bringing everything together</h3><h3 id=172-understanding-the-pods-lifecycle>17.2. Understanding the pod&rsquo;s lifecycle</h3><h4 id=1721-applicatiosn-must-expect-to-be-killed-and-replocated>17.2.1. Applicatiosn must expect to be killed and replocated</h4><ul><li>Expecting the local IP and hostname to change</li><li>Expecting the data written to disk to disappear</li><li>Using volumdes to preserve data across container restarts</li></ul><h4 id=1722-rescheduling-of-dead-or-partially-dead-pods>17.2.2. Rescheduling of dead or partially dead pods</h4><h4 id=1723-starting-pods-in-a-specific-order>17.2.3. Starting pods in a specific order</h4><ul><li>Understanding how pods are started</li><li>Introducing Init Containers</li><li>Adding an Init Container to a pod</li><li>Best practices for handlign inter-pod depdencies</li></ul><h4 id=1724-adding-lifecycle-hooks>17.2.4. Adding lifecycle hooks</h4><ul><li>Post-start hooks</li><li>Pre-stop hooks</li><li>Using a pre-stop hook because your app doesn&rsquo;t receive teh SIGTERM signal</li><li>Understanding that lifecycel hooks target containers, not pods</li></ul><h4 id=1725-understanding-pod-shutdown>17.2.5. Understanding pod shutdown</h4><ul><li><p>Specifying the termination grace period:</p><ul><li><code>spec.terminationGracePeriodSeconds</code> : defualt 30s</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete pod mypod --grace-period<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span> --force
</span></span></code></pre></div></li><li><p>Implementing the proper shutdown handler in your application:</p><ul><li>There are absolutely no guarantees that the pod will be allowed to complete its whole shut-down procedure.</li></ul></li><li><p>Replacing critical shut-down procedures with dedicated shut-down procedure pods</p></li></ul><h3 id=173-ensuring-all-client-requests-are-handled-properly>17.3. Ensuring all client requests are handled properly</h3><h4 id=1731-preventing-broken-client-connections-when-a-pod-is-starting-up>17.3.1. Preventing broken client connections when a pod is starting up</h4><h4 id=1732-preventing-broken-connections-during-pod-shutdown>17.3.2. Preventing broken connections during pod shutdown</h4><ul><li>Understanding the sequence of events occurring at pod deletion:<ul><li>A & B simultaneously run</li><li>A root : Watch notification(pod modified) - kubelet send <code>SIGTERM</code> to containers</li><li>B root : Watch notification(pod modified) - Endpoitns controller remove pod&rsquo;s IP from endpoints using API Server:<ul><li>kube-proxy recognize endpoints changes using watch notification:<ul><li>Update iptables rule</li></ul></li></ul></li></ul></li><li>To recap properly shutting down an application includes these steps:<ul><li>Wait for a few seconds, then stop accepting new connections</li><li>Close all keep-alive connections not in the middle of a request</li><li>Wait for all active requests to finish</li><li>Then shut down completely.</li></ul></li></ul><h3 id=174-making-your-apps-easy-to-run-and-manage-in-kubernetes>17.4. Making your apps easy to run and manage in Kubernetes</h3><h4 id=1741-making-manageable-container-images>17.4.1. Making manageable container images</h4><h4 id=1742-properly-tagging-your-images-and-using-imagepullpolicy-wisely>17.4.2. Properly tagging your images and using imagePullPolicy wisely</h4><ul><li><code>imagePullPolicy</code> set <code>Always</code> with <code>latest</code> tag.</li></ul><h4 id=1743-using-multi-dimensional-instead-of-single-dimensional-labels>17.4.3. Using multi-dimensional instead of single-dimensional labels</h4><ul><li>Labels:<ul><li>The name of the application (or perhaps microservice) the resource belongs to</li><li>Application tier (front-end, back-end, and so on)</li><li>Environment (development, QA, staging, production, and so on)</li><li>Version</li><li>Type of release (stable, canary, green or blue for green/blue development,s and so on)</li><li>Tenant (if you&rsquo;re running separate pods for each tenant instead of using namespaces)</li><li>Shard for sharded systems</li></ul></li></ul><h4 id=1744-describing-each-resource-through-annotations>17.4.4. Describing each resource through annotations</h4><h4 id=1745-providing-infromation-on-why-the-process-terminated>17.4.5. Providing infromation on why the process terminated</h4><ul><li><code>/dev/termination-log</code>, <code>terminationMessagePath</code></li></ul><h4 id=1746-handling-application-logs>17.4.6. Handling application logs</h4><ul><li>Using centralized logging:<ul><li>ELF, EFK</li></ul></li><li>Handling multi-line log statments</li></ul><h3 id=175-best-practices-for-development-and-testing>17.5. Best practices for development and testing</h3><h4 id=1751-running-apps-outside-of-kubernetes-during-development>17.5.1. Running apps outside of Kubernetes during development</h4><ul><li>Connecting to backend services:<ul><li><code>BACKEND_SERVICE_HOST</code> and <code>BACKEND_SERVICE_PORT</code></li></ul></li><li>Connecting to the API server</li><li>Runnign inside a container even during development</li></ul><h4 id=1752-using-minikube-in-development>17.5.2. Using Minikube in development</h4><h3 id=176-summary>17.6. Summary</h3><ul><li>Make you think about the difference between apps that are rarely moved between machines and apps running as pods, which are relocated much more frequently.</li><li>Help you understand that yuour multi-component apps (or microservices, if you will) shouldn&rsquo;t rely on a specific start-up order</li><li>Introduce init containers, which can be used to initialize a pod or delay the start of the pod&rsquo;s main containers until a precondtition is met.</li><li>Teach you about container lifecycle hooks and when to use them
Gain a deeper insight into the consequences of the distributed nature of Kubernetes components and its eventual consistency model.</li><li>Learn how to make your apps shut down properly without breaking client connections</li></ul></div><script src=/js/wikilink.js></script><div><script src=https://utteranc.es/client.js repo=minuk-dev/minuk-dev.github.io issue-term=pathname theme=github-dark crossorigin=anonymous async></script></div></main><footer class=footer><div class=footer-left></div><div class=footer-right><ul class=social><li><a href=/about>About</a></li><li><a href=https://github.com/minuk-dev>Github</a></li></ul></div></footer>
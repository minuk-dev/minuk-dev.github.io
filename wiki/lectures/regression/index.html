<!doctype html><html lang=ko-kr><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css><link rel=stylesheet href=/css/main.css><meta name=generator content="Hugo 0.140.0"><meta name=description content="minuk.dev wiki"><meta name=keywords content="hugo,site,new"><meta name=author content="Min-Uk.Lee"><title>Regression Analysis |
minuk dev wiki
</title><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></head><body><header class=header><div class=header_left><a href=/><img class=logo src=/images/Rb.png alt=logo>
MinUk.Dev</a></div><div class=header_middle>Regression Analysis -
minuk dev wiki</div></header><main><aside class=sidebar><ul class=section-tree><li class="dir opened-dir"><span class=dir-text>Front Page</span><ul class=section-tree><li class=file><a href=https://minuk.dev/wiki/kanban/ title=./wiki/kanban/></a></li><li class=file><a href=https://minuk.dev/wiki/daily/2025-01-13/ title=./wiki/daily/2025-01-13/>2025-01-13</a></li><li class=file><a href=https://minuk.dev/wiki/jekyll-%EA%B8%B0%EB%B0%98-wiki-%EC%97%90%EC%84%9C-hugo-%EB%A1%9C-%EB%84%98%EC%96%B4%EA%B0%80%EA%B8%B0/ title=./wiki/jekyll-%EA%B8%B0%EB%B0%98-wiki-%EC%97%90%EC%84%9C-hugo-%EB%A1%9C-%EB%84%98%EC%96%B4%EA%B0%80%EA%B8%B0/></a></li><li class=file><a href=https://minuk.dev/wiki/kubernetes-community-day-korea/ title=./wiki/kubernetes-community-day-korea/>Kubernetes Community Day 2024</a></li><li class=file><a href=https://minuk.dev/wiki/observability-engineering-kr/ title=./wiki/observability-engineering-kr/>Observability Engineering</a></li><li class=file><a href=https://minuk.dev/wiki/learning-opentelemetry/ title=./wiki/learning-opentelemetry/>Learning OpenTelemetry</a></li><li class=file><a href=https://minuk.dev/wiki/prometheus-native-histograms-in-production/ title=./wiki/prometheus-native-histograms-in-production/>prometheus-native-histograms-in-proudction</a></li><li class=file><a href=https://minuk.dev/wiki/making-sense-of-your-vital-signals-the-future-of-pod-and-containers-monitoring/ title=./wiki/making-sense-of-your-vital-signals-the-future-of-pod-and-containers-monitoring/>Making Sense of Your Vital Signals - The Future of Pod and Containers Monitoring</a></li><li class=file><a href=https://minuk.dev/wiki/defining-a-common-observability-query-language-and-other-observability-tag-updates/ title=./wiki/defining-a-common-observability-query-language-and-other-observability-tag-updates/>Defining A Common Observability Query Language and Other observability TAG Updates</a></li><li class=file><a href=https://minuk.dev/wiki/beyond-tracing-what-do-we-do-with-all-this-data/ title=./wiki/beyond-tracing-what-do-we-do-with-all-this-data/>Beyond Tracing - What do we do with all this data</a></li><li class=file><a href=https://minuk.dev/wiki/grafanacon/ title=./wiki/grafanacon/>grafanacon</a></li><li class=file><a href=https://minuk.dev/wiki/observability-engineering/ title=./wiki/observability-engineering/>Observability Engineering</a></li><li class=file><a href=https://minuk.dev/wiki/rust/ title=./wiki/rust/>rust</a></li><li class=file><a href=https://minuk.dev/wiki/opentelemetry-metrics-deep-dive/ title=./wiki/opentelemetry-metrics-deep-dive/>opentelemetry metrics deep dive</a></li><li class=file><a href=https://minuk.dev/wiki/armeria/ title=./wiki/armeria/>armeria</a></li><li class=file><a href=https://minuk.dev/wiki/kotlin/ title=./wiki/kotlin/>kotlin</a></li><li class=file><a href=https://minuk.dev/wiki/loki-best-practices/ title=./wiki/loki-best-practices/>loki label best practices</a></li><li class=file><a href=https://minuk.dev/wiki/loki/ title=./wiki/loki/>loki</a></li><li class=file><a href=https://minuk.dev/wiki/grafana-loki-like-prometheus-but-for-logs/ title=./wiki/grafana-loki-like-prometheus-but-for-logs/>Grafana Loki - Like Prometheus, But for logs.</a></li><li class=file><a href=https://minuk.dev/wiki/spring-boot-cli/ title=./wiki/spring-boot-cli/>spring boot cli</a></li><li class=file><a href=https://minuk.dev/wiki/autoconf/ title=./wiki/autoconf/>autoconf</a></li><li class=file><a href=https://minuk.dev/wiki/re-minuk-k8s/ title=./wiki/re-minuk-k8s/>다시 시작하는 쿠버네티스 세팅</a></li><li class=file><a href=https://minuk.dev/wiki/cloud-native-go/ title=./wiki/cloud-native-go/>Cloud Native Go</a></li><li class=file><a href=https://minuk.dev/wiki/overlayfs/ title=./wiki/overlayfs/>overlayfs</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/design-pattern/ title=./wiki/lectures/design-pattern/>lectures/design-pattern</a></li><li class=file><a href=https://minuk.dev/wiki/site-reliability-engineering/ title=./wiki/site-reliability-engineering/>사이트 신뢰성 엔지니어링</a></li><li class=file><a href=https://minuk.dev/wiki/to-ipv6-the-dual-stack-adoption-advisory-panel/ title=./wiki/to-ipv6-the-dual-stack-adoption-advisory-panel/>To IPv6 - The Dual-stack Adoption Advisory Panel</a></li><li class=file><a href=https://minuk.dev/wiki/prometheus-intro-and-deep-dive/ title=./wiki/prometheus-intro-and-deep-dive/>Prometheus Intro and Deep Dive</a></li><li class=file><a href=https://minuk.dev/wiki/cs/ title=./wiki/cs/>cs 기본</a></li><li class=file><a href=https://minuk.dev/wiki/coredns/ title=./wiki/coredns/>learning-coredns</a></li><li class=file><a href=https://minuk.dev/wiki/coredns-into-and-deep-dive/ title=./wiki/coredns-into-and-deep-dive/>CoreDNS - Intro and Deep Dive</a></li><li class=file><a href=https://minuk.dev/wiki/deep-dive-into-minikube/ title=./wiki/deep-dive-into-minikube/>Deep Dive into Minikube</a></li><li class=file><a href=https://minuk.dev/wiki/making-your-apps-and-infrastructure-services-failure-resilient-with-dapr/ title=./wiki/making-your-apps-and-infrastructure-services-failure-resilient-with-dapr/>Making Your Apps and Infrastructure Services Failure-Resilient with Dapr</a></li><li class=file><a href=https://minuk.dev/wiki/make-cloud-native-chaos-engineering-easier-deep-dive-into-chaos-mesh/ title=./wiki/make-cloud-native-chaos-engineering-easier-deep-dive-into-chaos-mesh/>Make Cloud Native Chaos Engineering Easier Deep Dive into Chaos Mesh</a></li><li class=file><a href=https://minuk.dev/wiki/selinux/ title=./wiki/selinux/>selinux</a></li><li class=file><a href=https://minuk.dev/wiki/http-go/ title=./wiki/http-go/>go snippet for go</a></li><li class=file><a href=https://minuk.dev/wiki/curl/ title=./wiki/curl/>curl</a></li><li class=file><a href=https://minuk.dev/wiki/volcano-intro-and-deep-dive/ title=./wiki/volcano-intro-and-deep-dive/>Volcano - Intro & Deep Dive</a></li><li class=file><a href=https://minuk.dev/wiki/intro-to-kubernetes-gitops-and-observability-hands-on-tutorial/ title=./wiki/intro-to-kubernetes-gitops-and-observability-hands-on-tutorial/>Intro to Kubernetes, GitOps, and Observability Hands-On Tutorial</a></li><li class=file><a href=https://minuk.dev/wiki/kubespray/ title=./wiki/kubespray/>kubespray</a></li><li class=file><a href=https://minuk.dev/wiki/spark-on-kubernetes-the-elastic-story/ title=./wiki/spark-on-kubernetes-the-elastic-story/>Spark on Kubernetes - The Elastic Story</a></li><li class=file><a href=https://minuk.dev/wiki/devops%EC%99%80-se%EB%A5%BC-%EC%9C%84%ED%95%9C-%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%BB%A4%EB%84%90-%EC%9D%B4%EC%95%BC%EA%B8%B0/ title=./wiki/devops%EC%99%80-se%EB%A5%BC-%EC%9C%84%ED%95%9C-%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%BB%A4%EB%84%90-%EC%9D%B4%EC%95%BC%EA%B8%B0/>DevOps와 SE를 위한 리눅스 커널 이야기</a></li><li class=file><a href=https://minuk.dev/wiki/running-containerd-and-k3s-on-macos/ title=./wiki/running-containerd-and-k3s-on-macos/>Running Containerd and k3s on MacOS</a></li><li class=file><a href=https://minuk.dev/wiki/twelve-factor-app/ title=./wiki/twelve-factor-app/>12요소 어플리케이션</a></li><li class=file><a href=https://minuk.dev/wiki/vim-go/ title=./wiki/vim-go/>vim/vim-go</a></li><li class=file><a href=https://minuk.dev/wiki/kubernetes-patterns/ title=./wiki/kubernetes-patterns/>쿠버네티스 패턴</a></li><li class=file><a href=https://minuk.dev/wiki/horizontalpodautoscaler/ title=./wiki/horizontalpodautoscaler/>Horizontal Pod AutoScaler</a></li><li class=file><a href=https://minuk.dev/wiki/what-if-kube-apiserver-could-be-extended-via-webassembly/ title=./wiki/what-if-kube-apiserver-could-be-extended-via-webassembly/>What If… Kube-Apiserver Could be Extended Via WebAssembly?</a></li><li class=file><a href=https://minuk.dev/wiki/the-future-of-reproducible-research-powered-by-kubeflow/ title=./wiki/the-future-of-reproducible-research-powered-by-kubeflow/>The Future Of Reproducible Research - Powered by Kubeflow</a></li><li class=file><a href=https://minuk.dev/wiki/this-is-the-way-a-crash-course-on-intricacies-of-managing-cpus/ title=./wiki/this-is-the-way-a-crash-course-on-intricacies-of-managing-cpus/>This is The Way- A Crash Course on the Intricacies of Managing CPUs in K8s</a></li><li class=file><a href=https://minuk.dev/wiki/kubernetes-graceful-shutdown/ title=./wiki/kubernetes-graceful-shutdown/>kubernetes-graceful-shutdown</a></li><li class=file><a href=https://minuk.dev/wiki/dockerfile/ title=./wiki/dockerfile/>dockerfile</a></li><li class=file><a href=https://minuk.dev/wiki/automated-progressive-delivery-using-gitops-and-service-mesh/ title=./wiki/automated-progressive-delivery-using-gitops-and-service-mesh/>Automated Progressive Delivery Using GitOps and Service Mesh</a></li><li class=file><a href=https://minuk.dev/wiki/containerd-proejct-update-and-deep-dive/ title=./wiki/containerd-proejct-update-and-deep-dive/>containerd Project Update and Deep Dive</a></li><li class=file><a href=https://minuk.dev/wiki/http2/ title=./wiki/http2/>http2 탐구</a></li><li class=file><a href=https://minuk.dev/wiki/grpc-for-microservices/ title=./wiki/grpc-for-microservices/>gRPC For Microservices Service-mesh and Observability</a></li><li class=file><a href=https://minuk.dev/wiki/kubecon/ title=./wiki/kubecon/>kubecon</a></li><li class=file><a href=https://minuk.dev/wiki/go-http/ title=./wiki/go-http/>go-http</a></li><li class=file><a href=https://minuk.dev/wiki/go/ title=./wiki/go/>go</a></li><li class=file><a href=https://minuk.dev/wiki/cri/ title=./wiki/cri/>CRI(Container Runtime Interface)</a></li><li class=file><a href=https://minuk.dev/wiki/vagrant/ title=./wiki/vagrant/>vagrant</a></li><li class=file><a href=https://minuk.dev/wiki/jsonpath/ title=./wiki/jsonpath/>jsonpath</a></li><li class=file><a href=https://minuk.dev/wiki/systemctl/ title=./wiki/systemctl/>systemctl 중요한것만 정리</a></li><li class=file><a href=https://minuk.dev/wiki/init/ title=./wiki/init/>linux init 요약</a></li><li class=file><a href=https://minuk.dev/wiki/process-cli/ title=./wiki/process-cli/>process 관련된 명령어 모음</a></li><li class=file><a href=https://minuk.dev/wiki/process-status/ title=./wiki/process-status/>process-status</a></li><li class=file><a href=https://minuk.dev/wiki/teamnote-go/ title=./wiki/teamnote-go/>teamnote-go</a></li><li class=file><a href=https://minuk.dev/wiki/cgroups/ title=./wiki/cgroups/>cgroup</a></li><li class=file><a href=https://minuk.dev/wiki/namespaces/ title=./wiki/namespaces/>namespaces</a></li><li class=file><a href=https://minuk.dev/wiki/lxc/ title=./wiki/lxc/>LXC</a></li><li class=file><a href=https://minuk.dev/wiki/devops/ title=./wiki/devops/>devops</a></li><li class=file><a href=https://minuk.dev/wiki/ringle/supply-and-demand/ title=./wiki/ringle/supply-and-demand/>Supply and Demand</a></li><li class=file><a href=https://minuk.dev/wiki/ringle/the-metaverse/ title=./wiki/ringle/the-metaverse/>ringle/The metaverse</a></li><li class=file><a href=https://minuk.dev/wiki/ringle/microsoft-x-activision-blizzard/ title=./wiki/ringle/microsoft-x-activision-blizzard/>ringle/Microsoft x Activision Blizzard</a></li><li class=file><a href=https://minuk.dev/wiki/topcit/ title=./wiki/topcit/>topcit 간략 공부</a></li><li class=file><a href=https://minuk.dev/wiki/software-engineering-at-google/ title=./wiki/software-engineering-at-google/>구글 엔지니어는 이렇게 일한다</a></li><li class=file><a href=https://minuk.dev/wiki/k8s-in-rpi/ title=./wiki/k8s-in-rpi/>k8s-in-rpi</a></li><li class=file><a href=https://minuk.dev/wiki/kubernetes/ title=./wiki/kubernetes/>kubernetes</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/numerical_analysis/ title=./wiki/lectures/numerical_analysis/>수치해석</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/information_security_theory/ title=./wiki/lectures/information_security_theory/>2022-1 정보보호이론</a></li><li class=file><a href=https://minuk.dev/wiki/%EC%A0%9C%ED%85%94%EC%B9%B4%EC%8A%A4%ED%85%90/ title=./wiki/%EC%A0%9C%ED%85%94%EC%B9%B4%EC%8A%A4%ED%85%90/>제텔카스텐</a></li><li class=file><a href=https://minuk.dev/wiki/%EB%A7%81%EA%B8%80/autonomous/ title=./wiki/%EB%A7%81%EA%B8%80/autonomous/>링글/Autonomous</a></li><li class=file><a href=https://minuk.dev/wiki/%EB%A7%81%EA%B8%80/metaverse/ title=./wiki/%EB%A7%81%EA%B8%80/metaverse/>링글/Metaverse</a></li><li class=file><a href=https://minuk.dev/wiki/algorithm/ title=./wiki/algorithm/>algorithm</a></li><li class=file><a href=https://minuk.dev/wiki/study-note/ title=./wiki/study-note/>study-note</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/machine-learning/ title=./wiki/lectures/machine-learning/>2022 1학기 머신러닝</a></li><li class=file><a href=https://minuk.dev/wiki/kubernetes-in-action/ title=./wiki/kubernetes-in-action/>Kubernetes in action</a></li><li class=file><a href=https://minuk.dev/wiki/effective-java/ title=./wiki/effective-java/>Effective Java</a></li><li class=file><a href=https://minuk.dev/wiki/kafka/ title=./wiki/kafka/>Kafka</a></li><li class=file><a href=https://minuk.dev/wiki/%EB%A9%B4%EC%A0%91%EC%A4%80%EB%B9%84/ title=./wiki/%EB%A9%B4%EC%A0%91%EC%A4%80%EB%B9%84/>면접 준비 자료</a></li><li class=file><a href=https://minuk.dev/wiki/zsh/ title=./wiki/zsh/>zsh</a></li><li class=file><a href=https://minuk.dev/wiki/simple-file/ title=./wiki/simple-file/>Simple한 File Server</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2021-12-03/ title=./wiki/%ED%9A%8C%EA%B3%A0/2021-12-03/>회고/2021-12-03</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/bayesian/week3/ title=./wiki/lectures/bayesian/week3/>bayesian/week3</a></li><li class=file><a href=https://minuk.dev/wiki/jupyter/ title=./wiki/jupyter/>jupyter notebook</a></li><li class=file><a href=https://minuk.dev/wiki/ffmpeg/ title=./wiki/ffmpeg/>ffmpeg 를 사용한 convert 요약</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/bayesian/week2/ title=./wiki/lectures/bayesian/week2/>bayesian/week2</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/bayesian/week1/ title=./wiki/lectures/bayesian/week1/>bayesian/week1</a></li><li class=file><a href=https://minuk.dev/wiki/pdf-test/ title=./wiki/pdf-test/>pdf-test</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/automata/ title=./wiki/lectures/automata/>오토마타와 형식언어 정리</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/computer-communication/ title=./wiki/lectures/computer-communication/>컴퓨터통신</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/introduction-to-statistical-learning/ title=./wiki/lectures/introduction-to-statistical-learning/>통계학습개론(Introduction to statistical learnning) 수업 정리</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/bayesian-statistics/ title=./wiki/lectures/bayesian-statistics/>베이지안 통계학(Bayesian Statistics)</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/multi-variant-statistical-analysis/ title=./wiki/lectures/multi-variant-statistical-analysis/>Multi Variant Statistical Analysis</a></li><li class=file><a href=https://minuk.dev/wiki/comment/ title=./wiki/comment/>주석 관련 좋은 글</a></li><li class=file><a href=https://minuk.dev/wiki/ssh/ title=./wiki/ssh/>ssh 관련 명령어 모음</a></li><li class=file><a href=https://minuk.dev/wiki/msk/ title=./wiki/msk/>amazon msk 삽질</a></li><li class=file><a href=https://minuk.dev/wiki/keras-book/ title=./wiki/keras-book/>케라스 창시자에게 배우는 딥러닝 책 공부</a></li><li class=file><a href=https://minuk.dev/wiki/msa-from-ddd/ title=./wiki/msa-from-ddd/>도메인 주도 설계로 시작하는 마이크로서비스 개발</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2021-07-18/ title=./wiki/%ED%9A%8C%EA%B3%A0/2021-07-18/>2021-07-21 회고</a></li><li class=file><a href=https://minuk.dev/wiki/iamroot19/ title=./wiki/iamroot19/>아이엠루트 스터디 자료 정리</a></li><li class=file><a href=https://minuk.dev/wiki/boj-9019/ title=./wiki/boj-9019/>boj-9019</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2021-06-19/ title=./wiki/%ED%9A%8C%EA%B3%A0/2021-06-19/>2021년 6월 19일 회고</a></li><li class=file><a href=https://minuk.dev/wiki/%EB%8F%85%EC%84%B1%EB%A7%90%ED%88%AC/ title=./wiki/%EB%8F%85%EC%84%B1%EB%A7%90%ED%88%AC/>독성말투</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/regression/ title=./wiki/lectures/regression/>Regression Analysis</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%94%BC%EC%8B%9C%EC%8B%A4/ title=./wiki/%ED%94%BC%EC%8B%9C%EC%8B%A4/>피시실</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/multicore/ title=./wiki/lectures/multicore/>Multicore Computing</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/ns3/ title=./wiki/lectures/ns3/>Network Simulator 3</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/nonparametric-statistic/ title=./wiki/lectures/nonparametric-statistic/>비모수 통계학</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/wireless/ title=./wiki/lectures/wireless/>wireless 무선이동통신 수업</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/database_system/ title=./wiki/lectures/database_system/>Database System</a></li><li class=file><a href=https://minuk.dev/wiki/%EC%B9%B4%EC%B9%B4%EC%98%A4%EC%9B%8C%ED%81%AC/ title=./wiki/%EC%B9%B4%EC%B9%B4%EC%98%A4%EC%9B%8C%ED%81%AC/>카카오워크</a></li><li class=file><a href=https://minuk.dev/wiki/soma/ title=./wiki/soma/>소프트웨어 마에스트로</a></li><li class=file><a href=https://minuk.dev/wiki/linux_kakaotalk/ title=./wiki/linux_kakaotalk/>리눅스 카카오톡</a></li><li class=file><a href=https://minuk.dev/wiki/latina/ title=./wiki/latina/>라틴어</a></li><li class=file><a href=https://minuk.dev/wiki/blk-mq/ title=./wiki/blk-mq/>Multi-Queue Block IO Queueing (blk-mq)</a></li><li class=file><a href=https://minuk.dev/wiki/linux-study/ title=./wiki/linux-study/>linux-study</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2020-12-20/ title=./wiki/%ED%9A%8C%EA%B3%A0/2020-12-20/>2020-12-20 회고</a></li><li class=file><a href=https://minuk.dev/wiki/linux-debug/scheduling/ title=./wiki/linux-debug/scheduling/>linux-debug/scheduling</a></li><li class=file><a href=https://minuk.dev/wiki/linux-debug/synchronization/ title=./wiki/linux-debug/synchronization/>linux-debug/synchronization</a></li><li class=file><a href=https://minuk.dev/wiki/linux-debug/timer/ title=./wiki/linux-debug/timer/>linux-debug/timer</a></li><li class=file><a href=https://minuk.dev/wiki/linux-debug/workqueue/ title=./wiki/linux-debug/workqueue/>linux-debug/workqueue</a></li><li class=file><a href=https://minuk.dev/wiki/linux-debug/interrupt/ title=./wiki/linux-debug/interrupt/>linux-debug/interrupt</a></li><li class=file><a href=https://minuk.dev/wiki/linux-debug/process/ title=./wiki/linux-debug/process/>linux-debug/process</a></li><li class=file><a href=https://minuk.dev/wiki/input-method/ title=./wiki/input-method/>linux input-method 삽질</a></li><li class=file><a href=https://minuk.dev/wiki/assembly/ title=./wiki/assembly/>assembly</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2020-10-24/ title=./wiki/%ED%9A%8C%EA%B3%A0/2020-10-24/>2020-10-24 회고</a></li><li class=file><a href=https://minuk.dev/wiki/contextmenu/ title=./wiki/contextmenu/>contextmenu</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2020-10-09/ title=./wiki/%ED%9A%8C%EA%B3%A0/2020-10-09/>2020-10-09 회고</a></li><li class=file><a href=https://minuk.dev/wiki/others/ title=./wiki/others/>others</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2020-09-18/ title=./wiki/%ED%9A%8C%EA%B3%A0/2020-09-18/>2020-09-18 회고</a></li><li class=file><a href=https://minuk.dev/wiki/seccomp/ title=./wiki/seccomp/>seccomp</a></li><li class=file><a href=https://minuk.dev/wiki/debug-linux/ title=./wiki/debug-linux/>디버깅을 통해 배우는 리눅스 커널의 구조와 원리</a></li><li class=file><a href=https://minuk.dev/wiki/fuse/ title=./wiki/fuse/>Filesystem in Userspace</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2020-08-30/ title=./wiki/%ED%9A%8C%EA%B3%A0/2020-08-30/>회고/2020-08-30</a></li><li class=file><a href=https://minuk.dev/wiki/raid/ title=./wiki/raid/>RAID(Redundant Array of Independent Disks)</a></li><li class=file><a href=https://minuk.dev/wiki/storage/ title=./wiki/storage/>Storage</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2020-08-17/ title=./wiki/%ED%9A%8C%EA%B3%A0/2020-08-17/>2020-08-17 회고</a></li><li class=file><a href=https://minuk.dev/wiki/teamnote/ title=./wiki/teamnote/>teamnote</a></li><li class=file><a href=https://minuk.dev/wiki/git/ title=./wiki/git/>git</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2020-07-31/ title=./wiki/%ED%9A%8C%EA%B3%A0/2020-07-31/>2020년 7월 31일자 회고</a></li><li class=file><a href=https://minuk.dev/wiki/3%EA%B3%B5%EB%85%B8%ED%8A%B8/ title=./wiki/3%EA%B3%B5%EB%85%B8%ED%8A%B8/>3공 노트</a></li><li class=file><a href=https://minuk.dev/wiki/nas/ title=./wiki/nas/>NAS</a></li><li class=file><a href=https://minuk.dev/wiki/lfs/ title=./wiki/lfs/>LFS Paper</a></li><li class=file><a href=https://minuk.dev/wiki/ftl/ title=./wiki/ftl/>Flash Translation Layer</a></li><li class=file><a href=https://minuk.dev/wiki/ppn/ title=./wiki/ppn/>PPN(Physical Page Number)</a></li><li class=file><a href=https://minuk.dev/wiki/lpn/ title=./wiki/lpn/>LPN(Logical Page Number)</a></li><li class=file><a href=https://minuk.dev/wiki/load-balance/ title=./wiki/load-balance/>Load Balance</a></li><li class=file><a href=https://minuk.dev/wiki/cache/ title=./wiki/cache/>Cache</a></li><li class=file><a href=https://minuk.dev/wiki/uart/ title=./wiki/uart/>UART (Universal asynchronous receiver/transmitter)</a></li><li class=file><a href=https://minuk.dev/wiki/vhdci/ title=./wiki/vhdci/>VHDCI (Very-high-dencity cable interconnect)</a></li><li class=file><a href=https://minuk.dev/wiki/boxplot/ title=./wiki/boxplot/>boxplot</a></li><li class=file><a href=https://minuk.dev/wiki/quartile/ title=./wiki/quartile/>quartile (사분위수)</a></li><li class=file><a href=https://minuk.dev/wiki/statistics/ title=./wiki/statistics/>statistics</a></li><li class=file><a href=https://minuk.dev/wiki/fsm/ title=./wiki/fsm/>FSM (Finite State machine)</a></li><li class=file><a href=https://minuk.dev/wiki/open-nvm/ title=./wiki/open-nvm/>open-nvm</a></li><li class=file><a href=https://minuk.dev/wiki/mram/ title=./wiki/mram/>MRAM (Magnetic Random Access Memory)</a></li><li class=file><a href=https://minuk.dev/wiki/file/ title=./wiki/file/>vfs - file</a></li><li class=file><a href=https://minuk.dev/wiki/kiocb/ title=./wiki/kiocb/>kiocb</a></li><li class=file><a href=https://minuk.dev/wiki/vfs/ title=./wiki/vfs/>VFS-Virtual File System</a></li><li class=file><a href=https://minuk.dev/wiki/linux/ title=./wiki/linux/>linux</a></li><li class=file><a href=https://minuk.dev/wiki/f2fs-paper/ title=./wiki/f2fs-paper/>F2FS- A New File System for Flash Storage</a></li><li class=file><a href=https://minuk.dev/wiki/english/proverb/ title=./wiki/english/proverb/>english/proverb</a></li><li class=file><a href=https://minuk.dev/wiki/english/ title=./wiki/english/>english</a></li><li class=file><a href=https://minuk.dev/wiki/tool/ title=./wiki/tool/>tool</a></li><li class=file><a href=https://minuk.dev/wiki/verilog/ title=./wiki/verilog/>verilog (베릴로그)</a></li><li class=file><a href=https://minuk.dev/wiki/iommu/ title=./wiki/iommu/>IOMMU (Input Output Memory Management Unit)</a></li><li class=file><a href=https://minuk.dev/wiki/delayed_work/ title=./wiki/delayed_work/>delayed work</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2020-06-21/ title=./wiki/%ED%9A%8C%EA%B3%A0/2020-06-21/>회고/2020-06-21</a></li><li class=file><a href=https://minuk.dev/wiki/jwt/ title=./wiki/jwt/>json web token(jwt)</a></li><li class=file><a href=https://minuk.dev/wiki/tmuxinator/ title=./wiki/tmuxinator/>tmuxinator</a></li><li class=file><a href=https://minuk.dev/wiki/vim-staritfy/ title=./wiki/vim-staritfy/>vim-startify</a></li><li class=file><a href=https://minuk.dev/wiki/my-page/ title=./wiki/my-page/>my-page (나만의 홈페이지 만들기)</a></li><li class=file><a href=https://minuk.dev/wiki/blk_mq/ title=./wiki/blk_mq/>blk_mq</a></li><li class=file><a href=https://minuk.dev/wiki/prp/ title=./wiki/prp/>PRP (Physical Region Page)</a></li><li class=file><a href=https://minuk.dev/wiki/numa/ title=./wiki/numa/>NUMA</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2020-06-17/ title=./wiki/%ED%9A%8C%EA%B3%A0/2020-06-17/>회고/2020-06-17</a></li><li class=file><a href=https://minuk.dev/wiki/block-layer/ title=./wiki/block-layer/>block layer</a></li><li class=file><a href=https://minuk.dev/wiki/workqueue/ title=./wiki/workqueue/>workqueue</a></li><li class=file><a href=https://minuk.dev/wiki/nvme/ title=./wiki/nvme/>nvme</a></li><li class=file><a href=https://minuk.dev/wiki/gem5/ title=./wiki/gem5/>gem5</a></li><li class=file><a href=https://minuk.dev/wiki/simplessd/ title=./wiki/simplessd/>simple-ssd</a></li><li class=file><a href=https://minuk.dev/wiki/mmap/ title=./wiki/mmap/>mmap</a></li><li class=file><a href=https://minuk.dev/wiki/b+tree/ title=./wiki/b+tree/>B+ Tree</a></li><li class=file><a href=https://minuk.dev/wiki/database/ title=./wiki/database/>Database</a></li><li class=file><a href=https://minuk.dev/wiki/memory-cache-clean/ title=./wiki/memory-cache-clean/>memory cache 비우기 (linux command)</a></li><li class=file><a href=https://minuk.dev/wiki/free/ title=./wiki/free/>free (linux command)</a></li><li class=file><a href=https://minuk.dev/wiki/clflush/ title=./wiki/clflush/>clflush (cache line flush)</a></li><li class=file><a href=https://minuk.dev/wiki/c++/ title=./wiki/c++/>C++ Language</a></li><li class=file><a href=https://minuk.dev/wiki/%EC%82%AC%EC%A7%80%EB%B0%A9/ title=./wiki/%EC%82%AC%EC%A7%80%EB%B0%A9/>사지방</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/3%EC%9B%94/ title=./wiki/%ED%9A%8C%EA%B3%A0/3%EC%9B%94/>2020년 3월 회고</a></li><li class=file><a href=https://minuk.dev/wiki/mysql/ title=./wiki/mysql/>mysql (storage engine)</a></li><li class=file><a href=https://minuk.dev/wiki/block-group/ title=./wiki/block-group/>block group</a></li><li class=file><a href=https://minuk.dev/wiki/journal/ title=./wiki/journal/>journal(journaling)</a></li><li class=file><a href=https://minuk.dev/wiki/group-descriptor-table/ title=./wiki/group-descriptor-table/>group descriptor table</a></li><li class=file><a href=https://minuk.dev/wiki/inode/ title=./wiki/inode/>inode</a></li><li class=file><a href=https://minuk.dev/wiki/superblock/ title=./wiki/superblock/>Superblock</a></li><li class=file><a href=https://minuk.dev/wiki/ext4/ title=./wiki/ext4/>The new ext4 filesystem: current status and future plans</a></li><li class=file><a href=https://minuk.dev/wiki/%EA%B3%84%EB%A3%A1-%EA%B0%9C%EB%B0%9C-%EB%AA%A8%EC%9E%84/ title=./wiki/%EA%B3%84%EB%A3%A1-%EA%B0%9C%EB%B0%9C-%EB%AA%A8%EC%9E%84/>계룡 개발 모임</a></li><li class=file><a href=https://minuk.dev/wiki/ssd/ title=./wiki/ssd/>SSD</a></li><li class=file><a href=https://minuk.dev/wiki/gutentags/ title=./wiki/gutentags/>gutentags</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-18--%EB%A9%94%EB%A9%98%ED%86%A0/ title=./wiki/modern-c++-design-pattern/chapter-18--%EB%A9%94%EB%A9%98%ED%86%A0/>Modern C++ Design Pattern/Chatper 18. 메멘토</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-17--%EB%A7%A4%EA%B0%9C%EC%9E%90/ title=./wiki/modern-c++-design-pattern/chapter-17--%EB%A7%A4%EA%B0%9C%EC%9E%90/>Modern C++ Design Pattern/Chatper 17. 매개자</a></li><li class=file><a href=https://minuk.dev/wiki/boj/ title=./wiki/boj/>boj</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-16--%EB%B0%98%EB%B3%B5%EC%9E%90/ title=./wiki/modern-c++-design-pattern/chapter-16--%EB%B0%98%EB%B3%B5%EC%9E%90/>Modern C++ Design Pattern/Chatper 16. 반복자</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2020-04-20/ title=./wiki/%ED%9A%8C%EA%B3%A0/2020-04-20/>회고/2020.04.20</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-15--%EC%9D%B8%ED%84%B0%ED%94%84%EB%A6%AC%ED%84%B0/ title=./wiki/modern-c++-design-pattern/chapter-15--%EC%9D%B8%ED%84%B0%ED%94%84%EB%A6%AC%ED%84%B0/>Modern C++ Design Pattern/Chatper 15. 인터프리터</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-14--%EC%BB%A4%EB%A7%A8%EB%93%9C/ title=./wiki/modern-c++-design-pattern/chapter-14--%EC%BB%A4%EB%A7%A8%EB%93%9C/>Modern C++ Design Pattern/Chatper 14. 커맨드</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-13--%EC%B1%85%EC%9E%84%EC%82%AC%EC%8A%ACchain-of-responsibility/ title=./wiki/modern-c++-design-pattern/chapter-13--%EC%B1%85%EC%9E%84%EC%82%AC%EC%8A%ACchain-of-responsibility/>Modern C++ Design Pattern/Chatper 13. 책임사슬(Chain of Responsibility)</a></li><li class=file><a href=https://minuk.dev/wiki/coc/ title=./wiki/coc/>coc (vim plugin coc)</a></li><li class=file><a href=https://minuk.dev/wiki/regex/ title=./wiki/regex/>Regular Expression (regex)</a></li><li class=file><a href=https://minuk.dev/wiki/glob/ title=./wiki/glob/>glob</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-12--%ED%94%84%EB%A1%9D%EC%8B%9C/ title=./wiki/modern-c++-design-pattern/chapter-12--%ED%94%84%EB%A1%9D%EC%8B%9C/>Modern C++ Design Pattern/Chapter 12. 프록시</a></li><li class=file><a href=https://minuk.dev/wiki/hugo/ title=./wiki/hugo/>hugo</a></li><li class=file><a href=https://minuk.dev/wiki/tee/ title=./wiki/tee/>tee (Linux Command)</a></li><li class=file><a href=https://minuk.dev/wiki/rm/ title=./wiki/rm/>rm (Linux Command)</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-11--%ED%94%8C%EB%9D%BC%EC%9D%B4%EC%9B%A8%EC%9D%B4%ED%8A%B8/ title=./wiki/modern-c++-design-pattern/chapter-11--%ED%94%8C%EB%9D%BC%EC%9D%B4%EC%9B%A8%EC%9D%B4%ED%8A%B8/>Modern C++ Design Pattern/Chapter 11. 플라이웨이트</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-10--%ED%8D%BC%EC%82%AC%EB%93%9C/ title=./wiki/modern-c++-design-pattern/chapter-10--%ED%8D%BC%EC%82%AC%EB%93%9C/>Modern C++ Design Pattern/Chapter 10. 퍼사드</a></li><li class=file><a href=https://minuk.dev/wiki/effective-debugging/chapter-7--%EC%8B%A4%ED%96%89-%EC%8B%9C%EA%B0%84-%EA%B8%B0%EB%B2%95/ title=./wiki/effective-debugging/chapter-7--%EC%8B%A4%ED%96%89-%EC%8B%9C%EA%B0%84-%EA%B8%B0%EB%B2%95/>Effective Debugging/Chatper 7. 컴파일 시간 기법</a></li><li class=file><a href=https://minuk.dev/wiki/effective-debugging/chapter-8--%EB%A9%80%ED%8B%B0%EC%8A%A4%EB%A0%88%EB%93%9C-%EC%BD%94%EB%93%9C-%EB%94%94%EB%B2%84%EA%B9%85/ title=./wiki/effective-debugging/chapter-8--%EB%A9%80%ED%8B%B0%EC%8A%A4%EB%A0%88%EB%93%9C-%EC%BD%94%EB%93%9C-%EB%94%94%EB%B2%84%EA%B9%85/>Effective Debugging/Chatper 8. 멀티스레드 코드 디버깅</a></li><li class=file><a href=https://minuk.dev/wiki/todo/ title=./wiki/todo/>TODO Lists</a></li><li class=file><a href=https://minuk.dev/wiki/vim/ title=./wiki/vim/>vim</a></li><li class=file><a href=https://minuk.dev/wiki/vimwiki/ title=./wiki/vimwiki/>vimwiki</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2020-04-08/ title=./wiki/%ED%9A%8C%EA%B3%A0/2020-04-08/>회고/2020-04-08</a></li><li class=file><a href=https://minuk.dev/wiki/cloudatcost/ title=./wiki/cloudatcost/>cloudatcost</a></li><li class=file><a href=https://minuk.dev/wiki/web/ title=./wiki/web/>web</a></li><li class=file><a href=https://minuk.dev/wiki/nginx/ title=./wiki/nginx/>nginx</a></li><li class=file><a href=https://minuk.dev/wiki/understanding-linux-kernel/ title=./wiki/understanding-linux-kernel/>Understanding Linux Kernel</a></li><li class=file><a href=https://minuk.dev/wiki/mathematical-statistics/ title=./wiki/mathematical-statistics/>Mathematical Statistics</a></li><li class=file><a href=https://minuk.dev/wiki/effective-debugging/ title=./wiki/effective-debugging/>Effective Debugging</a></li><li class=file><a href=https://minuk.dev/wiki/effective-debugging/chapter-1--%EA%B3%A0%EC%B0%A8%EC%9B%90-%EC%A0%84%EB%9E%B5/ title=./wiki/effective-debugging/chapter-1--%EA%B3%A0%EC%B0%A8%EC%9B%90-%EC%A0%84%EB%9E%B5/>Effective Debugging/Chapter 1. 고차원 전략</a></li><li class=file><a href=https://minuk.dev/wiki/effective-debugging/chapter-2--%EB%B2%94%EC%9A%A9%EC%A0%81%EC%9D%B8-%EB%94%94%EB%B2%84%EA%B9%85-%EA%B8%B0%EB%B2%95/ title=./wiki/effective-debugging/chapter-2--%EB%B2%94%EC%9A%A9%EC%A0%81%EC%9D%B8-%EB%94%94%EB%B2%84%EA%B9%85-%EA%B8%B0%EB%B2%95/>Effective Debugging/Chapter 2. 범용적인 디버깅 기법</a></li><li class=file><a href=https://minuk.dev/wiki/effective-debugging/chapter-3--%EB%B2%94%EC%9A%A9-%EB%8F%84%EA%B5%AC%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%EA%B8%B0%EB%B2%95/ title=./wiki/effective-debugging/chapter-3--%EB%B2%94%EC%9A%A9-%EB%8F%84%EA%B5%AC%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%EA%B8%B0%EB%B2%95/>Effective Debugging/Chapter 3. 범용 도구를 활용한 기법</a></li><li class=file><a href=https://minuk.dev/wiki/effective-debugging/chapter-5--%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EA%B8%B0%EB%B2%95/ title=./wiki/effective-debugging/chapter-5--%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EA%B8%B0%EB%B2%95/>Effective Debugging/Chapter 5. 프로그래밍 기법</a></li><li class=file><a href=https://minuk.dev/wiki/effective-debugging/chatper-4--%EB%94%94%EB%B2%84%EA%B1%B0-%ED%99%9C%EC%9A%A9%EB%B2%95/ title=./wiki/effective-debugging/chatper-4--%EB%94%94%EB%B2%84%EA%B1%B0-%ED%99%9C%EC%9A%A9%EB%B2%95/>Effective Debugging/Chatper 4. 디버거 활용법</a></li><li class=file><a href=https://minuk.dev/wiki/effective-debugging/chatper-6--%EC%BB%B4%ED%8C%8C%EC%9D%BC-%EC%8B%9C%EA%B0%84-%EA%B8%B0%EB%B2%95/ title=./wiki/effective-debugging/chatper-6--%EC%BB%B4%ED%8C%8C%EC%9D%BC-%EC%8B%9C%EA%B0%84-%EA%B8%B0%EB%B2%95/>Effective Debugging/Chatper 6. 컴파일 시간 기법</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/ title=./wiki/modern-c++-design-pattern/>Modern C++ Design Pattern</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-1--%EA%B0%9C%EC%9A%94/ title=./wiki/modern-c++-design-pattern/chapter-1--%EA%B0%9C%EC%9A%94/>Modern C++ Design Pattern/Chatper 1. 개요</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-2--%EB%B9%8C%EB%8D%94/ title=./wiki/modern-c++-design-pattern/chapter-2--%EB%B9%8C%EB%8D%94/>Modern C++ Design Pattern/Chatper 2. 빌더</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-4--%ED%94%84%EB%A1%9C%ED%86%A0%ED%83%80%EC%9E%85/ title=./wiki/modern-c++-design-pattern/chapter-4--%ED%94%84%EB%A1%9C%ED%86%A0%ED%83%80%EC%9E%85/>Modern C++ Design Pattern/Chatper 4. 프로토타입</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-5--%EC%8B%B1%EA%B8%80%ED%84%B4/ title=./wiki/modern-c++-design-pattern/chapter-5--%EC%8B%B1%EA%B8%80%ED%84%B4/>Modern C++ Design Pattern/Chatper 5. 싱글턴</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-6--%EC%96%B4%EB%8C%91%ED%84%B0/ title=./wiki/modern-c++-design-pattern/chapter-6--%EC%96%B4%EB%8C%91%ED%84%B0/>Modern C++ Design Pattern/Chatper 6. 어댑터</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-7--%EB%B8%8C%EB%A6%BF%EC%A7%80/ title=./wiki/modern-c++-design-pattern/chapter-7--%EB%B8%8C%EB%A6%BF%EC%A7%80/>Modern C++ Design Pattern/Chatper 7. 브릿지</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-8--%EC%BB%B4%ED%8F%AC%EC%A7%80%ED%8A%B8/ title=./wiki/modern-c++-design-pattern/chapter-8--%EC%BB%B4%ED%8F%AC%EC%A7%80%ED%8A%B8/>Modern C++ Design Pattern/Chatper 8. 컴포지트</a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-9--%EB%8D%B0%EC%BD%94%EB%A0%88%EC%9D%B4%ED%84%B0/ title=./wiki/modern-c++-design-pattern/chapter-9--%EB%8D%B0%EC%BD%94%EB%A0%88%EC%9D%B4%ED%84%B0/>Modern C++ Design Pattern/Chatper 9. 데코레이터</a></li><li class=file><a href=https://minuk.dev/wiki/book-reviews/ title=./wiki/book-reviews/>Book Review</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/algorithm/ title=./wiki/lectures/algorithm/>lectures/algorithm</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/computer-architecture/ title=./wiki/lectures/computer-architecture/>lectures/computer architecture</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/image-processing/ title=./wiki/lectures/image-processing/>lectures/image processing</a></li><li class=file><a href=https://minuk.dev/wiki/lectures/ title=./wiki/lectures/>학교 수업</a></li><li class=file><a href=https://minuk.dev/wiki/tool-configuration/ title=./wiki/tool-configuration/>Tool configuration</a></li><li class=file><a href=https://minuk.dev/wiki/ssh-server/ title=./wiki/ssh-server/>SSH Server Configuration</a></li><li class=file><a href=https://minuk.dev/wiki/firewall/ title=./wiki/firewall/>Firewall (방화벽) Configuration</a></li><li class=file><a href=https://minuk.dev/wiki/ftp/ title=./wiki/ftp/>ftp server command</a></li><li class=file><a href=https://minuk.dev/wiki/user/ title=./wiki/user/>linux user command</a></li><li class=file><a href=https://minuk.dev/wiki/brightness/ title=./wiki/brightness/>Brightness (화면 밝기 조절) command</a></li><li class=file><a href=https://minuk.dev/wiki/wifi-command-line/ title=./wiki/wifi-command-line/>Wifi commands</a></li><li class=file><a href=https://minuk.dev/wiki/wifi/ title=./wiki/wifi/>Wifi commands</a></li><li class=file><a href=https://minuk.dev/wiki/linux-command/ title=./wiki/linux-command/>Linux Command 모음</a></li><li class=file><a href=https://minuk.dev/wiki/docker/ title=./wiki/docker/>docker</a></li><li class=file><a href=https://minuk.dev/wiki/sql/ title=./wiki/sql/>SQL</a></li><li class=file><a href=https://minuk.dev/wiki/typescript/function/ title=./wiki/typescript/function/>Typescript/Function</a></li><li class=file><a href=https://minuk.dev/wiki/typescript/class/ title=./wiki/typescript/class/>Typescript/Class</a></li><li class=file><a href=https://minuk.dev/wiki/typescript/interface/ title=./wiki/typescript/interface/>Typescript/Interface</a></li><li class=file><a href=https://minuk.dev/wiki/typescript/variable-declaration/ title=./wiki/typescript/variable-declaration/>Typescript/Variable Declaration</a></li><li class=file><a href=https://minuk.dev/wiki/typescript/types/ title=./wiki/typescript/types/>Typescript/Types</a></li><li class=file><a href=https://minuk.dev/wiki/typescript/ title=./wiki/typescript/>Typescript</a></li><li class=file><a href=https://minuk.dev/wiki/graphql-typescript/ title=./wiki/graphql-typescript/>graphql typescript (deprecated)</a></li><li class=file><a href=https://minuk.dev/wiki/winston/ title=./wiki/winston/>winston</a></li><li class=file><a href=https://minuk.dev/wiki/jest/ title=./wiki/jest/>Jest</a></li><li class=file><a href=https://minuk.dev/wiki/sequelize/ title=./wiki/sequelize/>Sequelize</a></li><li class=file><a href=https://minuk.dev/wiki/fetch/ title=./wiki/fetch/>Fetch 문법 간단 정리</a></li><li class=file><a href=https://minuk.dev/wiki/promise/ title=./wiki/promise/>Promise 정리</a></li><li class=file><a href=https://minuk.dev/wiki/javascript/ title=./wiki/javascript/>JavaScript</a></li><li class=file><a href=https://minuk.dev/wiki/nexus/ title=./wiki/nexus/>Nexus</a></li><li class=file><a href=https://minuk.dev/wiki/ssdsolid-state-drive/ title=./wiki/ssdsolid-state-drive/>SSD(Solid-State Drive)</a></li><li class=file><a href=https://minuk.dev/wiki/pintos/ title=./wiki/pintos/>Pintos</a></li><li class=file><a href=https://minuk.dev/wiki/%EB%B2%84%EC%8A%A4-%EC%8B%9C%EA%B0%84-%EB%A9%94%EB%AA%A8/ title=./wiki/%EB%B2%84%EC%8A%A4-%EC%8B%9C%EA%B0%84-%EB%A9%94%EB%AA%A8/>Bus 시간 메모</a></li><li class=file><a href=https://minuk.dev/wiki/5-articles-per-week/ title=./wiki/5-articles-per-week/>5 articles per week</a></li><li class=file><a href=https://minuk.dev/wiki/%EC%8D%A9%EC%96%B4%EB%B2%84%EB%A6%B0-query-language/ title=./wiki/%EC%8D%A9%EC%96%B4%EB%B2%84%EB%A6%B0-query-language/>썩어버린 Query Language</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/ title=./wiki/%ED%9A%8C%EA%B3%A0/>회고 모음</a></li><li class=file><a href=https://minuk.dev/wiki/memory_leak/ title=./wiki/memory_leak/>Javascript Memory Leak</a></li><li class=file><a href=https://minuk.dev/wiki/%EC%86%A1%ED%8E%B8%EB%8C%80%ED%9A%8C/ title=./wiki/%EC%86%A1%ED%8E%B8%EB%8C%80%ED%9A%8C/>송편 생성기 (추석 대회)</a></li><li class=file><a href=https://minuk.dev/wiki/endurable_transient_inconsistency_in_byte_addressable_persistent_b+-tree/ title=./wiki/endurable_transient_inconsistency_in_byte_addressable_persistent_b+-tree/>Endurable Transient Inconsistency in Byte Addressable Persistent B+-Tree</a></li><li class=file><a href=https://minuk.dev/wiki/ssd-%EA%B3%B5%EB%B6%80%EC%9E%90%EB%A3%8C-%EB%AA%A8%EC%9D%8C/ title=./wiki/ssd-%EA%B3%B5%EB%B6%80%EC%9E%90%EB%A3%8C-%EB%AA%A8%EC%9D%8C/>SSD 공부 자료 모음</a></li><li class=file><a href=https://minuk.dev/wiki/%EA%B0%9C%EB%B0%9C_todo/ title=./wiki/%EA%B0%9C%EB%B0%9C_todo/>개발 TODO</a></li><li class=file><a href=https://minuk.dev/wiki/f2fs/ title=./wiki/f2fs/>F2FS</a></li><li class=file><a href=https://minuk.dev/wiki/%EC%84%A4%EB%8C%80%ED%9A%8C/ title=./wiki/%EC%84%A4%EB%8C%80%ED%9A%8C/>설대회</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2020-01-01/ title=./wiki/%ED%9A%8C%EA%B3%A0/2020-01-01/>회고/2020-01-01</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2020-01-17/ title=./wiki/%ED%9A%8C%EA%B3%A0/2020-01-17/>회고/2020-01-17</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2019-11-24/ title=./wiki/%ED%9A%8C%EA%B3%A0/2019-11-24/>회고/2019.11.24</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2019-10-19/ title=./wiki/%ED%9A%8C%EA%B3%A0/2019-10-19/>회고/2019.10.19</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2019-09-19/ title=./wiki/%ED%9A%8C%EA%B3%A0/2019-09-19/>회고/2019.09.19</a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2019-09-18/ title=./wiki/%ED%9A%8C%EA%B3%A0/2019-09-18/>회고/2019.09.18</a></li><li class=file><a href=https://minuk.dev/wiki/2024-12-23/ title=./wiki/2024-12-23/></a></li><li class=file><a href=https://minuk.dev/wiki/2025-01-05/ title=./wiki/2025-01-05/></a></li><li class=file><a href=https://minuk.dev/wiki/daily/2025-01-10/ title=./wiki/daily/2025-01-10/></a></li><li class=file><a href=https://minuk.dev/wiki/daily/drawing-2025-01-05-06.04.41.excalidraw/ title=./wiki/daily/drawing-2025-01-05-06.04.41.excalidraw/></a></li><li class=file><a href=https://minuk.dev/wiki/daily/drawing-2025-01-05-06.04.49.excalidraw/ title=./wiki/daily/drawing-2025-01-05-06.04.49.excalidraw/></a></li><li class=file><a href=https://minuk.dev/wiki/daily/drawing-2025-01-05-06.04.59.excalidraw/ title=./wiki/daily/drawing-2025-01-05-06.04.59.excalidraw/></a></li><li class=file><a href=https://minuk.dev/wiki/drawings/drawing-2025-01-05-05.12.23.excalidraw/ title=./wiki/drawings/drawing-2025-01-05-05.12.23.excalidraw/></a></li><li class=file><a href=https://minuk.dev/wiki/drawings/drawing-2025-01-05-05.54.32.excalidraw/ title=./wiki/drawings/drawing-2025-01-05-05.54.32.excalidraw/></a></li><li class=file><a href=https://minuk.dev/wiki/drawings/drawing-2025-01-13-00.26.22.excalidraw/ title=./wiki/drawings/drawing-2025-01-13-00.26.22.excalidraw/></a></li><li class=file><a href=https://minuk.dev/wiki/excalidraw/test-draw/ title=./wiki/excalidraw/test-draw/></a></li><li class=file><a href=https://minuk.dev/wiki/ipad/ title=./wiki/ipad/></a></li><li class=file><a href=https://minuk.dev/wiki/lectures/compiler/ title=./wiki/lectures/compiler/></a></li><li class=file><a href=https://minuk.dev/wiki/lectures/signal_and_system/ title=./wiki/lectures/signal_and_system/></a></li><li class=file><a href=https://minuk.dev/wiki/macos-initialization/ title=./wiki/macos-initialization/></a></li><li class=file><a href=https://minuk.dev/wiki/modern-c++-design-pattern/chapter-3--%ED%8C%A9%ED%86%A0%EB%A6%AC/ title=./wiki/modern-c++-design-pattern/chapter-3--%ED%8C%A9%ED%86%A0%EB%A6%AC/></a></li><li class=file><a href=https://minuk.dev/wiki/test.excalidraw/ title=./wiki/test.excalidraw/></a></li><li class=file><a href=https://minuk.dev/wiki/test/ title=./wiki/test/></a></li><li class=file><a href=https://minuk.dev/wiki/ucpc_2018_%EC%98%88%EC%84%A0/ title=./wiki/ucpc_2018_%EC%98%88%EC%84%A0/></a></li><li class=file><a href=https://minuk.dev/wiki/%EB%8D%B0%EC%A4%91%EC%96%B4%EC%84%A4/ title=./wiki/%EB%8D%B0%EC%A4%91%EC%96%B4%EC%84%A4/></a></li><li class=file><a href=https://minuk.dev/wiki/%EC%9D%98%EA%B2%AC-%EB%A9%94%EB%AA%A8/ title=./wiki/%EC%9D%98%EA%B2%AC-%EB%A9%94%EB%AA%A8/></a></li><li class=file><a href=https://minuk.dev/wiki/%EC%A3%BC%EC%9E%A5-%EB%A9%94%EB%AA%A8/ title=./wiki/%EC%A3%BC%EC%9E%A5-%EB%A9%94%EB%AA%A8/></a></li><li class=file><a href=https://minuk.dev/wiki/%ED%82%A4%EC%9B%8C%EB%93%9C-%EB%A9%94%EB%AA%A8/ title=./wiki/%ED%82%A4%EC%9B%8C%EB%93%9C-%EB%A9%94%EB%AA%A8/></a></li><li class=file><a href=https://minuk.dev/wiki/%ED%85%8C%EC%8A%A4%ED%8A%B8%EC%9A%A9/ title=./wiki/%ED%85%8C%EC%8A%A4%ED%8A%B8%EC%9A%A9/></a></li><li class=file><a href=https://minuk.dev/wiki/%ED%9A%8C%EA%B3%A0/2020-05-30/ title=./wiki/%ED%9A%8C%EA%B3%A0/2020-05-30/></a></li><li class=file><a href=https://minuk.dev/wiki/tags/ title=./wiki/tags/>tag page</a></li></ul></li><li class=file><a href=https://minuk.dev/about/ title=./about/>about</a></li></ul></aside><aside class=exapandable></aside><article class=main><button class=sidebar-toggle-btn type=menu aria-expanded=false aria-haspopup=true>
<i class="bi bi-list"></i></button><div class=title><h1 class=title-header>Regression Analysis</h1></div><a href=https://github.com/minuk-dev/minuk-dev.github.io/blame/master/content/wiki/lectures/regression.md><h5>created : Sun, 06 Jun 2021 14:56:14 +0900</h5><h5>modified : Sun, 20 Jun 2021 23:05:24 +0900</h5></a><div class=article-meta><div class="breadcumb content"><i class="bi bi-folder"></i>
Front Page
[[lectures]]</div></div><div class=list-terms><ul><i class="bi bi-tags" title=Tags></i>
<a href=/tags/statistics class=tag-btn>statistics</a>
<a href=/tags/lectures class=tag-btn>lectures</a></ul></div><aside class=navbar id=nav-toc style=text-align:left><nav id=TableOfContents><ul><li><a href=#chapter-1-introduction>Chapter 1. Introduction</a><ul><li><a href=#11-what-is-regression-analysis>1.1 What is regression analysis?</a></li><li><a href=#12-hisotry>1.2 Hisotry</a></li><li><a href=#14-procedures-of-regression-analysis>1.4 Procedures of regression analysis</a></li></ul></li><li><a href=#chapter-2-corrleation-analysis-and-simple-linear-regression>Chapter 2. Corrleation analysis and simple linear regression</a><ul><li><a href=#21-covariance-and-correlation>2.1 Covariance and correlation</a></li><li><a href=#anscombes-quartet>Anscombe&rsquo;s quartet</a></li><li><a href=#22-simple-linear-regression>2.2 Simple linear regression</a></li><li><a href=#23-least-squares-estimation-lse>2.3 Least Squares Estimation (LSE)</a></li><li><a href=#24-properties-of-the-lse>2.4 Properties of the LSE</a></li><li><a href=#25-quality-of-fit>2.5 Quality of fit</a></li><li><a href=#26-simple-linear-regression-with-no-intercept>2.6 Simple linear regression with no intercept</a></li><li><a href=#27-trivial-regression-and-one-sample-t-test>2.7 Trivial regression and one sample t test</a></li><li><a href=#28-hypothesis-tests-about-a-population-correlation-coefficient>2.8 Hypothesis tests about a population correlation coefficient</a></li><li><a href=#appendix>Appendix</a></li></ul></li><li><a href=#chatper-3-multiple-linear-regression>Chatper 3. Multiple linear regression</a><ul><li><a href=#chatper-31-parameter-estimation-least-squares-estimation-lse>Chatper 3.1 Parameter Estimation: Least Squares Estimation (LSE)</a></li><li><a href=#chapter-32-interpretation-of-the-regression-coefficients>Chapter 3.2 Interpretation of the regression coefficients</a></li><li><a href=#chapter-33-centering-and-scaling>Chapter 3.3. Centering and scaling</a></li><li><a href=#chapter-34-properties-of-lses>Chapter 3.4 Properties of LSEs</a></li><li><a href=#chapter-35-multiple-correlation-coefficient>Chapter 3.5 Multiple correlation coefficient</a></li><li><a href=#chapter-36-inference-for-individual-regression-coefficients>Chapter 3.6 Inference for individual regression coefficients</a></li><li><a href=#chatper-37-tests-of-hypotheses-in-a-linear-model>Chatper 3.7 Tests of hypotheses in a linear model</a></li></ul></li><li><a href=#chatper-4-diagnostics>Chatper 4. Diagnostics</a><ul><li><a href=#41-standard-regression-assumptions>4.1 Standard Regression Assumptions</a></li><li><a href=#42-residuals>4.2 Residuals</a></li><li><a href=#43-graphical-methods>4.3 Graphical Methods</a></li><li><a href=#44-leverages-outliers-influence>4.4 Leverages, Outliers Influence</a></li><li><a href=#45-added-variable-av-plot-and-residual-plus-component-rpc-plot>4.5 Added-variable (AV) plot and residual plus component (RPC) plot</a></li></ul></li><li><a href=#chapter-5-regression-analysis-with-qualitative-explanatory-variables>Chapter 5. Regression analysis with qualitative explanatory variables</a><ul><li><a href=#51-introduction>5.1 Introduction</a></li><li><a href=#52-interactions>5.2 Interactions</a></li><li><a href=#53-equal-slopes-and-unequal-intercepts>5.3 Equal slopes and unequal intercepts</a></li><li><a href=#54-unequal-slopes-and-unequal-intercepts>5.4 Unequal slopes and unequal intercepts</a></li><li><a href=#55-seasonality>5.5 Seasonality</a></li></ul></li><li><a href=#chapter-6-transformations>Chapter 6. Transformations</a><ul><li><a href=#61-transformations-for-lienarity>6.1 Transformations for lienarity</a></li><li><a href=#62-detection-of-heterogeneity>6.2 Detection of heterogeneity</a></li><li><a href=#63-variance-stabilizing-transformations>6.3 Variance stabilizing transformations</a></li><li><a href=#64-weighted-least-squares-wls>6.4 Weighted Least Squares (WLS)</a></li><li><a href=#65-box-cox-power-transformations>6.5 Box-Cox power transformations</a></li></ul></li><li><a href=#chatper-7-weighted-least-squares>Chatper 7. Weighted Least Squares</a></li><li><a href=#chapter-8-correlated-errors>Chapter 8. Correlated errors</a><ul><li><a href=#81-runs-test>8.1 Runs Test</a></li><li><a href=#82-durbin-watson-test>8.2 Durbin-Watson test</a></li><li><a href=#83-transformation-to-remove-autocorrelation-cochrane-and-orcutt-1949>8.3 Transformation to remove autocorrelation (Cochrane and Orcutt, 1949)</a></li><li><a href=#84-autocorrelation-and-missing-predictors>8.4 Autocorrelation and missing predictors</a></li><li><a href=#85-seasonality-and-dummy-variables>8.5 Seasonality and dummy variables</a></li></ul></li><li><a href=#chatper-9-multicollinearity>Chatper 9 Multicollinearity</a><ul><li><a href=#91>9.1</a></li><li><a href=#92-detection-of-multicollinearity>9.2 Detection of multicollinearity</a></li><li><a href=#summary-of-chapter-9>Summary of Chapter 9</a></li></ul></li><li><a href=#chatper-10-methods-for-data-with-multicollinearity>Chatper 10. Methods for data with multicollinearity</a><ul><li><a href=#101-principal-components>10.1 Principal components</a></li><li><a href=#102-recovering-the-regression-coefficients-of-the-original-variables>10.2 Recovering the regression coefficients of the original variables</a></li><li><a href=#103-principal-component-regression-dimension-reduction>10.3 Principal component regression (Dimension reduction)</a></li><li><a href=#104-ridge-regression>10.4 Ridge regression</a></li><li><a href=#105-least-absolute-shrinkage-and-selection-operator-lasso>10.5 Least Absolute Shrinkage and Selection Operator (LASSO)</a></li></ul></li><li><a href=#chapter-11-variable-selections>Chapter 11 Variable selections</a><ul><li><a href=#111-why-do-we-need-variable-selections>11.1 Why do we need variable selections?</a></li><li><a href=#112-effects-of-variable-selections>11.2 Effects of variable selections</a></li><li><a href=#appendix-effects-of-incorrect-model-specifications>Appendix: Effects of Incorrect Model Specifications</a></li><li><a href=#113-practical-issues-in-variable-selections>11.3 Practical issues in variable selections</a></li><li><a href=#114-forward-backward-stepwise-selection>11.4 Forward, backward, stepwise selection</a></li><li><a href=#115-best-subset-selection-regression>11.5 Best subset selection (regression)</a></li><li><a href=#116-criteria>11.6 Criteria</a></li><li><a href=#118-variable-selections-with-multicollinear-data>11.8 Variable selections with multicollinear data</a></li></ul></li></ul></nav></aside><div class=content><h2 id=chapter-1-introduction>Chapter 1. Introduction</h2><h3 id=11-what-is-regression-analysis>1.1 What is regression analysis?</h3><ul><li>Regression analysis a method ofr investigating functional relationships among variables.</li><li>Relationship is expressed in the form of an equiation or a model connecting the response or dependent variable and one or more explanatory or predictor variables.</li><li>$ Y = f(X_1, \cdots, X_p) + \epsilon $:<ul><li>where $\epsilon$ is a random error and the funciton $f$ describes the relationship between $Y$ and $X_1, \cdots, X_p$</li></ul></li><li>$Y$ is called a response (output, or dependent) variable.</li><li>$X_1, \cdots, X_p$ are called predictors (explanatory, independent, input) variables, regressors, covariates, factors or carriers.</li><li>When $f(X_1, \cdots, X_p) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p$, it is called a linear regression model:<ul><li>$ Y = \beta_0 + \beta_1 X_1 + \cdots \beta_p X_p + \epsilon$</li><li>where $\beta_0, \beta_1, \cdots, \beta_p$ are called the regression coefficients (parameters) to be unknown and estimated.</li></ul></li></ul><h3 id=12-hisotry>1.2 Hisotry</h3><ul><li>Least squares estimation (LSE) to fit a traight line to 2-dimensional data:<ul><li>Legendre(1805), Gauss(1809)</li><li>-> Laplace(1810) : CLT and connecting LS method with normality</li><li>-> Gauss (1822): first part of Gauss-Markov theorem</li></ul></li><li>The term of &ldquo;regression&rdquo; was coined by F. Galton:<ul><li>Francis Galton (1886), &ldquo;Regression towards mediocrity in hereditary stature&rdquo;. The journal of the Anthropological Institute of Great Britain and Ireland 15</li><li>Galton conclude in the paper &ldquo;it is that the height-deviate of the offspring is, on the average, two-thirds the height-deviate of tis mid-perentage.&rdquo;</li></ul></li><li>Yule(1892) and K.Pearson (1903) theoretically formulated the regression analysis for a bivariate normal ditribution.</li><li>RA Fisher (1922, 1925) released the assumption of the bivariate normality to the conditional normality of response given predictor.</li><li>George E.P.Box (1979) in his Response Surface Methodology book:<ul><li>&ldquo;Essentially, all models are wrong, but some are useful.&rdquo;</li></ul></li></ul><h3 id=14-procedures-of-regression-analysis>1.4 Procedures of regression analysis</h3><ol><li>Statement of the problem</li><li>Selection of potentially relevant variables</li><li>Experimental design and data colelciton</li><li>Model specification</li><li>Choice of fittin model</li><li>Model fitting</li><li>Model validation and criticism</li><li>Using the model for the intended purpose</li></ol><h4 id=various-models>Various Models</h4><ul><li>Univariate : Only one quantative response variable</li><li>Multivariate : Two or more quantitative response variables</li><li>Simple : Only one predictor variable</li><li>Multiple : two or mor predictor variables</li><li>Linear : All parameters enter the equation linearly, possibly after transformation of the data</li><li>Nonlinear : The relationship between the resposne and some of the predictors is nonlinear or some of the parameters appear nonlinearly, but no transformation is possible to make the parameters appear linearly</li><li>Analysis of variance : All predictors are qualitative variable</li><li>Analysis of covariance : Some predictors are quantitative variables and others are qualitative variables</li><li>Logistic : The response variable is qualitative</li></ul><h2 id=chapter-2-corrleation-analysis-and-simple-linear-regression>Chapter 2. Corrleation analysis and simple linear regression</h2><h3 id=21-covariance-and-correlation>2.1 Covariance and correlation</h3><ul><li>Let $X$ and $Y$ be RVs defined on a population of interest. The vocariance of $X$ and $Y$ is defined by:<ul><li>$Cov(X,Y)=E{(X-EX)(Y-EY)} = E(XY) - (EX)(EY)$</li></ul></li><li>The sample covariance is the unbaised estimator of $Cov(X,Y)$ and defined as:<ul><li>$\hat {Cov}(X,Y) = \frac{1}{n-1}\sum_{i=1}^n (y_i - \bar y) (x_i - \bar x)$</li></ul></li><li>The (population) correlation coefficient is defined by:<ul><li>$\rho = \rho_{xy} = Cor(X,Y) = Cov(\frac{X-EX}{\sigma_X}, \frac{Y - EY}{\sigma_Y})$</li></ul></li><li>The sample correlation coefficient is defined by:<ul><li>$\gamma = \gamma_{xy} = \hat {Cor}(X, Y) = \frac{\sum_{i=1}^n (x_i - \bar x)(y_i - \bar y)}{\sqrt{(\sum_{i=1}^n(x_i - \bar x)^2 (\sum_{i=1}^n (y_i - \bar y)^2)}} \\ = \frac{1}{n-1}\sum_{i=1}^n \frac{x_i - \bar x}{s_x} \frac{y_i - \bar y)}{s_y}$</li></ul></li></ul><h3 id=anscombes-quartet>Anscombe&rsquo;s quartet</h3><ul><li>What is the lesson from Anscombe&rsquo;s quartet? The quartet illustration the importance of looking at a set of data graphically before staring to analyze according to a particular type of relationship, and the inadequancy of basic statistic properties for describing relistic datasets.</li><li>A single extern observation (outlier) may influence the correlation analysis or linear model too much and distort the result.</li></ul><h3 id=22-simple-linear-regression>2.2 Simple linear regression</h3><ul><li>$Y = \beta_0 + \beta_1 X + \epsilon$</li><li>We call $\beta_0$ and $\beta_1$ the regression coefficients (parameters).</li></ul><h3 id=23-least-squares-estimation-lse>2.3 Least Squares Estimation (LSE)</h3><ul><li>Sum of squares of the vertical distance:<ul><li>$S(\beta_0, \beta_1) = \sum_{i=1}^n \epsilon_i^2 \\ = \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2$</li></ul></li><li>By differentiating in $\beta_0$ and $\beta_1$, the LSEs are obtained as follows:<ul><li>$\hat \beta_1 = \frac{\sum_{i=1}^n (x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^n(x_i - \bar x)^2} = \frac{S_{xy}}{S_{xx}}$</li><li>$\hat \beta_0 = \bar y - \hat \beta_1 \bar x$</li></ul></li><li>Relation between the correlation coefficient and the regression coefficient:<ul><li>$\beta_1 = \gamma_{xy} \frac{s_y}{s_x}$</li></ul></li></ul><h3 id=24-properties-of-the-lse>2.4 Properties of the LSE</h3><ul><li><p>Assume $E \epsilon_i = 0, Var(\epsilon_i) = \sigma^2$, and they are independent.:</p><ul><li>$E \hat \beta_0 = \beta_0$</li><li>$E \hat \beta_1 = \beta_1$</li><li>$Var(\hat \beta_0) = \sigma^2 [\frac{1}{n} + \frac{\bar x ^2}{\sum_{i=1}^n (x_i - \bar x)^2}] = \sigma^2 [\frac{1}{n} + \frac{\bar x^2}{S_{xx}}]$</li><li>$Var(\hat \beta_1) = \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar x)^2 = \frac{\sigma^2}{S_{xx}}}$</li><li>$Cov(\hat \beta_0, \hat \beta_1) = - \frac{\bar x}{S_xx} \sigma^2$</li></ul></li><li><p>Assume $\epsilon_i \sim^{i.i.d} N(0, \sigma^2)$:</p><ul><li>$\hat \beta_0 \sim N(\beta_0, \sigma^2 [\frac{1}{n} + \frac{\bar x^2}{\sum_{i=1}^n (x_i - \bar x)^2}])$</li><li>$\hat \beta_1 \sim N(\beta_1, \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar x)^2})$</li></ul></li><li><p>How to estimate (or get) $\sigma$?:</p><ul><li>If we have a priori knowledge about $\sigma$ from previous studies, then we may use the value of $\sigma$.</li><li>Otherwise, estimate it:<ul><li>$\hat \sigma^2 = \frac{sum_{i=1}^n e_i^2}{n-2} = \frac{\sum_{i=1}^n (y_i - \hat y_i)^2}{n-2} = \frac{SSE}{n-2} = MSE$</li><li>The estimator $\hat \sigma^2$ is unbiased for $\sigma^2$, that is, $E \hat \sigma^2 = \sigma^2$</li><li>Without proof, $\frac{SSE}{\sigma^2} \sim \chi_{n-2}^2$ if $\epsilon_i \sim^{i.i.d} N(0, \sigma^2)$</li></ul></li></ul></li><li><p>Standard error: the estimate of the standard deviation is called the standard error:</p><ul><li>$s.e.(\hat \beta_0) = \hat \sigma \sqrt{\frac{1}{n} + \frac{\bar x^2}{\sum_{i=1}^n (x_i - \bar x)^2}} \\ = \hat \sigma \sqrt{\frac{1}{n} + \frac{\bar x^2}{S_{xx}}} = \hat \sigma \sqrt{\frac{\sum x_i ^2}{n S_{xx}}}$</li><li>$s.e.(\hat \beta_1) = \frac{\hat \sigma}{\sqrt{\sum_{i=1}^n (x_i - \bar x)^2}} = \hat \sigma \sqrt{\frac{1}{S_{xx}}}$</li></ul></li><li><p>Under normality assumption, e.e., $\epsilon_i \sim^{i.i.d} N(0, \sigma^2)$:</p><ul><li>$\frac{\hat \beta_0 - \beta_0}{\hat \sigma \sqrt{\frac{1}{n} + \frac{\bar x^2}{\sum (x_i - \hat x)^2}}} \sim t_{n-2}$</li><li>$\frac{\hat \beta_1 - \beta_1}{\hat \sigma \frac{1}{\sqrt{\sum (x_i - \bar x)^2}}} \sim t_{n-2}$</li></ul></li><li><p>Hypothesis testing:</p><ul><li><p>$H_0 : \beta_0 = \beta_0^*$</p></li><li><p>Test statistic: $T = \frac{\hat \beta_0 - \beta_0^*}{\hat \sigma \sqrt{\frac{1}{n} + \frac{\bar x^2}{\sum(x_i - \bar x)^2}}} \sim t_{n-2} \text{ under } H_0$</p></li><li><p>Let $t_0$ be the value of the test statistic with the data</p></li><li><p>Calculate the two-sided p-value by $2P(T > | t_0 |)$ and reject H_0 if and only if the p-value &lt;= the level of significance $\alpha$</p></li><li><p>$H_0 : \beta_1 = \beta_1^*$</p></li><li><p>Test statistic: $T = \frac{\hat \beta_1 - \beta_1^*}{\hat \sigma \sqrt{\frac{1}{\sum (x_i - \bar x)^2}}} \sim t_{n-2} \text{ under } H_0$</p></li><li><p>Let $t_1$ be the observed value of the test statistic from the data.</p></li><li><p>Calculate the p-value by $2P(T > | t_1 |)$ and reject $H_0$ if and only if the p-value &lt;= the level of significance $\alpha$</p></li></ul></li><li><p>Confidence intervals:</p><ul><li>$\hat \beta_0 \pm t_{n-2, \alpha / 2} \times s.e.(\hat \beta_0)$</li><li>$\hat \beta_1 \pm t_{n-2, \alpha / 2} \times s.e.(\hat \beta_1)$</li></ul></li><li><p>Prediction and predction intervals:</p><ul><li>Predicted value at $x = x_0$:<ul><li>$\hat y_0 = \hat \beta_0 + \hat \beta_1 x_0$</li></ul></li><li>Prediction interval:<ul><li>$\hat y_0 \pm t_{n-2, \alpha / 2} \times s.e.(\hat y_0)$</li><li>where $s.e.(\hat y_0) = \hat \sigma \sqrt{1 + \frac{1}{n} + \frac{(x_0 - \bar x)^2}{\sum(x_i - \bar x)^2}}$</li></ul></li></ul></li><li><p>Mean response estimate and confidence limits:</p><ul><li>Point estimate of the mean response at $x = x)0$:<ul><li>$\hat \mu_0 = \hat \beta_0 + \hat \beta_1 x_0$</li></ul></li><li>Confidence interval for $\mu_0$:<ul><li>$\hat \mu \pm t_{n-2, \alpha/2} \times s.e.(\hat \mu_0)$</li><li>where $s.e.(\hat \mu_0) = \hat \sigma \sqrt{\frac{1}{n} + \frac{(x_0 - \bar x)^2}{\sum(x_i - \bar x)^2}}$</li></ul></li></ul></li></ul><h3 id=25-quality-of-fit>2.5 Quality of fit</h3><ol><li>(objective) Thre greater t test statistic of $H_0 : \beta_1 = 0$ (or the smaller the p-value) is, the stronger the strength of the linear relationship between X and Y is.</li><li>(subjective) The scatter plot may be used to discover the strength of the linear relationship.</li><li>Examine the scatter plot of Y versus $\hat Y$. The closer the set of points to a straight line, the stronger the linear relationship between Y and X. One can measure the strength of the linear relationship in this graph by computing the correlation coefficient between Y and $\hat Y$,:</li></ol><ul><li>$Cor(Y, \hat Y) = | Cor(Y, X) |$</li></ul><ol start=4><li>Furtuermore, in both simple and multiple regressions, $Cor(Y, \hat Y)$ is related to another useful measure of the quality (goodness) of fit of the linear model to the observed data, that is called the coefficient of determination $R^2$.</li></ol><ul><li><p>Decomposition of the sum of square erros and the coefficient of determination $R^2$:</p><ul><li>$\sum(y_i - \bar y)^2 \text{ (SST) } = \sum (\hat y_i - \bar y)^2 \text{ (SSR) } + \sum (y_i - \hat y_i)^2 \text{ (SSE) }$</li><li>$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST} = [cor(X, Y)]^2 = [cor(\hat Y, Y)]^2$</li><li>$R^2 = \frac{\text{Explained variance}}{\text{Total variance}}$</li><li>$R^2 = \frac{\sum (\hat y_i - \bar y)^2}{\sum {y_i - \bar y)^2}}$</li></ul></li><li><p>The coefficient of determination $R^2$ is a statistical measure of how well the regression line approximates the real data points.</p></li><li><p>$R^2$ does not provide the validty of the regression assumptions</p></li><li><p>We reemphasize that the regression assumptions should be checked before drawing statistical conclusions from the analysis (e.g., conducting tests of hypothesis or constructing confidence or prediction intervals) because the validity of these statistical procedures hinges on the validity of the assumptions.</p></li></ul><h3 id=26-simple-linear-regression-with-no-intercept>2.6 Simple linear regression with no intercept</h3><ul><li>Model:<ul><li>$Y = \beta_1 X + \epsilon$</li></ul></li><li>The LS of $\beta_1$:<ul><li>$\hat \beta_1 = \frac{y_i x_i}{\sum x_i^2}$</li></ul></li><li>Fitted values and residuals:<ul><li>$\hat y_i = \hat \beta_1 x_i$</li><li>$e_i = y_i - \hat y_i$</li></ul></li><li>Note that $\sum e_i \not = 0$</li><li>The standard error of the slope:<ul><li>$s.e.(\hat \beta_1) = \frac{\hat \sigma}{\sqrt{\sum x_i^2}} = \sqrt{\frac{SSE}{(n-1) \sum x_i^2}}$</li><li>The coefficient of determination:<ul><li>$\sum y_i^2 = \sum \hat y_i ^2 + \sum e_i ^2$</li><li>$R^2 = \frac{\sum \hat y_i^2}{\sum y_i^2}$</li></ul></li></ul></li></ul><h3 id=27-trivial-regression-and-one-sample-t-test>2.7 Trivial regression and one sample t test</h3><ul><li>Consider a linear model with zero slope:<ul><li>$Y = \beta_0 + \epsilon$</li></ul></li><li>To test $H_0 : \beta_0 = \mu_0$:<ul><li>$t = \frac{\bar y - \mu_0}{s.e.(\bar y)} = \frac{\bar y - \mu_0}{s_y / \sqrt{n}}$</li></ul></li></ul><h3 id=28-hypothesis-tests-about-a-population-correlation-coefficient>2.8 Hypothesis tests about a population correlation coefficient</h3><ul><li>Consider a bivariate randome variable (X, Y). Let $\rho$ be the population correlation of X and Y. We may perform a hypothesis test $H_0: \rho = 0$ using linear regression analysis when (X, Y) has a bivariate normal distribution.:<ul><li>$t = r \sqrt{\frac{n-2}{1 - r ^2}} \sim t_{n-2} \text { under } H_0$</li><li>$r = \frac{t}{\sqrt{n-2 + t^2}}$</li></ul></li><li>A permutation test or bootstrap method can be used without assuming normality</li><li>Fisher&rsquo;s z-transform can be used under assumption of large sample and normality.:<ul><li>$z(r) = \frac{1}{2} log (\frac{1 + r}{1 - r})$</li><li>$\sqrt{n - 3} (z(r)-z(\rho_0)) \sim ^{asymp} N(0, 1) \text{ under } H_0 : \rho = \rho_0$</li></ul></li><li>Under normality assumption, the exact distribution of the sample correlation coefficient is known</li></ul><h3 id=appendix>Appendix</h3><ul><li><p>(Derivation of the LSEs of the regression coefficients) Consider a simple linear regression model:</p><ul><li>$Y = \beta_0 + \beta x + \epsilon$</li><li>Suppose we have an iid randome sample $(x_1, Y_1), \cdots, (x_n, Y_n)$:<ul><li>$Y_i = \beta_0 + \beta_1 x_i + \epsilon_i$</li></ul></li><li>The unobservable errors are:<ul><li>$\epsilon_i = Y_i - \beta_0 - \beta_1 x_i$</li></ul></li><li>The LSEs are obtained by minimizing the sum of squared erros:<ul><li>$SSE(\beta_0, \beta_1) = \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 x_i)^2$</li></ul></li><li>To find the LSEs, we differentiate $SSE(\beta_0, \beta_1)$ in $\beta_0$ and $\beta_1$ and the equations below are called the normal equations:<ul><li>$\frac{\partial SSE(\beta_0, \beta_1)}{\partial \beta_0} = -2 \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 x_i) = 0$</li><li>$\frac{\partial SSE(\beta_0, \beta_1)}{\partial \beta_1} = -2 \sum_{i=1}^n x_i (Y_i - \beta_0 - \beta_1 x_i) = 0$</li></ul></li><li>The normal equations can be written as:<ul><li>$\sum_{i=1}^n Y_i - n \beta_0 - \sum_{i=1}^n x_i \beta_1 = 0$:<ul><li>This equation is equivalent to $\bar Y = \beta_0 + \beta_1 \bar x$</li></ul></li><li>$\sum_{i=1}^n x_i Y_i - \sum_{i=1}^n x_i \beta_0 - \sum_{i=1}^n x_i^2 \beta_1 = 0$</li></ul></li><li>Substitute $\beta_0 = \bar Y - \beta_1 \bar x$ in the equation:<ul><li>$\sum_{i=1}^n x_i Y_i - \sum_{i=1}^n x_i (\bar Y - \beta_1 \bar x) - \sum_{i=1}^n x_i^2 \beta_1 = 0$</li><li>$\sum_{i=1}^n x_i (Y_i - \bar Y) - \beta_1 \sum_{i=1}^n x_i (x_i - \bar x) = 0$</li><li>$\beta_1 = \frac{\sum_{i=1}^n x_i (Y_i - \bar Y)}{\sum_{i=1}^n x_i (x_i - \bar x)} = \frac{\sum_{i=1}^n (x_i - \bar x) (Y_i - \bar Y)}{\sum_{i=1}^n{(x_i - \bar x)(x_i - \bar x)}} = \frac{S_{xy}}{S_{xx}}$</li></ul></li><li>Hence, the LSEs are:<ul><li>$\hat \beta_ 1 = \frac{S_{xy}}{S_{xx}}$</li><li>$\hat \beta_0 = \bar Y - \hat \beta_1 \bar x$</li></ul></li></ul></li><li><p>(The Expected Values of $\hat \beta_0$ and $\hat \beta_1$) Assume $E \epsilon_i = 0$:</p><ul><li>$E(\hat \beta_1) = E(\frac{\sum_{i=1}^n (x_i - \bar x)(Y_i - \bar Y)}{S_{xx}}) \\ E(\frac{\sum_{i=1}^n(x_i - \bar x)Y_i - \bar Y \sum_{i=1}^n (x_i - \bar x)}{S_{xx}}) \\ = \frac{1}{S_{xx}} \sum_{i=1}^n (x_i - \bar x) EY_i \\ = \frac{1}{S_{xx}} \sum_{i=1}^n (x_i - \bar x)(\beta_0 + \beta_1 x_i + E(\epsilon_i)) \\ = \frac{1}{S_{xx}} \beta_1 \sum_{i=1}^n (x_i - \bar x)^2 \\ = \beta_1$</li><li>$E(\hat \beta_0) = E(\bar Y - \hat \beta_1 \bar x) \\ = \frac{1}{n} \sum_{i=1}^n EY_i - E(\hat \beta_1) \bar x \\ = \frac{1}{n} \sum_{i=1}^n(\beta_0 + \beta_1 x_i + E(\epsilon_i)) - \beta_1 \bar x \\ = \beta_0 + \beta_1 \bar x - \beta_1 \bar x \\ = \beta_0$</li></ul></li><li><p>Both $\hat \beta_0$ and $\hat \beta_1$ are unbiased estimators for $\beta_0$ and $\beta_1$, respectively.</p></li><li><p>(The Variances of $\hat \beta_0$ and $\hat \beta_1$ and the convariance of $\hat \beta_0$ and $\hat \beta_1$) Assume $Var(\epsilon_i) = \sigma^2$ and $Cov(\epsilon_i, \epsilon_j) = 0 \text{ for } i \not = j$:</p><ul><li>$Var(\hat \beta_1) = Var(\frac{\sum_{i=1}^n (x_i - \bar x) Y_i}{S_{xx}}) \\ = \frac{1}{S_{xx}^2} \sum_{i=1}^n (x_i - \bar x)^2 Var(Y_i) \\ = \frac{1}{S_{xx}^2} \sum_{i=1}^n (x_i - \bar x)^2 Var(\epsilon_i) \\ = \frac{\sigma^2}{S_{xx}}$</li><li>$Var(\hat \beta_0) = Var(\bar Y - \hat \beta_1 \bar x) \\ = Var(\bar Y) - 2 \bar x Cov(\bar Y, \hat \beta_1) + \bar x^2 Var(\hat \beta_1) \\ = \frac{1}{n} \sigma^2 - 2 \bar x Cov(\bar Y, \hat \beta_1) + \frac{\bar x^2}{S_{xx}} \sigma^2 \\ = \sigma^2 (\frac{1}{n} + \frac{\bar x^2}{S_{xx}})$</li><li>$Cov(\bar Y, \hat \beta_1) = \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^n \frac{(x_j - \bar x)}{S_{xx}} Cov(Y_i, Y_j) \\ = \frac{1}{n} \sum_{i=1}^n \sum{j=1}^n \frac{(x_j - \bar x)}{S_{xx}} Cov(\epsilon_i, \epsilon_j) \\ = \frac{1}{n} \sum_{i=1}^n \sum_{i=1}^n \frac{(x_i - \bar x)}{S_{xx}} \sigma^2 \\ = \frac{\sigma^2}{n S_{xx}} \sum_{i=1}^n (x_i - \bar x) = 0$</li></ul></li><li><p>By using the above,:</p><ul><li>$Cov(\hat \beta_0, \hat \beta_1) = Cov(\bar Y - \hat \beta_1 \bar x, \hat \beta_1) \\ = Cov(\bar Y, \hat \beta_1) - \bar x Cov(\hat \beta_1, \hat \beta_1) \\ = 0 - \bar x \frac{\sigma^2}{S_{xx}} \\ = - \frac{\bar x}{S_{xx}} \sigma^2$</li></ul></li><li><p>(The expected value of $\hat \sigma^2 = MSE$):</p><ul><li>$E(MSE) = E(\frac{1}{n-2} \sum_{i=1}^n e_i^2) \\ = \frac{1}{n-2} E { \sum_{i=1}^n (Y_i - \hat \beta_0 - \hat \beta_1 x_i)^2} \\ = \frac{1}{n-2} E{ \sum_{i=1}^n ((Y_i - \bar Y) - \hat \beta_1 (x_i - \bar x))^2} \\ = \frac{1}{n-2} E { \sum_{i=1}^n (Y_i - \bar Y)^2 - s \sum_{i=1}^n (x_i - \bar x)(Y_i \bar Y)\hat \beta_1 + \sum_{i=1}^n (x_i - \bar x)^2 \hat \beta_1^2} \\ = \frac{1}{n-2} E { \sum_{i=1}^n (\beta_1(x_i - \bar x) + (\epsilon_i - \bar \epsilon))^2 - S_{xx} \hat \beta_1 ^2} \\ = frac{1}{n-2} { \beta_1 ^2 S_{xx} + 2 \beta_1 \sum_{i=1}^n (x_i - \bar x) E(\epsilon_i) + E| \sum_{i=1}^n (\epsilon_i - \bar \epsilon)^2 | - S_{xx}E(\hat \beta_1^2) } \\ = \frac{1}{n-2} { \beta_1^2 S_{xx} + (n-1) \sigma^2 - S_{xx} (Var \hat \beta_1) + | E(\hat \beta_1) |^2) } \\ = \frac{1}{n-2} { \beta_1 ^2 S_{xx} + (n-1)\sigma^2 - \sigma^2 -\beta_1 ^2 S_{xx} } \\ = \sigma^2$</li></ul></li><li><p>(The coefficient of determination $R^2 = SSR/SST = 1 -SSE/SST$):</p><ul><li>Proof of SST = SSE + SSR where:<ul><li>$SST = \sum(y_i - \bar y)^2$</li><li>$SSE = \sum (y_i - \hat y_i)^2$</li><li>$SSR = \sum (\hat y_i - \bar y)^2$</li><li>$SST = \sum(y_i - \bar y)^2 \\ = \sum (y_i - \hat y_i + \hat y_i - \bar y)^2 \\ = \sum [(y_i - \bar y_i)^2 + (\hat y_i - \bar y)^2 + 2(y_i - \hat y_i)(\hat y_i - \bar y)] \\ = \sum (y_i - \bar y_i)^2 + \sum (\hat y_i - \bar y)^2 + 2 \sum (y_i - \hat y_i)(\hat y_i - \bar y) \\ = SSE + SSR + 2 \sum (y_i - \hat y_i)(\hat y_i - \bar y)$</li><li>Therefore, we need to show the corss-product term vanishes, that is, $\sum(y_i - \hat y_i)(\hat y_i - \bar y) = 0$</li><li>$\sum (y_i - \hat y_i)(\hat y_i - \bar y) = \sum (y_i - \hat \beta_0 - \hat \beta_1 x_i)(\hat \beta_0 + \hat \beta_1 x_i - \hat \beta_0 - \hat \beta_1 \bar x) \\ = \sum (y_i - \hat \beta_0 - \hat \beta_1 x_i) \hat \beta_1 (x_i - \bar x) \\ = \hat \beta_1 \sum y_i (x_i - \bar x) - \hat \beta_0 \hat \beta_1 \sum(x_i - \bar x) - \hat \beta_1^2 \sum x_i(x_i - \bar x) \\ = \hat \beta_1 \sum(y_i - \bar y) (x_i - \bar x) - \beta_1^2 \sum(x_i - \bar x)(x_i - \bar x) \\ = \hat \beta_1 (S_{xy} - \beta_1 S_{xx}) = 0$</li></ul></li></ul></li><li><p>Proof of $R^2 = | Cor(X, Y) |^2 = | Cor(\hat Y, Y) |^2$. In class, we have done $R^2 = | Cor(X, Y) |^2$, henc it&rsquo;s left to show that $|Cor(X, Y) |^2 = | Cor(\hat Y, Y) |^2$. From the definition of the coreelation coefficient,:</p><ul><li>$| Cor(\hat Y, Y) |^2 = \frac{(\sum(\hat y_i - \bar \hat y) (y_i - \bar y))^2}{\sum(\hat y_i - \bar \hat y)^2 \sum(y_i - \bar y)^2} $</li><li>Note that $\bar \hat y = \hat \beta_0 \hat \beta_1 \bar x$:<ul><li>$| Cor (\hat Y, Y) |^2 = \frac{(\sum )\hat y_i - \bar \hat y)(y_i - \bar y))^2}{\sum (\hat y_i - \bar \hat y)^2 \sum(y_i - \bar y)^2} \\ = \frac{(\sum (\hat \beta_0 + \hat \beta_1 x_i - \hat \beta_0 - \hat \beta_1 \bar x)(y_i - \bar y))^2}{\sum(\hat \beta_0 + \hat \beta_1 x_i - \hat \beta_0 - \hat \beta_1 \bar x)^2 \sum(y_i - \bar y)^2} \\ = \frac{(\hat \beta_1 \sum(x_i - \bar x)(y_i - \bar y))^2}{\hat \beta_1 ^2 \sum(x_i - \bar x)^2 \sum(y_i - \bar y)^2} \\ = \frac{(\sum(x_i - \bar x)(y_i - \bar y))^2}{\sum (x_i - \bar x)^2 \sum(y_i - bar y)^2} \\ = | Cor(X, Y) |^2$</li></ul></li></ul></li></ul><h2 id=chatper-3-multiple-linear-regression>Chatper 3. Multiple linear regression</h2><ul><li>Multiple Linear Regression Model: $Y = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} + \epsilon_i, i= 1, \cdots, n$</li><li>Matrix notation: $y = X \beta + \epsilon$</li></ul><h3 id=chatper-31-parameter-estimation-least-squares-estimation-lse>Chatper 3.1 Parameter Estimation: Least Squares Estimation (LSE)</h3><ul><li><p>$\epsilon = y - X \beta$</p></li><li><p>$S(\beta) \epsilon&rsquo; \epsilon = (y - X \beta)&rsquo;(y - X \beta)$</p></li><li><p>Differentiate $S(\beta)$ in $\beta$ to find the minimum point.:</p><ul><li>$\frac{\partial}{\partial \beta} S(\beta) = 2(X&rsquo;X \beta - X&rsquo;y) = 0$</li><li>\hat \beta_{LSE} = (X&rsquo;X)^{-1} X&rsquo;y$</li></ul></li><li><p>The fitted values and residuals:</p><ul><li>$\hat y_i = \hat \beta_0 + \hat \beta_1 x_{i1} + \cdots + \hat \beta_p x_{ip}$</li><li>$e_i = y_i - \hat y_i$</li></ul></li><li><p>In matrix notation,:</p><ul><li>Fitted Vector $\hat y = X \hat \beta = X(X&rsquo;X)^{-1} X&rsquo; y = P_X y$</li><li>Residual Vector $e = y - \hat y = (I - P_X)y$</li></ul></li><li><p>Remark 1. The matrix $P_X = X(X&rsquo;X)^{-1} X&rsquo;$ is called the projection matrix onto the oclumn space of X. It is also known as the hat matrix $H = P_X$</p></li><li><p>Remark 2. Check $P_X^2 = P_X$, that is, it is idempotent. In addition, we can show $(I - P_X)^2 = I - P_X$ as well.</p></li><li><p>The unbiased estimate of the rror variance:</p><ul><li>$\hat \sigma^2 = \frac{\sum_{i=1}^n e_i^2}{n-p-1} = \frac{e&rsquo;e}{n-p-1} = \frac{SSE}{n-p-1}$</li><li>In matrix notation,</li><li>$\hat \sigma^2 = \frac{1}{n-p-1} e&rsquo;e = \frac{1}{n-p-1} y&rsquo;(I-P_X)^2 y = \frac{1}{n-p-1}y&rsquo;(I-P_X)y$</li></ul></li></ul><h3 id=chapter-32-interpretation-of-the-regression-coefficients>Chapter 3.2 Interpretation of the regression coefficients</h3><ul><li><p>Let $Y = \beta_0 + \beta_1 X_1 + \cdots + \beta_j X_j + \cdots + \beta_p X_p + \epsilon$ be a linear regression model to explain the data.</p></li><li><p>The (partial) regression coefficient $\beta_j$ is the increment in the mean response $EY$ when we increase $X_j$ by one unit while all other predictors $X_i (i \not = j)$ are held fixed.</p></li><li><p>The partial regression coefficient $\beta_j$ represents the contribution of $X_j$ to $Y$ after adjuesting for the other predictors</p></li><li><p>Let $e_{Y X_1}$ be the residual vector obtained from the linear regression model $Y = \alpha_0 + \alpha_1 X_1 + \epsilon$ and $e_{X_2 X_1}$ be the residual vector obtained from the regression model $X_2 = \gamma_0 + \gamma_1 X_1 + \epsilon$. Then, the partial regression coefficient $\beta_2$ is equal to the regression coefficient $\delta$ obtained from $e_{Y X_1} = 0 + \delta e_{X_2 X_1} + \epsilon$</p></li></ul><h3 id=chapter-33-centering-and-scaling>Chapter 3.3. Centering and scaling</h3><ul><li>The magnitudes of the regression coefficients in a regression equation depend on the unit of measurements of the variables</li><li>To make the regression coefficients unitless, one may first center and/or scale teh variables before performing the regression computations</li></ul><ol><li>Unit-Length scaling:</li></ol><ul><li>$Z_y = \frac{Y - \bar y}{L_y}$</li><li>$Z_j = \frac{X_j - \bar x_j}{L_j}$</li><li>where $L_j = \sqrt{\sum_{i=1}^n (y_i - \bar y)^2}$ and $L_j = \sqrt{\sum_{i=1}^n (x_{ij} - \bar x_j)^2}$</li></ul><ol start=2><li>Standardizing:</li></ol><ul><li>$\hat Y = \frac{Y - \bar y}{s_y}$</li><li>$\hat X_j = \frac{X_j - \bar x_j}{s_j}$</li></ul><h3 id=chapter-34-properties-of-lses>Chapter 3.4 Properties of LSEs</h3><ul><li>(Gauss-Markov theorem) LSE = BLUE</li><li>Assume $\epsilon_i \sim^{iid} N(0, \sigma^2)$, that is, $\epsilon \sim N(0, \sigma^2 I)$, and let $C = (X&rsquo;X)^{-1}$:<ul><li>A. $\hat \beta_j \sim N(\beta_j, \sigma^2 C_{jj})$</li><li>B. $\hat \beta \sim N(\beta, \sigma^2 C)$</li><li>C. $SSE/\sigma^2 \sim \chi_{n-p-1}^2$ and $\beta_j$ and $\hat \sigma^2$ are independent.</li></ul></li></ul><h3 id=chapter-35-multiple-correlation-coefficient>Chapter 3.5 Multiple correlation coefficient</h3><ul><li>$R^2 = (cor(Y, \hat Y))^2 = \frac{(\sum(y_i - \bar y)(\hat y_i - \bar y))^2}{\sum (y_i - \bar y)^2 \sum (\hat y_i - \bar y)^2} \\ = \frac{SSR}{SST} = 1 - \frac{SSE}{SST} = 1 - \frac{\sum(y_i - \hat y_i)^2}{\sum (y_i - \bar y)^2}$</li><li>The adjuested $R^2$ is defined to be:<ul><li>$R_{adj}^2 = 1 - \frac{SSE/(n-p-1)}{SST/(n-1)}$</li></ul></li></ul><h3 id=chapter-36-inference-for-individual-regression-coefficients>Chapter 3.6 Inference for individual regression coefficients</h3><ul><li>Test $H_0 : \beta_j = \beta_{j, 0} \text{ versus } H_j : \beta_j \not = \beta_{j, 0}$</li><li>Test statistic:<ul><li>$t_j = frac{\hat \beta_j - \beta_{j,0}}{s.e.(\hat \beta_j)} \sim t_{n - p - 1} \text{ under H_0 }$</li><li>Calculate the p-value by P-value = $p(|t_j|) = 2 Pr(t_{n-p-1} > | t_j |)$ and reject $H_0$ iff the P-value &lt;= $\alpha$</li></ul></li></ul><h3 id=chatper-37-tests-of-hypotheses-in-a-linear-model>Chatper 3.7 Tests of hypotheses in a linear model</h3><ul><li><p>Full model ($M_F$) : $y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} + \epsilon_{i}$ (In matrix form $Y = X \beta + \epsilon$)</p></li><li><p>Reduced model ($M_R$) : a model such as the above with k regression coefficients</p></li><li><p>$\hat y_i$ : the fitted value in the full model</p></li><li><p>$\hat y_i^*$ : the fitted value in the reduced model</p></li><li><p>$SSE(M_F) = \sum(y_i - \hat y_i)^2$ : the sum of squared redisudals in the full model</p></li><li><p>$SSE(M_R) = \sum(y_i - \hat y_i^*)^2$ : the sum of squared residudals in the reduced model</p></li><li><p>Note that $SSE(M_R) \ge SSE(M_F)$:</p><ul><li>$\frac{SSE(M_R)}{\sigma^2} \sim \chi^2_{n-k}$</li><li>$\frac{SSE(M_F)}{\sigma^2} \sim \chi^2_{n-p-1}$</li><li>$\frac{SSE(M_R) - SSE(M_F)}{\sigma^2} \sim \chi^2_{p+1+k}$</li></ul></li><li><p>In addition, $SSE(M_F)$ and $SSE(M_R) - SSE(M_F)$ are independent.</p></li><li><p>Condier a ypothesis test $H_0$ : reduced model is adequate versus $H_1$ : Full model is adequate.:</p><ul><li>$F= \frac{\frac{SSE(M_R) - SSE(M_F)}{\sigma^2} / (p+ 1 - k)}{\frac{SSE(M_F)}{\sigma^2} / (n-p-1)} \\ = \frac{[SSE(M_R) - SSE(M_F)] / (p+1-k)}{SSE(M_F) / (n-p-1)} \sim F_{p+1 - k, n-p-1} \text{ under } H_0$</li></ul></li><li><p>Suppose the reduced model has q= k -1 predictors and the coefficient of determination $R_q^2$ whereas teh full model has p predictors and the coefficient of determination $R_p^2$. Then,:</p><ul><li>$R_p^2 = 1 - \frac{SSE(M_F)}{SST}$</li><li>$SSE(M_F) = SST[1-R_p^2]$</li><li>Similarly, $SSE(M_R) = SST(1 - R_q^2)$</li><li>Therefore, the F statistic is given by:<ul><li>$F = \frac{(R_p^2 - R_q^2) / (p-q)}{(1 - R_p^2) / (n -p - 1)} \sim F_{p-q, n-p-1}$</li></ul></li><li>More generally, let $M_F$ be the model including m independent parameters and $M_R$ be a submodel in the full model $M_F$ consisting of k independent parameters:<ul><li>$F = \frac{\frac{SSE(M_R) - SSE(M_F)}{\sigma^2} / (m-k)}{\frac{SSE(M_F)}{\sigma^2}/ (n - m)} \\ = frac{[SSE(M_R) - SSE(M_F)] / (m-k)}{SSE(M_F) / (n-m)} \sim F_{m-k, n-m} \text{ under } H_0$</li></ul></li></ul></li></ul><table><thead><tr><th>Source</th><th>SS</th><th>df</th><th>MS</th><th>F</th><th>P-value</th></tr></thead><tbody><tr><td>Regression</td><td>SSR</td><td>p</td><td>MSR = SSR/p</td><td>F = MSR/MSE</td><td>$P(F_{p+1 -k, n-p-1} \ge F)$</td></tr><tr><td>Residudals</td><td>SSE</td><td>n-p-1</td><td>MSE = SSE(n-p-1)</td><td></td><td></td></tr></tbody></table><h2 id=chatper-4-diagnostics>Chatper 4. Diagnostics</h2><ul><li>As seen in the Anscombe data, particularly dataset 3 and 4, one or few observations may too much influence or completely determine the regression line. We need to check whether the fit is overly determined by few observations.</li><li>The confidence or prediction intervals and t-test or anova approach in hypothesis tests in Chapter 3 require normality assumption on the random errors.</li><li>When some of the regression assumptions are violated, the analysis based on the fit may be distorted and they are not reliable.</li><li>Need to check the assumptions made to apply the linear models to fit the datset.</li></ul><h3 id=41-standard-regression-assumptions>4.1 Standard Regression Assumptions</h3><ol><li>Assumption about the form of the model (linearity of Y and X_1, &mldr;, X_p): $Y = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p + \epsilon$</li><li>Assumptions about the random erros: $\epsilon_1, \cdots, \epsilon_n \sim^{iid} N(0, \sigma^2)$:</li><li>Normality assumption</li><li>Mean zero</li><li>Constant variance (homogeneity or homoscedasticity) assumption: when violated, it is called the heterogeity (or heteroscedasticity) problem</li><li>Independent assumption: when violated, it is called the autocorrelation problem</li><li>Assumptions about the predictors:</li><li>Nonrandom</li><li>No measurement erros</li><li>Linearly independent: when violated, it is called the multicollinearity problem</li><li>Assumption about the observations (equally reliable)</li></ol><h3 id=42-residuals>4.2 Residuals</h3><ul><li><p>A simple and effective method ofr detecting model deficiencies in regression analyssis is the examination of residuals</p></li><li><p>Projection or hat matrix : $H = P_X = X(X&rsquo;X)^{-1} X'$</p></li><li><p>Note that $\hat y = P_X y$ and $e = (I - P_X) y$</p></li><li><p>Let $p_{ij}$ be the (i, j) element of the projection matrix. For a simple linear regression,:</p><ul><li>$p_{ij} = \frac{1}{n} + \frac{(x_i - \bar x)(x_j - \bar x)}{\sum (x_i - \bar x)^2}$</li></ul></li><li><p>The diagonal term $h_i = p_{ii}$ is called the leverage value of the i th observation. $p_{ii}$ is denoted by $h_i$ in other literatures. For a simple linear regression,:</p><ul><li>$h_i = p_{ii} = \frac{1}{n} + \frac{(x_i - \bar x)^2}{\sum (x_i - \bar x)^2}$</li></ul></li><li><p>Since $\hat y_i = p_{i1}y_1 + \cdots + p_{ii}y_i + \cdots + p_{in} y_n$, the leverage $p_{ii}$ is the weight (leverage) given to $y_i$ in determining the i th fitted value $\hat y_i$</p></li><li><p>$Var(e_i) = \sigma^2 (1 - p_{ii})$</p></li><li><p>$\hat \sigma^2 = \frac{1}{n - p - 1} \sum_{i=1}^n e_i^2 = \frac{1}{n-p-1} SSE$</p></li><li><p>$\hat \sigma^2_{(i)} = \frac{1}{n - p - 2} SSE_{(i)}$ where $SSE_{(i)}$ is the sum of squares of residuals when n - 1 observations are used to fit by deleting the i th observation.</p></li><li><p>Various types of residuals:</p><ol><li>Standardized residual:</li></ol><ul><li>$z_i = \frac{e_i}{\sigma\sqrt{1 - p_{ii}}}$</li><li>$E(z_i) = 0$ and $V(z_i) = 1$, but $\sum z_i \not = 0$</li></ul><ol start=2><li>Internally studentized residual:</li></ol><ul><li>$r_i = \frac{e_i}{\hat \sigma \sqrt{1 - p_{ii}}}$</li></ul><ol start=3><li>Externally studentized reisudal:</li></ol><ul><li>$r_i^* = \frac{e_i}{\hat \sigma_{(i)} \sqrt{1 - p_{ii}}} \sim t_{n-p-2}$</li><li>For simplicity, we refer to the studentized residuals as the standardized residuals. Note:<ul><li>$r_i^* = r_i \sqrt{\frac{n-p-2}{n-p-1 - r_i^2}}$</li><li>so that $r_i$ and $r_i^*$ are close to each other when the sample size n is large</li></ul></li></ul></li><li><p>Remark 3. For the purpose of residual plots, it makes little difference as to which of the two forms of the standardized residuals is used. From here on, we shall use the internally standardized residuals in the graphs. We need not make any distinction between the internally and externally standardized residuals in our residual plots. Several graphs of the residuals are used for checking the regression assumptions.</p></li></ul><h3 id=43-graphical-methods>4.3 Graphical Methods</h3><ol><li>Detect errors in the data (e.g., an outlying point may be a result of a typographical error)</li><li>Recognize patterns in the data (e.g., clusters, outliers, gaps, etc.)</li><li>Explore relationships among variables</li><li>Discover new phenomena</li><li>Confirm or negate assumptions</li><li>Assess the adequacy of a fitted model</li><li>Suggest remedial actions (e.g., transform the data, redesign the experiment, collect more data, etc.)</li><li>Enhace numerical analyses in general</li></ol><ul><li>Graphs before fitting a model. These are useful, for exmpale, in correcting erros in data and in selecting a model</li><li>Graphs after fitting a model. These are particularly useful for checking the assumptions and fore assessing the goodness of the fit.</li></ul><ol><li>Q-Q plot or P-P plot or normal probability plot: checking normality</li><li>Scatter plots of the standardized residual vs X_i : checking linearity and homogeneity</li><li>Scatter plot of standardized residual vs fitted values: checking linearity and homogeneity</li><li>Index plot of the standardized residuals: checking independent errors</li></ol><h3 id=44-leverages-outliers-influence>4.4 Leverages, Outliers Influence</h3><ul><li>We do not want our fit to be determined by one or few observations</li></ul><h4 id=441-outliers-in-response>4.4.1 Outliers in response</h4><ul><li>Observation with large standardized residuals are outliers in the resposne variable</li><li>See Anscombe&rsquo;s quartet (c). But the outlier in (d) can not be detected by the residuals</li><li>In R, use <code>outlierTest</code></li></ul><h4 id=442-outliers-in-predictors>4.4.2 Outliers in predictors</h4><ul><li>$h_i = H_{ii} = P_{ii}$ are called levwerages and depend only on the predictors</li><li>From $Var(e_i) = \sigma^2(1 - h_i)$, a large leverage $h_i$ will make $Var(e_i)$ small (&lt;=> The fit will be forced to be close to $y_i$)</li><li>In addition, a large leverage point may not be detected in the residual plot since $e_i$ is small</li><li>High leverage points : $h_i \ge 2(p+1)/n$</li><li>See Anscombe&rsquo;s quartet (d). The leverage of 8 th observation is 1 and its residual is 0</li><li>In R, <code>X=model.matrix(fit); hat(X)</code></li></ul><h4 id=443-masking-and-swamping-problems>4.4.3 Masking and Swamping Problems</h4><ul><li>Masking occurs when the data contain outliers but we fail to detect them. THis can happen because some of the outliers may be hidden by other outliers in the data.</li><li>Swamping occurs when we wrongly declare some of the nonoutlying points as outliers. This can occur because outliers tend to pull the regression equation toward them, thence make other points lie far from the fitted equation</li><li>Masking is a false negative decision whereas swamping is a false positive.</li></ul><h4 id=444-incluential-points>4.4.4 Incluential points</h4><ul><li><p>If deletion of an observation make substantial change in the fit (estimated coefficients, fitted values, t-Tests, etc), then it is called an influential point</p></li><li><p>See Anscombe&rsquo;s quartet (c) and (d)</p></li><li><p>Meausres of influence:</p><ul><li>Cook&rsquo;s distance: $C_i = \frac{(\hat \beta - \hat \beta_{(i)})&rsquo; (X&rsquo;X) (\hat \beta - \hat \beta_{(i)})}{\hat sigma^2 (p+1)} = \frac{\sum_{j=1}^n (\hat y_j - \hat y_{j(i)})^2}{\hat \sigma^2 (p+1)} = \frac{r_i^2}{p+1} \frac{p_{ii}}{1 - p_{ii}}$:<ul><li>Rule of thumbs : $C_i \ge 1$</li></ul></li><li>Welsch and Kuh&rsquo;s measure: $DFFITS_i = \frac{\hat y_i - \hat y_i(i)}{\hat \sigma_{(i)} \sqrt{p_{ii}}} = r_i^* \sqrt{p_{ii}}{1 - p_{ii}}$:<ul><li>Rule of thumbs : $|DFFITS_i| \ge 2 \sqrt{(p+1) / (n-p-1)}$</li></ul></li><li>Hadi&rsquo;s influence meausre: $H_i = \frac{p_{ii}}{1-p_{ii}} + \frac{p+1}{1-p_{ii}} \frac{d_i^2}{1 - d_i^2} \text{ where } d_i = e_i / \sqrt{SSE}$</li></ul></li><li><p>What do we do with the outliers and influential points? Do not automatically remove the observations!:</p><ol><li>Check for a data entry error first</li><li>Examine the physical context - why did it happen?</li><li>Exclude the point from the analysis but try reincluding it later if the model is changed</li><li>Redesign the experiment or sample survey, collect more data</li></ol></li></ul><h3 id=45-added-variable-av-plot-and-residual-plus-component-rpc-plot>4.5 Added-variable (AV) plot and residual plus component (RPC) plot</h3><ul><li><p>What are the effects of deleting or adding one of variables from or to the model? The t-test is valid only if the underlying assumptions hold</p></li><li><p>There are two popular graphical ways to complement the t-test</p></li><li><p>Scatter plot matrix shows marginal relationship of each predictor and the response variable, that is, simple linear regression models</p></li><li><p>Added-variable plot (partial regression plot, adjuested variable plots, and individual coefficient plots):</p><ul><li>Let $e(Y | X_{(i)})$ be the residuals of the regression of $Y$ on $X_1, &mldr;, X_{j-1}, X_{j+1}, &mldr;, X_p$, that is , the residuals of:<ul><li>$Y = \beta_0 + \beta_1 X_1 + \cdots + \beta_{j-1} X_{j-1} + \beta_{j+1} X_{j+1} + \cdots + \beta_p X_p + \epsilon$</li></ul></li><li>Let $e(X_j | X_{(j)})$ be the residuals of the regression of $X_j$ on $X_1, &mldr;, X_{j-1}, X_{j+1}, &mldr;, X_p$, that is, the residuals of:<ul><li>$X_j = \alpha_0 + \alpha_1 X_1 + \cdots + \alpha_{j-1} X_{j-1} + \alpha_{j+1} X_{j+1} + \cdots + \alpha_p X_p + \epsilon$</li></ul></li><li>The scatter plot $(e(X_j | X_{(j)}), e(Y | X_{(j)}))$ is called the added-variable plot.:<ul><li>$e(Y | X_{(j)}) = (I - H_{(j)}) y$</li><li>$e(X_j | X_{(j)}) = (I - H_{(j)}) x_j$</li><li>$(I - H_{(j)}) y = (I - H_{(j)}) x_j \beta_j + \epsilon &lsquo;$&rsquo;</li></ul></li><li>Hence, the slope of the added-variable plot is the same as the jth regression coefficient in the full model.</li><li>The AV plot can detect nonlinearity between the response and the predictors</li><li>The AV plot is also useful to detect outliers or influential points</li></ul></li><li><p>Residual plus component plot (parital residual plot):</p><ul><li>A multiple linear regression model can be written as:<ul><li>$Y - \beta_0 - \beta_1 X_1 - \cdots - \beta_{j-1}X_{j-1} - \beta_{j+1} X{j + 1} - \cdots - \beta_p X_p = \beta_j X_j + \epsilon$</li><li>The response adjusting the effects of other predictors is equal to $\beta_j X_j + \epsilon$ and the estimated quantity is equal to $\hat \beta_j X_j + e$</li></ul></li><li>The residual plus component plot is $(X_j, e + \hat \beta_jX_j)$, where the residuals $e_i$&rsquo; and the regression coefficients $\hat \beta_j$&rsquo;s are obtained from the full model.</li><li>The residual plus component plot tends to detect nonlinearity between $X_j$ and $Y$ better than the added-variable plot</li></ul></li></ul><h4 id=451-effects-of-adding-a-variable>4.5.1 Effects of adding a variable</h4><ul><li>There are four possible cases when adding a predictor in a model.:<ul><li>The new variable has an insignificant regression coefficient and the remaining regression coefficients do not change substantially from their previous values. Under these conditions the new variable should not be included in the regression equation, unless omse other external conditions (e.g., theory or subject matter considerations) dictate its inclusion.</li><li>The new variable has a significant regression coefficient, and the regression coefficients for the previously introduced variables are chagned in a substantial way. In this case the new variable should be retained, but an examination of collinearity should be carried out. If there is no evidence of collinearity, the variable should be included in the equation and other additional variables should be examined for possible inclusion. On the other hand, if the variables show collinearity, corrective actions should be taken.</li><li>The new variable has a significant regression coefficient, and the coefficients of the previously introduced variables do not change in any substantial way. This is the ideal situation and aries when the new variable in uncorrelated with the previously introduced variables. Under these conditions the new variable should be retained in the equation.</li><li>The new variable has an insignificant regression coefficient, but the regression coefficients of the previously introduced variables are substantially changed as a result of the introduction of the new variable. This is clear evidence of collinearity, and corrective actions have to be taken before the equation of the inclusion or exclusion of the new variable in the regression equation can be resolved.</li></ul></li></ul><h4 id=452-robust-regression>4.5.2 Robust regression</h4><ul><li>Another approach, useful for the identification of outliers and influential observations, is robust regression, a method of fitting that gives less weight to points with high leverage.</li></ul><h2 id=chapter-5-regression-analysis-with-qualitative-explanatory-variables>Chapter 5. Regression analysis with qualitative explanatory variables</h2><h3 id=51-introduction>5.1 Introduction</h3><ul><li>Predictor(s) may be qualitative: Use dummy (indicator) variables</li><li>Dummy variable takes only on 0 or 1.</li><li>For a qualitative variable with k possible categories $(C_1, \cdots, C_k)$, we need k-1 dummy variables: $I(C_1), \cdots, I(C_{k-1})$ since $I(C_1) + \cdots + I(C_k) = 1$</li></ul><h3 id=52-interactions>5.2 Interactions</h3><ul><li>Interaction effects example</li></ul><h3 id=53-equal-slopes-and-unequal-intercepts>5.3 Equal slopes and unequal intercepts</h3><ul><li>Let D be a dummy variable.:<ul><li>$EY = \beta_0 + \beta_1 X + \beta_2 D$</li><li>is a model equivalent to:<ul><li>$EY = \beta_0 + \beta_1 X \text{ for } D = 0$</li><li>$EY = (\beta_0 + \beta_2) + \beta_1 \text{ for } D = 1$</li></ul></li></ul></li></ul><h3 id=54-unequal-slopes-and-unequal-intercepts>5.4 Unequal slopes and unequal intercepts</h3><ul><li>There may exist an interaction effect between a qualitative predictor and a quantitative predictor to the response variable</li><li>Let D be a dummy variable. The interaction effect of D (qualitative predictor) and X (quantitative predictor) to the response variable Y produces the model with unequal slopes of X (quantitative predictor) and uneuqla intercepts:<ul><li>$EY = \beta_0 + \beta_1 X + \beta_2 D + \beta_3 (X: D)$</li><li>is a model equivalent to:<ul><li>$EY = \beta_0 + \beta_1 X \text{ for } D = 0$</li><li>$EY = (\beta_0 + \beta_2) + (\beta_1 + \beta_3) X \text{ for } D = 1$</li></ul></li></ul></li></ul><h3 id=55-seasonality>5.5 Seasonality</h3><h2 id=chapter-6-transformations>Chapter 6. Transformations</h2><ul><li>Use transformations to achieve linearity or/and homogeneity</li></ul><h3 id=61-transformations-for-lienarity>6.1 Transformations for lienarity</h3><ul><li>Non-linearity between predictor(s) and response may be detected by a scatter plot (simple linear regression) or an added-variable plot or residual plus component plot (multiple linear regression)</li></ul><table><thead><tr><th>Function</th><th>Transformation</th><th>Linear Form</th></tr></thead><tbody><tr><td>$Y=\alpha X^{\beta}$</td><td>$Y&rsquo; = logY, X&rsquo; = logX$</td><td>$Y&rsquo;=\alpha &rsquo; + \beta X'$</td></tr><tr><td>$Y = \alpha e^{\beta X}$</td><td>$Y&rsquo; = logY$</td><td>$Y&rsquo; = log \alpha + \beta X$</td></tr><tr><td>$Y = \alpha + \beta log X$</td><td>$X&rsquo; = logX$</td><td>$Y = \alpha + \beta X'$</td></tr><tr><td>$Y = \frac{X}{\alpha X - \beta}$</td><td>$Y&rsquo; = \frac{1}{Y}, X&rsquo; = \frac{1}{X}$</td><td>$Y&rsquo; = \alpha - \beta X'$</td></tr><tr><td>$Y = \frac{e^{\alpha + \beta X}}{1 + e^{\alpha + \beta X}}$</td><td>$Y&rsquo; = log \frac{Y}{1-Y}$</td><td>$ Y&rsquo; = \alpha + \beta X$</td></tr></tbody></table><h3 id=62-detection-of-heterogeneity>6.2 Detection of heterogeneity</h3><ul><li>Heterogeneity may be detected by the residual plots (residuals vs each predictors or residuals vs fitted values). A formal test can be performed by ncvTest (car) or bptest (lmtest)</li></ul><h3 id=63-variance-stabilizing-transformations>6.3 Variance stabilizing transformations</h3><ul><li>Note that the variance stabilizing transform makes the rror distribution closer to a normal distribution as well</li></ul><table><thead><tr><th>$\sigma$</th><th>Transformation</th></tr></thead><tbody><tr><td>$\sigma = \mu^k$</td><td>$Y^{1 - k}$</td></tr><tr><td>$\sigma = \mu$</td><td>$log Y$</td></tr><tr><td>$\sigma = \sqrt{\mu}$</td><td>$\sqrt{Y}$</td></tr><tr><td>$\sigma = \sqrt{\mu (1 - \mu) / n}$</td><td>$arcsin(\sqrt{Y})$</td></tr></tbody></table><h3 id=64-weighted-least-squares-wls>6.4 Weighted Least Squares (WLS)</h3><h3 id=65-box-cox-power-transformations>6.5 Box-Cox power transformations</h3><ul><li>$f(Y;\lambda) = \begin{cases} \frac{Y^{\lambda} - 1}{\lambda} & \text{ for } \lambda \not = 0 \\ log Y & \text{ for } \lambda = 0 \end{cases}$</li><li>is called the Box-Cox power transformation where $\lambda$ may be estimated from the data. Historically, the main purpose of the Box-Cox transform was to make a random variable closer to a normal distribution</li></ul><h2 id=chatper-7-weighted-least-squares>Chatper 7. Weighted Least Squares</h2><ul><li><p>Model: $y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} + \epsilon_i$. In a matrix form, $Y = X \beta + \epsilon$</p></li><li><p>Suppose that $Var(\epsilon) = \sigma_i^2$</p></li><li><p>Then the transformed new model $\frac{y_i}{\sigma_i} = \beta_0 \frac{1}{\beta_i} + \beta_1 \frac{x_{i1}}{\sigma_i} + \cdots + \beta_p \frac{x_{ip}}{\sigma_i} + \frac{\epsilon_i}{\sigma_i}$ would have a constant variance erros.</p></li><li><p>Let $w_i = \frac{1}{\sigma_i^2}$ be the weight on Observation i. Let $W = diag(w_1, \cdots, w_n)$</p></li><li><p>Minimize:</p><ul><li>$ SSE^*(\beta) = \sum w_i(y_i - \beta_0 - \beta_1 x_{i1} - \cdots - \beta_p x_{ip})^2 \\ = \sum (\frac{y_i}{\sigma_i} - \beta_0 \frac{1}{\sigma_i} - \beta_1 \frac{x_{i1}}{\sigma_i} - \cdots - \beta_p \frac{x_{ip}}{\sigma_i})^2 \\ = \sum (y_i ^ * - \beta_0 x_{i0} ^ * - \beta_1 x_{i1} ^ * - \cdots - \beta_p x_{ip} ^ *)^2 $</li><li>by defining new variables $y_i^* = y_i / \sigma_i$ and $x_{ij}^* = x_{ij} / \sigma_i$</li></ul></li><li><p>In matrix form,:</p><ul><li>$\epsilon = Y - X \beta$</li><li>$SSE^*(\beta) = \epsilon &rsquo; W \epsilon = (Y - X\beta)&rsquo; W (Y - X \beta)$</li><li>We define $Y^* = W^{1/2} Y$ and $X^* = W^{1/2} X$, where $W^{1/2} = diag(1/\sigma_1, \cdots, 1/\sigma_n)$</li></ul></li><li><p>The LSEs with the transformed data are given by:</p><ul><li>$\hat \beta^* = ((X ^ *)&rsquo; X ^ <em>)^{-1} (X^</em>)&rsquo; Y ^ * \\ = (X&rsquo;WX)^{-1} X&rsquo;WY$</li><li>that are teh WLS estimates of the regression coefficient vector $\beta$ with the untransformed data</li></ul></li><li><p>Statistics in terms of the transformed data and the untransformed data:</p><ul><li>Let $y_i ^ *$ be the ith transformed response value: $y_i ^ * = \sqrt{w_i} y_i$, that is, $Y ^ * = W^{1/2} Y$</li><li>Let $x_{i0} ^ *, \cdots, x_{ip} ^ *$ be the ith transformed predictor values : $x_{i0} ^ * = \sqrt {w_i} x_{i0}, \cdots, x_{ip} ^ * = \sqrt{w_i} x_{ip} ^ <em>$, that is, $X^</em> = W^{1/2} X$</li><li>The WLS method minimizes $\sum_{i=1}^n w_i (y_i - \beta_0, x_{i0} - \cdots - \beta_p x_{ip})^2 = \sum_{i = 1}^n(y_i ^ * - \beta_0 x_0 ^ * - \cdots - \beta_p x_{ip} ^ *)^2$:<ul><li>Let $\hat \beta_i ^ *$ be teh WLS estimate of $\beta_i$ and the residual sum of squares of the transformed data can be written as:<ul><li>$SSE ^ * = \sum_{i=1}^n w_i (y_i - \hat \beta_0 ^ * x_{i0} - \cdots - \hat \beta_p ^ * x_{ip})^2 = \sum_{i=1}^n (y_i ^ * - \hat \beta_0 ^ * x_{i0} ^ * - \cdots - \hat \beta_p ^ * x_{ip} ^ *)^2 \\ = (Y - X \hat \beta ^ *)&rsquo; W (Y - X \hat \beta ^ *) = (Y ^ * - X ^ * \hat \beta ^ *) &rsquo; (Y ^ * - X ^ * \hat \beta ^ *)$</li><li>$SST ^ * = \sum_{i=1}^n w_i (y_i - \bar y)^2 = \sum_{i=1}^n (y_i ^ * - \bar y ^ *)^2 \\ = (Y - \bar Y)&rsquo; W (Y - \bar Y0) = (Y ^ * - \bar Y ^ *)&rsquo; (Y ^ * - \bar Y ^ *)$</li><li>$\hat \sigma ^ * = \sqrt{ \frac{ SSE ^ * }{n - p - 1}}$ => R shows this value</li><li>$R^2 = 1 - \frac{SSE ^ * }{SST ^ *}$ => R shows this value</li></ul></li><li>Using the WLS estimates $\hat \beta_i ^ *$ and the untransformed data $y_i, x_{i0}, \cdots, x_{ip}$:<ul><li>$SSE(\hat \beta ^ *) = \sum_{i=1}^n(y_i - \hat \beta_0 ^ * x_{i0} - \cdots - \hat \beta_p ^ * x_{ip}) ^2 \\ = (Y - X \hat \beta ^ *) &rsquo; (Y - X \hat \beta ^ *)$</li><li>$SST = \sum_{i=1}^n (y_i - \bar y)^2 \\ = (Y - \bar Y) &rsquo; (Y - \bar Y)$</li><li>$\hat \sigma = \sqrt{\frac{SSE(\hat \beta ^ *)}{n-p-1}}$</li><li>$Pseudo-R^2 = 1 - \frac{SSE(\hat \beta ^ *)}{SST}$</li></ul></li></ul></li><li>Since the LSEs $\hat \beta_{LS}$ minimize SSE and the WLS estimates $\hat \beta ^ *$ do not minimize SSE, it is always true that $SSE(\hat \beta ^ *) \ge SSE(\hat \beta_{LS})$. The pseudo $R^2$ is always smaller than $R^2$ calculated usign the LSE $\hat \beta_{LS}$</li></ul></li><li><p>How to know or find the weight $w_i$?:</p><ul><li>If known a priori, then use it.</li><li>If unknown, then first fit the data using the OLS method and then figure out the weights $w_i$. At the second stage, fit the data using WLS with the esitmated weightts (Two-stage procedure)</li></ul></li><li><p>Adding the qualitative variable Region to the model may cure the heterogeneity problem</p></li><li><p>We may try a variance stabilizing transform as well through Box-Cox transform.</p></li></ul><h2 id=chapter-8-correlated-errors>Chapter 8. Correlated errors</h2><ul><li>One of the standard regression assumptions: $\epsilon_i$ and $\epsilon_j$ for $i \not = j$ are independent.</li><li>WHen the observations have a natural sequential order as in time series data or spatial data, the coreelation is referred to as autocorrelation (serial correlation, lagged correlation), that is the correlation between the current value and the past value for an example in time series data</li><li>Why does the auto correlation occur?:<ol><li>Successive observations that are positively correlated: adjacent random errors and residuals tend to be similar in both temporal and spatial dimensions due to similar external conditions</li><li>Omitted an important predictor of which adjacent values are correlated.</li></ol></li><li>Effects or consequences of ignoring the autocorrelation: What happens if we ignore the autocorrelation when it exists?:<ol><li>LSEs are still unbiased but not minimum variance, ie.e., lose efficiency</li><li>$\hat \sigma^2$ may be an underestimate of $\sigma^2$, that is, $E(\hat \sigma^2) &lt; \sigma^2$</li><li>CIs and hypothesis tests may be invalid</li></ol></li></ul><h3 id=81-runs-test>8.1 Runs Test</h3><ul><li>Definition 8.8.1 (Runs test):<ul><li>Suppose that we have $n_1$ &ldquo;+&ldquo;s and $n_2$ &ldquo;-&ldquo;s. Under assumption of randomness of &ldquo;+&rdquo; and &ldquo;-&rdquo;, every sequence would have the same probabilty with $\frac{1}{\binom{n_1 + n_2}{n_1}}$. A run is defined as the number of groups in the sequence.</li><li>For instance, ++&mdash;++ has three runs and +-+-++- has 6 runs. To test randomness of a sequence, we can calculate the p-value from all the permutations of the sequence of which the probability is equal.</li></ul></li><li>When $n_1$ = #+&rsquo;s and $n_2$ = #-&rsquo;s are large (>= 10), the expected value and the variance of the runs under the null hypothesis of randomness are:<ul><li>$\mu = \frac{2 n_1 n_2}{n_1 + n_2} + 1$</li><li>$\sigma^2 = \frac{2 n_1 n_22 (2 n_1 n_2 - n_1 - n_2)}{(n_1 + n_2)^2 (n_1 + n_2 - 1)}$</li></ul></li><li>The large sample test statistic of randomness is:<ul><li>$ZZ = \frac{runs - \mu}{\sigma} \sim N(0, 1) \text{ under } H_0$</li></ul></li></ul><h3 id=82-durbin-watson-test>8.2 Durbin-Watson test</h3><ul><li>First order autocorrelation model:<ul><li>$\epsilon_t = p \epsilon_{t - 1} + w_t \text{ where } | \rho | &lt; 1, w_t \sim^{iid} N(0, \sigma^2)$</li><li>where $\rho$ is the correlation coefficient between $\epsilon_t$ and $\epsilon_{t-1}$</li></ul></li><li>Test $H_0: \rho = 0 \text{ against } H_1 : \rho > 0$</li><li>Test statistic is:<ul><li>$d = \frac{\sum_{t=2}^n(e_t - e_{t-1})^2}{\sum_{t=1}^n e_t^2}$</li></ul></li><li>The estimate of $\rho$ is:<ul><li>$\hat \rho = \frac{\sum_{t=2}^n e_t e_{t-1}}{\sum_{t=1}^n e_t^2}$</li></ul></li><li>The Durbin-Watson statistic is approximately:<ul><li>$d = 2(1 - \hat \rho)$</li></ul></li></ul><h3 id=83-transformation-to-remove-autocorrelation-cochrane-and-orcutt-1949>8.3 Transformation to remove autocorrelation (Cochrane and Orcutt, 1949)</h3><ul><li><p>Cochrane and Orcutt&rsquo;s iterative procedure:</p><ol><li>Estimate $\beta_0$ and $\beta_1$ of $y_t = \beta_0 + \beta_1 x_t + \epsilon_t$ using OLS method.</li><li>Obtain the residuals $e_t$ and estimate $\hat \rho = \frac{\sum_{2}^n e_t e_{t-1}}{\sum_{1}^n e_t^2}$</li><li>Calculate $y_t ^ * = y_t - \rho y_{t-1}$ and $x_t ^ * = x_t - \rho x_{t-1}$ and estimate $\beta_0 ^ *$ and $\beta_1 ^ *$. Obtain $\hat \beta_0 = \frac{\hat \beta_0 ^ *}{1 - \hat \rho}$ and $\hat \beta_1 = \hat \beta_1 ^ *$</li><li>Examine the autocorrelation by using the residuals obtained from 3: $\e_t = y_t - \hat \beta_0 - \hat \beta_1 x_t$</li><li>Repeat 2-4 until we remove the autocorrelation</li></ol></li><li><p>Note that Cochrane-Orcutt&rsquo;s procedure does not guarantee convergence</p></li><li><p>Another approach, the textbook called it iterative method, is to minimize:</p><ul><li>$S(\rho, \beta_0, \beta_1) = \sum_{i = 2}^n(y_t - \rho y_{t-1} - \beta_0 (1- \rho) - \beta_1 (x_t - \rho x_{t-1}))^2$</li><li>The results by this methoid usually do not significantly deviate from those by Cochrane-Orcutt procedure</li></ul></li></ul><h3 id=84-autocorrelation-and-missing-predictors>8.4 Autocorrelation and missing predictors</h3><ul><li>Autocorrelation may appear when a model is missing a significant predictor</li></ul><h3 id=85-seasonality-and-dummy-variables>8.5 Seasonality and dummy variables</h3><ul><li>Durbin-Watson test and runs test may not detect a certian type of correlations. One of which is seasonality</li></ul><h2 id=chatper-9-multicollinearity>Chatper 9 Multicollinearity</h2><ul><li>One of the standard regression assumptions is that the predictors are linearly independent</li><li>We call orthogonal predictors if there is completely no linear relationship between the predictors, that is, the predictors are uncorrelated.</li><li>Interpretation does not work any longer if there is multicollinearily ($\beta_j$: The increment of the response by increasing one unit of $X_j$ when all other variables are held fixed??)</li><li>Multicollinearity produces unstable estimates of the regression parameters, that is, the standard error of the regression parameter betcomes large.</li></ul><h3 id=91>9.1</h3><h4 id=911-multicollinearity-may-affect-inferences-in-a-regression-model>9.1.1 Multicollinearity may affect inferences in a regression model</h4><h4 id=912-multicollinearity-may-affect-forecasting>9.1.2 Multicollinearity may affect forecasting</h4><ul><li>Prediction may work with careful consideration of the predictors, that is , the values of the predictors are chosen by satisfying the correlation structures of the predictors, but the prediction under change of a single predictor whil holding others fixed may not be reasonable.</li></ul><h3 id=92-detection-of-multicollinearity>9.2 Detection of multicollinearity</h3><ul><li><p>Definition 9.2.1 (Variance INflaction Factor (VIF)):</p><ul><li>Let $R_j^2$ be the coefficient of determination whe n$X_j$ is regressed on all other predictors. The variance inflation factor (VIF) for $X_j$ is defined by:<ul><li>$VIF_j = \frac{1}{1 - R_j^2}$</li></ul></li></ul></li><li><p>Note that $ 1 \le VIF_j &lt; \infty$. When all predictors are orthogonal, $VIF_j = 1$. If $VIF_j > 10$, then the estimate of $\beta_j$ may be unstable. The variance of the LS estimate of $\beta_j$ is proportional to $VIF_j$ when $X_j$ is centered and scaled.</p></li><li><p>Note $R_j^2 = 1 - \frac{1}{VIF_j}$. Therefore, $VIF_j > 10$ is equivalent to $R_j^2 > 0.9$</p></li><li><p>The average $VIF_j$ values, denoted by $\bar VIF$, is the ratio of the squared error of the LSEs to the squared errors the estimates when the predictors are orthogonal</p></li><li><p>$Cov(\hat \beta) = \sigma^2 (X&rsquo; X)^{-1}$. The kth diagonal term of $(X&rsquo;X)^{-1}$ is:</p><ul><li>$((X&rsquo;X)^{-1})<em>{kk} = [x&rsquo;<em>k x_k - x&rsquo;<em>k X</em>{(k)} (X&rsquo;</em>{(k)} X</em>{(k)}) ^{-1} X&rsquo;<em>{(k)} x_k]^{-1} \\ = [x&rsquo;<em>k ( I - P</em>{X</em>{(k)}})]^{-1} \\ = [S_{kk}(1 - R_k^2)]^{-1} \\ = \frac{VIF_k}{S_{kk}} \\ = \frac{VIF_k}{(n-1)Var(x_k)}$</li></ul></li><li><p>Definition 9.2.2 (The condition indices and the condition number).:</p><ul><li>$Let \lambda_1 \ge \cdots \ge \lambda_p$ be the ordered eigenvalues of the correlation matrix of the predictors. THe jth condition index is defined by:<ul><li>$\kappa_j = \sqrt{\frac{\lambda_1}{\lambda_j}}$</li></ul></li><li>When it is large, there exists multicollinearity. The largest value $\kappa_p$ is called the condition number of the correlation matrix and there may be a strong multicollinearity if $\kappa_p \ge 15$</li></ul></li></ul><h3 id=summary-of-chapter-9>Summary of Chapter 9</h3><ul><li>Consequences of multicollinearity:<ul><li>Interpretation of the fitted model may not be reaonsable</li><li>Predcition may not work without careful handling of collinearity</li><li>The variance(s) of the LSE(s) may be large.</li></ul></li><li>How to detect multicollinearity?:<ul><li>Look at the scatter plot matrix</li><li>Calculate VIFs and see if any VIF > 10.</li><li>Calculate the condition indicies $\kappa_j = \sqrt{\lambda_1 / \lambda_j}$ and see if any condition index > 15</li></ul></li></ul><h2 id=chatper-10-methods-for-data-with-multicollinearity>Chatper 10. Methods for data with multicollinearity</h2><ul><li>We may delete some variables causing multicollinearity. => It may not be clear which variable(s) should be removed.</li><li>Principal components regression followed by deleting some components with small variances</li><li>Ridge regression and LASSO</li></ul><h3 id=101-principal-components>10.1 Principal components</h3><ul><li><p>Have X centered and scaled so that we assume $Var(X_j) = 1$ and $E(X_j) = 0$</p></li><li><p>Transform $X_1, \cdots, X_p$ to p orthogonal predictors, $C_1, \cdots, C_p$:</p><ul><li>$C_j = v_{1j} X_1 + v_{2j} X_2 + \cdots + v_{pj} X_p$</li><li>Let $V = (v_1; \cdots; v2) = \begin{pmatrix} v_{11}&amp;v_{12}& \cdots& v_{1p} \\ v_{21}& v_{22} &\cdots& v_{2p} \\ \vdots &\vdots& \ddots& \vdots \\ v_{p1}& v_{p2}& \cdots& v_{pp} \end{pmatrix}$</li><li>be the matrix whos column vectors are orthonormal eigenvectors of $\frac{1}{n-1} X&rsquo; X$, that is , $(\frac{1}{n-1} X&rsquo; X) v_i = \lambda_i v_i$ and $v_j &rsquo; v_i = \delta_{ij}$</li><li>New data set may be written as:<ul><li>$C_{n \times p} = X_{n \times p} V_{p times p}$</li><li>$\frac{1}{n-1} C&rsquo;C=V&rsquo;(\frac{1}{n-1} X&rsquo;X) V \\ = \Lambda = diag(\lambda_1, \cdots, \lambda_p)$</li><li>that is, $C_j$ has the sample variance $\lambda_j$ and $C_i$ and $C_j$ for $i \not = j$ have zreo sample correlation coefficient.</li></ul></li></ul></li><li><p>If some eigenvalues are quite smaller than others or near zero, then multicollinearity exists. The principal components (PCs) with samll eigenvalues may give use the relationship between the predictors</p></li><li><p>Summary of principal components:</p><ul><li>Diagonalize the covariance or correlation matrix</li><li>Eigenvector => Coefficients of the principal component</li><li>Eigenvalue => Variance of the principal component</li></ul></li></ul><h3 id=102-recovering-the-regression-coefficients-of-the-original-variables>10.2 Recovering the regression coefficients of the original variables</h3><h4 id=1021-recovering-lses-from-the-fit-using-the-centered-orand-scaled-data>10.2.1 Recovering LSEs from the fit using the centered or/and scaled data</h4><ul><li>Let $Y= \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p + \epsilon &lsquo;$, that is $Y = \beta_0 1 + X \beta + \epsilon&rsquo;$.</li><li>Assume that $W$ and $Z = (Z_1, \cdots, Z_p)$ are centered and scaled variables of $Y$ and $X = (X_1, \cdots, X_p)$, respectively.</li><li>Note that the LSE of the intercept for the centered variables iz zero since:<ul><li>$W = \theta_0 1 + Z \theta + \epsilon$</li><li>$\begin{pmatrix} \hat \theta_0 \\ \hat \theta \end{pmatrix} = [\begin{pmatrix} 1&rsquo; \\ Z&rsquo; \end{pmatrix} \begin{pmatrix} 1 : Z \end{pmatrix}]^{-1} \begin{pmatrix} 1&rsquo; \\ Z&rsquo; \end{pmatrix} W \\ = \begin{pmatrix} n &amp;0 \\ 0 &amp;Z&rsquo;Z\end{pmatrix} ^{-1} \begin{pmatrix} 0 \\ Z&rsquo;W \end{pmatrix} \\ = \begin{pmatrix} \frac{1}{n} &amp;0 \\ 0& [Z&rsquo;Z]^{-1} \end{pmatrix} \begin{pmatrix} 0 \\ Z&rsquo;W \end{pmatrix} \\ = \begin{pmatrix} 0 \\ (Z&rsquo;Z)^{-1}Z&rsquo;W \end{pmatrix}$</li></ul></li><li>Hence we do not need $\theta_0$ for the centered data:<ul><li>$W=Z\theta + \epsilon$</li><li>$\frac{Y - \bar y 1}{s_y} = \theta_1 \frac{X_1 - \bar x_1 1}{s_{x_1}} + \cdots + \theta_p \frac{X_1 - \bar x_p 1} {s_{x_p}} + \epsilon$</li><li>$Y = (\bar y - \theta_1 \frac{s_y}{s_{x_1}} \bar x_1 - \cdots - \theta_p \frac{s_y}{s_{x_p}} \bar x_p) 1 + \theta_1 \frac{s_y}{s_{x_1}} X_1 + \cdots + \theta_p \frac{s_y}{s_{x_p}} X_p + s_y \epsilon$</li></ul></li><li>Compare the above to:<ul><li>$Y = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p + \epsilon&rsquo;$</li></ul></li><li>We conclude that:<ul><li>$ \beta_i = \theta_i \frac{s_y}{s_{x_i}} \text{ for } i = 1, \cdots, p$</li><li>$ \beta_0 = \bar y - \beta_1 \bar x_1, - \cdots - \beta_p \bar x_p$</li></ul></li></ul><h4 id=1022-recovering-the-lses-from-the-fit-using-the-principal-components>10.2.2 Recovering the LSEs from the fit using the principal components</h4><ul><li>Let $(Z&rsquo;Z)V = (n-1)V \Lambda$, that is, $V&rsquo;(Z&rsquo;Z)V = (n-1)\Lambda = (n-1)diag(\lambda_1, \cdots, \lambda_p)$ where $\lambda_1 \ge \cdots \ge \lambda_p$ and $VV&rsquo; = V&rsquo;V =I$</li><li>Define $C=ZV$ The jth vector $C_j = Z_1 v_{1j} + \cdots + Z_p v_{pj}$ that is called the jth principal component.<ul><li>$W = Z\theta \epsilon$</li><li>$W = (ZV)(V&rsquo; \theta) + \epsilon$</li><li>$W = C \alpha + \epsilon$</li></ul></li><li>The LSE of $\alpha$ is:<ul><li>$\hat \alpha = (C&rsquo;C)^{-1} C&rsquo; W \\ = \frac{1}{n-1} \Lambda^{-1} C&rsquo; W$</li><li>$Cov(\hat \alpha) = \frac{1}{(n-1)^2} \Lambda ^{-1} C&rsquo; Cov(W) C \Lambda^{-1} \\ = \frac{1}{(n-1)^2} \Lambda^{-1} C&rsquo; \frac{\sigma^2}{s_y^2} IC\Lambda ^{-1} \\ = \frac{\sigma^2}{(n-1)s_y^2} \Lambda ^{-1}$</li><li>that means the variance of $\hat \alpha_i = \frac{\sigma^2}{(n-1)s_y^2} \lambda_i$ and $\hat \alpha_i$ and $\hat \alpha_j$ are note correlated for $i \not = j$. This implies that is $\lambda_i$ is small, then the LSE of $\alpha_i$ has high uncertainty. Since $V&rsquo; \theta = \alpha$,:</li><li>$\hat \theta = V \hat \alpha$</li><li>$Cov(\hat \theta) = \frac{\sigma^2}{(n-1) s_y^2} V \Lambda^{-1} V'$</li></ul></li></ul><h4 id=summary-of-recovering-regression-coefficients>Summary of recovering regression coefficients</h4><ul><li><p>Let $\hat \alpha$ be the regression coefficients vector from the principal components.</p></li><li><p>Let $\hat \theta$ be the regression coefficients vector from centered and/or scaled data</p></li><li><p>Let $\hat \beta$ be the regression coefficients vector from the original data</p></li><li><p>$\hat \theta = V \hat \alpha, \text{ that is }, \hat \theta_i = \sum_{j=1}^p v_{ij} \hat \alpha_j$</p></li><li><p>$\hat \beta_i = \hat \theta_i \frac{s_y}{s_{x_i}} \text{ for } i = 1, \cdots, p$</p></li><li><p>$\hat \beta_0 = \bar - \hat \beta_1 \bar x_1 - \cdots - \hat \beta_p \bar x_p$</p></li><li><p>Standard errors of the estimates: The variance of the regression coefficients can be written as:</p><ul><li>$Cov(\hat \alpha) = \frac{\sigma^2}{(n-1) s_y^2} \Lambda^{-1} = \frac{\sigma^2}{(n-1) s_y^2} diag(\frac{1}{\lambda_1}, \cdots, \frac{1}{\lambda_p})$</li><li>$Cov(\hat \theta) = V Cov(\hat \alpha) V&rsquo; = \frac{\sigma^2}{(n-1) s_y^2} V \Lambda^{-1} V'$</li><li>$Var(\hat \theta_i) = \frac{\sigma^2}{(n-1) s_y^2} \sum_{j=1}^p \frac{v_{ij}^2}{\lambda_j}$</li><li>$\hat {Var} (\hat \beta_i) = \frac{\hat \sigma^2}{(n-1) s_{x_i}^2} \sum_{j=1}^p \frac{v_{ij}^2}{\lambda_j}$</li><li>$s.e.(\hat \beta_i) = \frac{\hat \sigma}{\sqrt{n-1} s_{x_i}} \sqrt{\sum_{j=1}^p \frac{v_{ij}^2}{\lambda_j}}$</li></ul></li></ul><h3 id=103-principal-component-regression-dimension-reduction>10.3 Principal component regression (Dimension reduction)</h3><ol><li>Center and/or scale the data</li><li>Calculate the principal components of the sample variance-covariance matrix or the sample correlation matrix</li><li>Select the number of principal components</li><li>Fit the data using the selected principal components</li><li>Recover the estimates of the regression coefficients</li></ol><ul><li>Note that the number of principal components may be selected by the cross-validation (CV) method</li></ul><h3 id=104-ridge-regression>10.4 Ridge regression</h3><ul><li><p>Suppose that the data set is centered and scaled.</p></li><li><p>The LSE of the regression coefficients in matrix form can be written as:</p><ul><li>$\hat \theta = (X&rsquo; X)^{-1} X&rsquo; y$</li></ul></li><li><p>Multicollinearity makes $X&rsquo;X$ (almost) singular => LSEs are unstable</p></li><li><p>Hoerl and Kennard (1970) proposed the ridge regression:</p><ul><li>$\hat \theta ^ *(\lambda) = (X&rsquo;X + \lambda I) ^{-1} X&rsquo;y$</li></ul></li><li><p>The ridge estimator $\hat \theta^*(\lambda)$ is to minimize:</p><ul><li>$(y - X \theta)&rsquo;(y - X \theta) + \lambda \sum_{j=1}^p \theta_j ^2$</li><li>Equaivalently, minimize:<ul><li>$(y - X \theta)&rsquo; (y - X \theta) \text{ given } \sum_{j=1}^p \theta_j^2 \le C$</li></ul></li><li>$\lambda$ = tuning parameter to be selected.</li><li>$\lambda = 0$ => OLS</li><li>$\lambda = \infty$ => All estimates = 0</li></ul></li><li><p>How to select the tuning parameter $\lambda$?:</p><ul><li>HKB method : Hoerl, Kennard, and Baldwin (1975):<ul><li>$\lambda = \frac{p \hat \sigma^2 (0)}{\sum_{j=1}^p \hat \theta_j (0)}$</li></ul></li><li>Iterative method: Let $\lambda_0$ be the estimate of HKB:<ul><li>$\lambda_i = \frac{p \hat \sigma^2(0)}{\sum_{j=1}^p \hat \theta_j (\lambda_{i-1})}$</li></ul></li><li>Ridge trace: Use the plot of the ridge estiamtes over $\lambda$. Select $\lambda$ such that the stimates are stable with small $SSE(\lambda)$ and $VIF(\lambda)$s:<ul><li>$SSE(\lambda) = (Y - X \hat \theta ^ * (\lambda))&rsquo; (Y - X \hat \theta ^ * (\lambda))$</li><li>$VIF_j(\lambda) = ((X&rsquo;X + \lambda I)^{-1} X&rsquo; X (X&rsquo; X + \lambda I) ^{-1})_{jj}$</li></ul></li><li>Cross validation (CV) method: Find $\lambda$ minimizing the cross validation prediction error</li></ul></li><li><p>Properties of the ridge estimator:</p><ul><li>$E(\hat \theta ^ * (\lambda)) = (X&rsquo; X + \lambda I)^{-1} X&rsquo; X \theta$</li><li>$Cov(\hat \theta ^ * (\lambda)) = (X&rsquo; X + \lambda I) ^{-1} X&rsquo; X( X&rsquo; X + \lambda I) ^{-1} \sigma^2$</li><li>$MSE(\lambda) = E[(\hat \theta ^ * (\lambda) - \theta) &rsquo; (\hat \theta ^ * (\lambda) - \theta)] \\ = \sigma^2 \sum_{j=1}^p \frac{\lambda_j}{(\lambda_j + \lambda)^2} + \lambda^2 \theta &rsquo; (X&rsquo; X + \lambda I)^{-2} \theta \\ = Variance + (Bias)^2$</li></ul></li></ul><h3 id=105-least-absolute-shrinkage-and-selection-operator-lasso>10.5 Least Absolute Shrinkage and Selection Operator (LASSO)</h3><ul><li>Tibshirani, R. (1996) Regression shrinkabge and selection via the lasso. Journal of the Royal Statistical Society Series B</li><li>$L^1$ regularization:<ul><li>$(y - X \theta)&rsquo; (y - X \theta) + \lambda \sum_{j=1}^p | \theta_j |$</li><li>Equivalently, minimize:<ul><li>$(y - X \theta)&rsquo;(y - X \theta) \text{ given } \sum_{j=1}^p | \theta_j | \le C$</li></ul></li></ul></li><li>As $\lambda$ gets large (equivalently, C gets small), some of estimates shrink to zero. => It can be used for variable or model selection</li><li>Easier interpretation than the ridge regression.</li><li>The tuning parameter $\lambda$ or C can be estimated by CV method.</li></ul><h2 id=chapter-11-variable-selections>Chapter 11 Variable selections</h2><h3 id=111-why-do-we-need-variable-selections>11.1 Why do we need variable selections?</h3><ul><li>So far we assume that the predictors are predetermined to be included in our model.</li><li>In practice, we do not know which predictors should be included in the model.</li><li>We do not know the functional relationship as well</li><li>Selecting variables and the functional relationship must be simultaneously considered.</li></ul><h3 id=112-effects-of-variable-selections>11.2 Effects of variable selections</h3><ul><li>We have a resposne Y and q predictors $X_1, \cdots, X_q$</li><li>The linear regression equation including all available predictors can be written as:<ul><li>$y_i = \beta_0 + \sum_{j=1}^q \beta_j x_{ij} + \epsilon_i$</li><li>Let $\hat \beta_j ^ *$ be the LSE of the model including all available predictors.</li></ul></li><li>We need to statistically decide which predictors should be retained in the model and which predictors should be removed from the model.</li><li>If we remove $X_{p+1}, \cdots , X_q$ from our model, then we have a smaller model:<ul><li>$y_i = \beta_0 + \sum_{j=1}^p \beta_j x_{ij} + \epsilon_i$</li><li>Let $\hat \beta_j$ be the LSE of the model including p predictors by setting $\beta_{p+1} = \cdots = \beta_q = 0$</li></ul></li><li>What happens if the full model is correct and we removed some predictors?:<ul><li>$Var(\hat \beta_j ^ *) \ge Var(\hat \beta_j)$</li><li>$0 = Bias^2(\hat \beta_j ^*) \le Bias ^2(\hat \beta_j)$</li><li>$MSE(\hat \beta_j ^ *) = Var (\hat \beta_j ^ *) + bias ^2 (\hat \beta_j ^ *)$</li><li>Variance dec, Bias inc</li><li>This is called &ldquo;Bias-Variance trade-off&rdquo;</li></ul></li><li>Conclusions:<ul><li>Deleting variables may give us the smaller MSE esitmates than the unbiased LS estimates under the true model including all predictors.</li><li>Including extraneous variables results in loss of precision of the estimates.</li></ul></li></ul><h3 id=appendix-effects-of-incorrect-model-specifications>Appendix: Effects of Incorrect Model Specifications</h3><ul><li>$\hat \beta_p$ is a biased estimate of $\beta_p$ unless $\beta_r = 0$ or $X_p&rsquo;X_r = 0$</li><li>$Cov(\hat \beta_p ^ *) - Cov(\hat \beta_p) \ge 0$, that is , variances of the least squares estimates of regression coefficients obtained from the full model are larger than the corresponding variances of the estimates obtained from the subset model. In other words, the deletion of variables always results in smaller variances for the estimates of the regression coefficients of the remaining variables.</li><li>If $Cov(\hat \beta_r ^ <em>) - \beta_r \beta_r &rsquo; \ge 0$, then $MSE (\hat \beta_p ^</em>) - MSE(\hat \beta_p) \ge 0$. This means that the least squares estimates of regression coefficients obtained from the subset model have smaller mean square error than estimates obtained fro mthe full model when the variables deleted have regression coefficients that are smaller than the standard deviation of the esitmates of the coefficients.</li><li>$\hat \sigma_p ^2$ is generally biased upward as an estimate of $\sigma^2$</li></ul><h3 id=113-practical-issues-in-variable-selections>11.3 Practical issues in variable selections</h3><ul><li>When a dataset with k predictors is collected, the number of all possible linear regression models equals $2^k$ => Exhaustive search may not be feasible in practice.</li><li>When we compare two or more models with the same number of predictors, we may use $R^2$ to select better model. What if the models do not have the same number of predictors ? => We need the statistical criteria for variable selections.</li></ul><h3 id=114-forward-backward-stepwise-selection>11.4 Forward, backward, stepwise selection</h3><ul><li>Classical approach based on the p-values in variable selections or model selections: sequentially include or remove a variable based on the p-value of the F test discussed in earlier chapters.:<ul><li>What relationship between the probability that we select the true model and the level of significance? : No clear theoretical undertanding.</li><li>However, this approach is still widely used in many areas and most of statistical software provide this.</li></ul></li><li>Suppose that we have k possible predictors: $X_1, \cdots, X_k$. Assume there is no serious multicollinearity between predictors.:</li></ul><h4 id=fs-forward-selection>FS (Forward Selection)</h4><ol><li>Preselect level of significance $\alpha_{in}$</li><li>Start with the samllest model $y_i = \beta_0 + \epsilon_i$, denoted by $M_0$</li><li>Find the model having the smallest p-value among k models: $y_i = \beta_0 + \beta_j x_{ij} + \epsilon$. If the p-value of the model is less than or equal to $\alpha_{in}$, then include the variable and the model is denoted by $M_1$. Otherwise, stop the procedure.</li><li>Continue this procedure until there is no variable has the smaller p-value than $\alpha_in$ or it reaches the model including all variables.</li></ol><h4 id=be-backward-elimination>BE (Backward Elimination)</h4><ol><li>Predelect level of signifiance $\alpha_{out}$</li><li>Start with the biggest model: $y_i = \beta_0 + \sum_{j=1}^k \beta_j x_{ij} + \epsilon_i$</li><li>If the largest p-value of $X_j$ is greater than or equal to $\alpha_{out}$, then eliminate it.</li><li>Refit the data with $k-1$ variables</li><li>Continue the procedures until no predictor has larger p-value than $\alpha_{out}$ or it reaches $y_i = \beta_0 + \epsilon_i$</li></ol><h4 id=stepwise-selection>Stepwise selection</h4><ol><li>Preselect $\alpha_{in}$ and $\alpha_{out}$</li><li>At each step of the FS method, check whether a variable has the p-value greater than or equal to $\alpha_{out}$, if so, eliminate the variable.</li><li>Continue these procedure until no variable has smaller p-value less than or equal to $\alpha_{in}$ and greater than or equal to $\alpha_{out}$ or it reaches the full model.</li></ol><h3 id=115-best-subset-selection-regression>11.5 Best subset selection (regression)</h3><ul><li>For each number of predictors, select the best model based on the coefficient of determination $R^2$</li><li>We obtain k best models</li><li>If necessary, we select the best model among the k models based on one or some criteria.</li><li>It is an exhaustive search looking at all possible regression models</li><li>It cannot be used if the number of predictors k is too large.</li></ul><h3 id=116-criteria>11.6 Criteria</h3><ul><li>MSE(=RMS) : Mean Square Errors or Residual Mean Squares of a model with p parameters is defined by:<ul><li>$MSE_p = \frac{SSE_p}{n-p}$</li><li>We select a model with smallest $MSE_p$</li></ul></li><li>$R_{adj}^2$ = Adjusted $R^2$ is defined by:<ul><li>$R_{adj, p}^2 = 1 - \frac{SSE_p / (n-p)}{SST/(n-1)} = 1 - \frac{MSE_p}{SST/(n-1)}$</li><li>Hence, maximizing $R_{adj,p}^2$ is equivalent to minimizing $MSE_p$</li></ul></li><li>Mallows $C_p$ : Try to minimize the standardized total MSE of the prediction, that is,:<ul><li>$\frac{\sum_{i=1}^n MSE_p(\hat y_i)}{\sigma^2} = \frac{\sum_{i=1}^n (\hat y - E y_i)^2}{\sigma^2}$</li><li>Mallows (1973) estimated it as:<ul><li>$C_p = \frac{SSE_p}{\hat \sigma^2} + 2p - n$</li><li>where $\hat \sigma^2$ is the MSE of the model including all possible predictors. If the model with p predictors is correct, then $E(C_p) = p$.</li></ul></li><li>In parctice,:<ul><li>Select a model of which $C_p$ is close to p.</li><li>Usually, there are many models of which $C_p \approx p$, select the model with smallest $C_p$ as well.</li><li>Disadvantage: The estimate $\hat \sigma ^2$ affects $C_p$ and the variable selection based on $C_p$ may not be reliable.</li></ul></li></ul></li><li>PRESS: It is the predction sum of squares:<ul><li>$PRESS_p = \sum_{i=1}^n (y_i - \hat y_{(i)})^2 = \sum_{i=1}^n (\frac{e_i}{1 - h_{ii}})^2$</li><li>where $\hat y_{(i)}$ is the predicted value of the ith observation based on the regression model with p variables excluding ith observation.</li><li>It is called a leav-on-out cross-validation (LOOCV)</li><li>For linear regression model, this statistic has a closed form, that is, it is very simple to calculate</li></ul></li><li>AIC: Akaike information criterion (AIC) derivated this to correct the bias term of the log-likelihood function in very general situation.:<ul><li>$AIC_p = -2 log (Likelihood) + 2p \\ = n log(\frac{SSE_p}{n}) + 2p$</li><li>AIC tends to overfit &lt;=> AIC commonly selects a bigger model than the true model.</li><li>AIC or modified versions of AIC are widely used for the purpose of prediction &lt;= Overfit is less problematic in prediction.</li><li>AIC is not consistent, that is, $lim_{n->\infty} P(\text{select a true model}) \not = 1$</li></ul></li><li>BIC: Schwarz(1978) derived Bayesian information criterion (BIC) under Bayesian framework of the problem:<ul><li>$BIC_p = -2 log (Likelihood) + plog n \\ = n log (\frac{SSE_p}{n}) + p log n$</li><li>BIC tends to select a smaller model than AIC &lt;= BIC penalizes more than AIC for $log n \ge 2$</li><li>BIC is consistent, that is , $lim_{n->\infty} P(\text{select the true model}) = 1$ if the model space includes the true model. => It provides very nice theoretical justification ubt in practice we do not know our modelspace includes the true model.</li><li>BIC is commonly used when our model selection is the purpose of explanation and interpretation.</li></ul></li><li>Cross-validation(CV): Recently, as the computer technology has been rapidly developed, this method is widely applied once the sample size is not so small.:<ul><li>PRESS is a kind of the cross-validation methods.</li><li>Randonly partition the dataset into (usually) 5 or 10 almost equal sizes subsets.</li><li>For instance, suppose that we partition the dataset into 10 subsets. Use 9 subsets as the traning set to fit a model and estimate the prediction erros using the subset that is not used to fit the model.</li><li>We will have 10 fitted models and 10 estimates of the prediction erros. => Take average of the prediction erros.</li><li>For each model, we estimate the average of the prediction erros.</li><li>Choose the model having the smallest average prediction errors.</li></ul></li></ul><h3 id=118-variable-selections-with-multicollinear-data>11.8 Variable selections with multicollinear data</h3><ul><li>We may eliminate some predictors to remove th multicollinearity</li><li>We may apply Ridge Regression:<ul><li>Eliminate variables whose coefficients are stable but small. Since ridge regression is applied to standardized data, the magnitude of the various coefficients are directly comparable.</li><li>Eliminate variables with unstable coefficients that do not hold their predicting power, that is, untable coefficients that tend to zero.</li><li>Eliminate one or more variables with unstable coefficients. The variables remaining from the original set, say p in number, are used to form the regression equation.</li></ul></li><li>We may apply LASSO with cross-validation (CV) method to determine the tuning parameter.</li></ul></div><hr><div class=list-files><ul class=section-tree></ul></div></article><script src=/js/wikilink.js></script><div><script src=https://utteranc.es/client.js repo=minuk-dev/minuk-dev.github.io issue-term=pathname theme=github-dark crossorigin=anonymous async></script></div></main><footer class=footer><div class=footer-left></div><div class=footer-right><ul class=social><li><a href=/about>About</a></li><li><a href=https://github.com/minuk-dev>Github</a></li></ul></div></footer></body><script src=/js/sidebar.js></script><script src=/js/dir_toggle.js></script><script src=/js/codeblock_copy.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.1/font/bootstrap-icons.css><script type=module>
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs";
mermaid.initialize({
  startOnLoad: true,
  theme: "dark",
});
</script></html>
<!doctype html><html lang=ko-kr><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet href=https://unpkg.com/simpledotcss/simple.css><link rel=stylesheet href=/css/main.css><meta name=generator content="Hugo 0.140.0"><meta name=description content="minuk.dev wiki"><meta name=keywords content="hugo,site,new"><meta name=author content="Min-Uk.Lee"><title>수치해석 |
minuk dev wiki
</title><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></head><body><header class=header><div class=header_left><a href=/><img class=logo src=/images/Rb.png alt=logo>
MinUk.Dev</a></div><div class=header_middle>수치해석</div></header><main><article class=main><div class=title><h1 class=title-header>수치해석</h1></div><a href=https://github.com/minuk-dev/minuk-dev.github.io/blame/master/content/wiki/lectures/numerical_analysis.md><h5>created : Thu, 21 Apr 2022 00:32:34 +0900</h5><h5>modified : Thu, 16 Jun 2022 10:30:04 +0900</h5></a><div class=article-meta><div class="breadcumb content"><i class="bi bi-folder"></i>
[[lectures]]</div></div><div class=list-terms><ul><i class="bi bi-tags" title=Tags></i></ul></div><aside class=navbar id=nav-toc style=text-align:left><nav id=TableOfContents><ul><li><a href=#curves>Curves</a><ul><li><a href=#curves---implicit-representation>Curves - Implicit Representation</a></li><li><a href=#curves---parametric-representation>Curves - Parametric Representation</a></li><li><a href=#criteria-for-choosing-a-representation>Criteria for choosing a representation</a></li><li><a href=#matrix-equation-for-parametric-curve>Matrix Equation for parametric curve</a></li><li><a href=#degree-of-the-polynomial>Degree of the Polynomial</a></li><li><a href=#multiple-curve-segments>Multiple curve segments</a></li><li><a href=#interpolation>Interpolation</a></li></ul></li><li><a href=#pca>PCA</a><ul><li><a href=#principal-components>Principal Components</a></li><li><a href=#principal-component-analysis>Principal Component Analysis</a></li><li><a href=#higher-diemnsions>Higher Diemnsions</a></li></ul></li><li><a href=#decomposition>Decomposition</a><ul><li><a href=#trianglular-systems>Trianglular Systems</a></li><li><a href=#lu-decomposition>LU Decomposition</a></li><li><a href=#error-reduction>Error Reduction</a></li><li><a href=#matrix-inversion>Matrix Inversion</a></li></ul></li><li><a href=#iterative>Iterative</a><ul><li><a href=#solving-large-lienar-systems>Solving Large Lienar Systems</a></li><li><a href=#jacobi-method>Jacobi Method</a></li><li><a href=#gauss-seidel-method>Gauss-Seidel Method</a></li><li><a href=#convergence>Convergence</a></li><li><a href=#relaxation-in-gauss-seidel>Relaxation in Gauss-Seidel</a></li></ul></li><li><a href=#interpolation-1>Interpolation</a><ul><li><a href=#interpolation-2>Interpolation</a></li><li><a href=#linear-interpolation>Linear Interpolation</a></li><li><a href=#quadratic-interpolation>Quadratic Interpolation</a></li><li><a href=#higher-degree-interpolation>Higher Degree Interpolation</a></li><li><a href=#newton-polynomials>Newton Polynomials</a></li><li><a href=#spherical-linear-interpolation>Spherical Linear Interpolation</a></li><li><a href=#bilinear-interopolation>Bilinear Interopolation</a></li></ul></li><li><a href=#least-squares>Least Squares</a><ul><li><a href=#least-squares-line>Least-Squares Line</a></li><li><a href=#errors>Errors</a></li><li><a href=#least-squares-line-1>Least-Squares Line</a></li><li><a href=#power-fit>Power Fit</a></li><li><a href=#data-linearization>Data Linearization</a></li><li><a href=#matrix-formulation>Matrix Formulation</a></li><li><a href=#polynomial-fitting>Polynomial Fitting</a></li><li><a href=#nonlinear-least-squares-method>Nonlinear Least-Squares Method</a></li></ul></li><li><a href=#optimization>Optimization</a><ul><li><a href=#functions-of-one-variable>Functions of One Variable</a></li><li><a href=#bracketing-search-methos>Bracketing Search Methos</a></li><li><a href=#golden-ratio-search>Golden Ratio Search</a></li><li><a href=#fibonacci-search>Fibonacci Search</a></li><li><a href=#multidimensional-optimization>Multidimensional Optimization</a></li><li><a href=#gradient>Gradient</a></li><li><a href=#hessian-matrix-of-hessian-of-f>Hessian Matrix (of Hessian of $f$)</a></li></ul></li><li><a href=#solution-of-nonlinear-equation>Solution of Nonlinear Equation</a><ul><li><a href=#basis-of-bisection-method>Basis of Bisection Method</a></li></ul></li></ul></nav></aside><div class=content><h2 id=curves>Curves</h2><h3 id=curves---implicit-representation>Curves - Implicit Representation</h3><ul><li>Most curvest can be represented implicitly:<ul><li>$$f(x, y) = 0$$</li><li>Examples:<ul><li>$$ax + by + c = 0$$</li><li>$$x^2 + y^2 - r^2 = 0$$</li></ul></li><li>In 3D, surfaces are represented as:<ul><li>$$f(x, y, z) = 0$$</li></ul></li></ul></li></ul><h3 id=curves---parametric-representation>Curves - Parametric Representation</h3><ul><li>The value of each component (x, y, z) depends on an independent variable, u (the parameter).</li><li>$$p(u) = \begin{bmatrix} x(u) \ y(u) \ z(u) \end{bmatrix}$$</li><li>$$\frac{dp(u)}{du} = \begin{bmatrix} \frac{dx(u)}{du} \ \frac{dy(u)}{y(u)} \ \frac{dz(u)}{z(u)} \end{bmatrix}$$<ul><li>Derivative : Direction = tangent, Magnitude = speed that curve changes with u</li></ul></li></ul><h3 id=criteria-for-choosing-a-representation>Criteria for choosing a representation</h3><ul><li>Parametric forms are not unique. Many different representations are possible for a single curve.</li><li>Criteria for a good representation:<ul><li>Local control of shape</li><li>Smoothness and continuity</li><li>Ability to evaluate derivatives</li><li>Stability</li><li>Ease of Rendering</li></ul></li><li>Parametric curves that are polynomials in u satisfy many of these criteria:<ul><li>$$x(y) = c_0 + c_1 u + c_2 u^2 + \cdots + c_n u^n = \sum_{k=0}^n c_ku^k$$</li></ul></li></ul><h3 id=matrix-equation-for-parametric-curve>Matrix Equation for parametric curve</h3><ul><li>We can write the 3 euqations for x(u), y(u), and z(u) in one matrix equation:<ul><li>$$p(u) = \begin{bmatrix} x(u) \ y(u) \ z(u) \end{bmatrix} = \begin{bmatrix} \sum_{k=0}^n u^k c_{xk} \ \sum_{k=0}^n u^k c_{yk} \ \sum_{k=0}^n u^k c_{zk} \end{bmatrix} = \sum_{k=0}^n u^k \begin{bmatrix} c_{xk} \ c_{yk} \ c_{zk} \end{bmatrix} = \sum_{k=0}^n u^k c_k$$</li></ul></li></ul><h3 id=degree-of-the-polynomial>Degree of the Polynomial</h3><ul><li>Tradeoff:<ul><li>High degree can have rapid changes and lots of turns, but it requires more computation and curve may not be as smooth.</li><li>Low degree means a smoother curve, but it may not fit the data as well.</li></ul></li><li>Compromise: Use low degree polynomials with short curve segments. Cubic polynomial work well.</li></ul><h3 id=multiple-curve-segments>Multiple curve segments</h3><ul><li>We usually want to define a curve segment between two endpoints. We define the curve between u=0 and u=1, so that:<ul><li>$p(0) = p_0$ and $p(1) = p_1$</li></ul></li><li>Longer curves are composed of multiple segments. We would like the connection between curves to be as smooth as possible.</li></ul><h3 id=interpolation>Interpolation</h3><ul><li>If one point provides 3 equations and 12 unknowns, then 4 points provide 12 equations with 12 unknowns.</li><li>We can choose any value of u to correspond to the 4 points, so we will choose to divide the interval 0 to 1 evenly(0, 1/3, 2/3, 1).</li></ul><hr><h2 id=pca>PCA</h2><h3 id=principal-components>Principal Components</h3><ul><li>All principal components(PCs) start at the origin</li><li>First PC is direction of maximum variance from origin</li><li>Subsequent PCs are orthogonal to 1st PC and describe maximum residual variance</li></ul><h3 id=principal-component-analysis>Principal Component Analysis</h3><ul><li><p>For an arbitrary set of N vertices $P_1, P_2, &mldr;, P_N$</p></li><li><p>Mean position : $m = \frac{1}{N} \sum_{i=1}^N P_i$</p></li><li><p>3x3 covariance matrix : $C= \frac{1}{N} \sum_{i=1}^N(P_i - m ) (P_i - m)^T$:</p><ul><li>Represents the corrleation between each pair of the $x,y,$ and $z$ coordinates</li></ul></li><li><p>Covariance matrix entries:</p><ul><li>$$C_{11} = \frac{1}{N} \sum_{i=1}^N(x_i - m_x)^2, C_{12} = C_{21} = \frac{1}{N} \sum_{i=1}^N (x_i - m_x)(y_i - m_y)$$</li><li>$$C_{22} = \frac{1}{N} \sum_{i=1}^N(y_i - m_y)^2, C_{13} = C_{31} = \frac{1}{N} \sum_{i=1}^N (x_i - m_x)(z_i - m_z)$$</li><li>$$C_{33} = \frac{1}{N} \sum_{i=1}^N(z_i - m_z)^2, C_{23} = C_{32} = \frac{1}{N} \sum_{i=1}^N (y_i - m_y)(z_i - m_z)$$</li></ul></li><li><p>An entry of zero : no corrleation</p></li><li><p>$C$ is diagonal matrix : three coordinates are uncorrelated</p></li><li><p>We want to transform points so that the covariance matrix is diagonal</p><ul><li>$$\begin{aligned}C&rsquo; &= \frac{1}{N} \sum_{i=1}^N(AP_i - Am)(AP_i - Am)^T \ &= \frac{1}{N} \sum_{i=1}^N A(P_i - m)(P_i - m)^T A^T \ &= ACA^T \end{aligned}$$</li></ul></li><li><p>Find $A$ using eigenvectors:</p><ul><li>Rows of $A$ are unit eigenvectors sorted by eigenvalues in decreasing order</li></ul></li></ul><h3 id=higher-diemnsions>Higher Diemnsions</h3><ul><li>Suppose data poitns are N-dimensional:<ul><li>Same procedure applies :$C&rsquo; = ACA^T$</li><li>The eigenvectors define a new coordinates:<ul><li>eigenvector with largest eigenvalue captures the most variation among training vectors</li><li>eigenvector with smallest eigenvalue has least variation</li></ul></li><li>We can compress the data by only using the top few eigenvectors:<ul><li>corresponds to choosing a &ldquo;linear subspace&rdquo;</li></ul></li></ul></li></ul><h2 id=decomposition>Decomposition</h2><h3 id=trianglular-systems>Trianglular Systems</h3><ul><li><p>Lower triangular matrix $L$:</p><ul><li>Square matrix for which $L_{ij} = 0$ when $i &lt; j$</li></ul></li><li><p>Linear system $Lx = r$</p></li><li><p>$$\begin{bmatrix} L_{11} & 0 & \cdots & 0 \ L_{21} & L_{22} & \cdots 0 \ \vdots & \vdots & \ddots & \vdots \ L_{n1} & L_{n2} & \cdots & L_{nn} \end{bmatrix} \begin{bmatrix} x_1 \ x_2 \ \vdots \ x_n\end{bmatrix} = \begin{bmatrix} r_1 \ r_2 \ \vdots \ r_n \end{bmatrix}$$</p></li><li><p>Forward substitution:</p><ul><li>$$x_i = \frac{1}{L_{ii}} ( r_i - \sum_{k=1}^{i-1} L_{ik}x_k)$$</li></ul></li><li><p>Upper triangle</p><ul><li>Square matrix for which $U_{ij} = 0$ when $i > j$</li></ul></li><li><p>$$\begin{bmatrix} U_{11} & U_{12} & \cdots & U_{1n} \ 0 & U_{22} & \cdots U_{2n} \ \vdots & \vdots & \ddots & \vdots \ 0 & 0 & \cdots & U_{nn} \end{bmatrix} \begin{bmatrix} x_1 \ x_2 \ \vdots \ x_n\end{bmatrix} = \begin{bmatrix} r_1 \ r_2 \ \vdots \ r_n \end{bmatrix}$$</p></li><li><p>Backward substitution:</p><ul><li>$$x_i = \frac{1}{U_{ii}} ( r_i - \sum_{k=i+1}^{n} U_{ik}x_k)$$</li></ul></li></ul><h3 id=lu-decomposition>LU Decomposition</h3><ul><li><p>$LU = M$</p></li><li><p>$$\begin{bmatrix} L_{11} & 0 & \cdots & 0 \ L_{21} & L_{22} & \cdots 0 \ \vdots & \vdots & \ddots & \vdots \ L_{n1} & L_{n2} & \cdots & L_{nn} \end{bmatrix} \begin{bmatrix} U_{11} & U_{12} & \cdots & U_{1n} \ 0 & U_{22} & \cdots U_{2n} \ \vdots & \vdots & \ddots & \vdots \ 0 & 0 & \cdots & U_{nn} \end{bmatrix} = \begin{bmatrix} M_{11} & M_{12} & \cdots & M_{1n} \ M_{21} & M_{22} & \cdots & M_{2n} \ \vdots & \vdots & \ddots & \vdots \ M_{n1} & M_{n2} & \cdots & M_{nn} \end{bmatrix}$$</p></li><li><p>$$M_{ij} = \sum_{k=1}^i L_{ik} U_{kj}, \text{ if } i \le j$$</p></li><li><p>$$M_{ij} = \sum_{k=1}^j L_{ik} U_{kj}, \text{ if } i \ge j$$</p></li><li><p>$Mx = r \Rightarrow LUx = r$:</p><ul><li>Let $Ux = y$</li><li>Solve $Ly = r$</li><li>Solve $Ux = y$</li></ul></li><li><p>Doolittle&rsquo;s Method:</p><ul><li>$L_{ii} \equiv 1, i =1, 2, &mldr;, n$</li></ul></li><li><p>We can express L and U in one matrix:</p><ul><li>$$D = \begin{bmatrix} U_{11} & U_{12} & U_{13} & \cdots & U_{1n} \ L_{21} & U_{22} & U_{23} & \cdots & U_{2n} \ L_{31} & L_{32} & U_{33} & \cdots & U_{3n} \ \vdots & \vdots & \vdots & \ddots & \vdots \ L_{n1} & L_{n2} & L_{n3} & \cdots & U_{nn} \end{bmatrix}$$</li></ul></li><li><p>Solve U and L for each column j for top to bottom:</p><ul><li>$$U_{1j} = M_{1j}; L_{i1} = \frac{M_i1}{U_{11}}$$</li><li>$$U_{ij} = M_{ij} - \sum_{k=1}^{i-1} L_{ik} U_{kj}, \text{ if } i > 1$$</li><li>$$L_{ij} = \frac{1}{U_{jj}} ( M_{ij} - \sum_{k=1}^{j - 1} L_{ik} U_{kj}), \text{ if } j > 1$$</li></ul></li></ul><h3 id=error-reduction>Error Reduction</h3><ul><li>Suppose we solve a linear system $Mx = r$:<ul><li>If we obtained a solution $x= x_0$, $x_0$ usually slightly different from the true solution due to round-off error</li><li>Thus $Mx_0 = r_0$:<ul><li>$M(x + \Delta x) = r + \Delta r$, where $\Deltax = x_0 - x, \Deltar = r_0 - r$</li><li>$M \Delta x = \Delta r$</li><li>$M \Delta = M x_0 - r$</li><li>solve for the error $\Delta x$ and improve the solution : $x = x_0 - \Delta x$</li></ul></li></ul></li></ul><h3 id=matrix-inversion>Matrix Inversion</h3><ul><li>Matrix inverse can be computed in a column-by-column method</li><li>$$MM^{-1} = I$$</li></ul><h2 id=iterative>Iterative</h2><h3 id=solving-large-lienar-systems>Solving Large Lienar Systems</h3><ul><li>Global illumination (Radiosity):<ul><li>Solve $B = (I - \rho F)^{-1} E$</li><li>Need to solve for a large matrix</li><li>It takes too much time with conventional methods</li></ul></li></ul><h3 id=jacobi-method>Jacobi Method</h3><ul><li>Iterative or approximate method</li><li>Solve a linear system $AX=B$</li><li>General formula:<ul><li>$$x_i^{(k+1)} = \frac{1}{a_{ii}} ( b_i - \sum_{j \not = i} a_{ij} x_{j}^{(k)}), i=1,2, &mldr;,n$$</li></ul></li><li>Convergence check:<ul><li>$\epsilon_i = \vert \frac{x_i^j x_i^{j-1}}{x_i^j} \vert &lt; \epsilon_s$</li></ul></li></ul><h3 id=gauss-seidel-method>Gauss-Seidel Method</h3><ul><li>Speed up the convergence</li><li>Use better approximations when possible</li><li>$x_{k+1}$ is a better approximation to $x$ than $x_k$.</li><li>$x_i^{(k+1)} = \frac{1}{a_{ii}} (b_i - \sum_{j &lt; i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)}), i = 1,2,&mldr;,n$</li></ul><h3 id=convergence>Convergence</h3><ul><li>Sufficient condition for convergence:<ul><li>Method always converge if the matrix $A$ is diagonally dominant</li><li>Row diagonal dominance: For each row, the absolution value of the diagonal term is greater than the sum of absolute values of other terms</li><li>$\vert a_{ii} \vert > \sum_{i \not = j} \vert a_{ij} \vert$</li></ul></li></ul><h3 id=relaxation-in-gauss-seidel>Relaxation in Gauss-Seidel</h3><ul><li>Slight modification for improving convergence</li><li>$x_i^{(k+1)} = \lambda x_i^{(k+1)} + (1 - \lambda) x_i^{(k)}$, $0 &lt; \lambda &lt; 2$:<ul><li>If $\lambda = 1$, no changes</li><li>If $0 &lt; \lambda &lt; 1$, underrelaxation helps to make a nonconvergent system converge</li><li>If $1 &lt; \lambda &lt; 2$, overrelaxation to accelerate the converge</li></ul></li></ul><h2 id=interpolation-1>Interpolation</h2><h3 id=interpolation-2>Interpolation</h3><ul><li>Iterpolation:<ul><li>A process of finding a function that passes through a given set of points $(x_i, y_i)$ (data fitting or curve fitting)</li><li>It can be used to estimate variable $y$ corresponding to unknown $x \in [a, b] - { x_i }$</li></ul></li><li>Extrapolation:<ul><li>Estimate variable $y$ corresponding to $x &lt; x_0$ or $x > x_n$</li></ul></li><li>Purpose of interpolation:<ol><li>Replace a set of data points ${(x_i, y_i)}$ with a function given analytically.</li><li>Approximate functions with simpler ones, usually polynomials or &lsquo;piecewise polynomials&rsquo;.</li></ol></li></ul><h3 id=linear-interpolation>Linear Interpolation</h3><ul><li>Interpolation with straight line:<ul><li>Let two data poitns $(x_0, y_0)$ and $(x_1, y_1)$ be given. There is a unique straight line passing through these points. We can write the formula for a straight line as:<ul><li>$$P_1(x) = y_0 + (\frac{y_1 - y_0}{x_1 - x_0})(x - x_0)$$</li></ul></li></ul></li></ul><h3 id=quadratic-interpolation>Quadratic Interpolation</h3><ul><li><p>We want to find a polynomial:</p><ul><li>$P_2(x) = a_0 + a_1 x + a_2 x^2$</li><li>which satisfies:<ul><li>$P_2(x_i) = y_i, i = 0, 1, 2$</li><li>for given data points $(x_0, y_0), (x_1, y_1), (x_2, y_2)$</li></ul></li></ul></li><li><p>Lagrange interpolation:</p><ul><li>$P_2(x) = y_0 L_0(x) + y_1 L_1(x) + y_2 L_2(x)$</li><li>with:<ul><li>$L_0(x) = \frac{(x - x_1)(x - x_2)}{x_0 - x_1)(x_0 - x_2)}$</li><li>$L_1(x) = \frac{(x - x_0)(x - x_2)}{x_1 - x_0)(x_1 - x_2)}$</li><li>$L_2(x) = \frac{(x - x_0)(x - x_1)}{x_2 - x_0)(x_2 - x_1)}$</li></ul></li></ul></li><li><p>Uniqueness:</p><ul><li>Can there be another polynomial, call it $Q(x)$, for which:<ul><li>$deg(Q) \le 2$</li><li>Let $R(x) = P_2(x) - Q(x)$</li><li>From the properties of $P_2$ and $Q$, we have $deg(R) \le 2$</li><li>$R(x_i) = P_2(x_i ) - Q(x_i) = y_i - y_i = 0$</li><li>Three zeros for quadratic function</li><li>So, $R(x) = 0$ for all x, $Q(x) = P_2(x)$ for all x</li></ul></li></ul></li></ul><h3 id=higher-degree-interpolation>Higher Degree Interpolation</h3><ul><li>Polynomail function of degree $n$:<ul><li>$deg(P_n) \le n$</li><li>$P_n(x_i) = y_i, i = 0, 1, &mldr;, n$</li><li>with data points $(x_0, y_0), &mldr;, (x_n, y_n)$</li></ul></li><li>Lagrange interpolation:<ul><li>$P_n(x) = y_0 L_0(x) + y_1 L_1(x) + \cdots + y_n L_n(x)$</li><li>$L_k(x) = \frac{(x - x_0) &mldr; (x - x_{k-1})(x - x_{k+1}) &mldr; (x - x_n)}{(x_k - x_0) &mldr; (x_k - x_{k-1})(x_k - x_{k+1})&mldr;(x_k - x_n)}$</li></ul></li></ul><h3 id=newton-polynomials>Newton Polynomials</h3><ul><li>It is sometimes useful to find $P_1(x), P_2(x), &mldr;, P_N(x)$</li><li>Lagrange polynomials:<ul><li>No constructive relationship between $P_{N-1}(x)$ and $P_N(x)$</li></ul></li><li>Newton polynomials:<ul><li>Recursive pattern</li><li>$P_N(x) = P_{N-1}(x) + g_N(x)$</li><li>$g_N(x) = a_N(x - x_0)(x - x_1) &mldr; (x - x_{N-1})$</li></ul></li><li>To find $a_k$ for all polynomials $P_1(x), P_2(x), &mldr;, P_N(x)$ that approximate a given function $f(x)$</li><li>$P_k(x)$ is based on the centers $x_0, x_1, &mldr;, x_k$</li><li>$a_1$ : slope of the line between $(x_0, f(x_0))$ and $(x_1, f(x_1))$</li></ul><h4 id=general-divided-differnce>General Divided Differnce</h4><ul><li>Given $n + 1$ distinct points $x_0, &mldr;, x_n$, with $n \ge 2$, define:<ul><li>$$f[x_0, &mldr;, x_n] = \frac{f[x_1, &mldr;, x_n] - f[x_0, &mldr;, x_{n-1}]}{x_n - x_0}$</li><li>$$f[x_0, &mldr;l, x_n] = \frac{1}{n!} f^{(n)}(xc)$</li><li>for some $c$ intermediate to the points ${x_0, &mldr;, x_n }$</li></ul></li></ul><h3 id=spherical-linear-interpolation>Spherical Linear Interpolation</h3><ul><li>$$q(t) = \frac{sin \theta (1 - t)}{sin \theta} q_1 + \frac{sing \theta t}{sin \theta} q_2$$</li></ul><h3 id=bilinear-interopolation>Bilinear Interopolation</h3><ul><li>$$f(R_1) \approx \frac{x_2 - x}{x_2 - x_1} f(Q_{11}) + \frac{x - x_1}{x_2 - x_1} f(Q_{21})$$</li><li>$$f(R_2) \approx \frac{x_2 - x}{x_2 - x_1} f(Q_{12}) + \frac{x - x_1}{x_2 - x_1} f(Q_{22})$$</li><li>$$f(P) \approx \frac{y_2 - y}{y_2 - y_1} f(R_1) + \frac{y - y_1}{y_2 - y_1} f(R_2)$$</li></ul><h2 id=least-squares>Least Squares</h2><h3 id=least-squares-line>Least-Squares Line</h3><ul><li>Data points $(x_1, y_1), &mldr;, (x_N, y_N), {x_k }$ are distinct</li><li>Numerical method is to determine $y=f(x)$</li><li>Lienar approximation: $y = f(x) = Ax + B$</li><li>If there is an error in the data points:<ul><li>$$f(x_k) = y_k + e_k$$,</li><li>where $e_k$ is the error</li></ul></li></ul><h3 id=errors>Errors</h3><ul><li>Erros: $e_k = f(x_k) - y_k$ for $1 \le k \le N$</li><li>Norms to measure how far the curve $y=f(x)$ lies from the data</li><li>Maximum error: $E_{\infty} (f) = {max}_{1 \le k \le N} { \vert f(x_k) - y_k }$</li><li>Average error: $E_1 (f) = \frac{1}{N} \sum_{k=1}^N \vert f(x_k) - y_k \vert$</li><li>Root-mean-squre error: $E_2 (f) = ( \frac{1}{N]} \sum_{k=1}^N \vert f(x_k) - y_k \vert ^2)^{1/2}$</li><li>Maximum error is sensitive to extreme data</li><li>Average is often used since it&rsquo;s easy to compute</li><li>RMS is often used when statistical nature is used</li></ul><h3 id=least-squares-line-1>Least-Squares Line</h3><ul><li><p>A best-fitting line is minimizing the rror</p></li><li><p>RMS is the traditional choice</p></li><li><p>Coefficients of least-squares line $y=Ax + B$ are the solution of the followign linear system:</p><ul><li>$$(\sum_{k=1}^N x_k^2) A + (\sum_{k=1}^N x_k) B = \sum_{k=1}^N x_k y_k$$</li><li>$$\sum_{k=1}^N x_k) A + NB = \sum_{k=1}^N y_k$</li></ul></li><li><p>Eror is teh vertical distance between $(x_k, y_k)$ and $(x_k, Ax_k + B)$ and we want minimize their squared sum</p></li><li><p>$E(A, B)$ is minimum when the partical derivatives $\partial E / \partial A$ and $\partial E / \partial B$ equal to zero</p></li><li><p>$$\begin{aligned} \frac{\partial E(A, B)}{\partial A} &= \sum_{k=1}^N 2(A x_k + B - y_k)(x_k) \ &= 2 \sum_{k=1}^N (A x_k^2 + B x_k - x_k y_k) \end{aligned}$$</p><ul><li>$$0 = A \sum_{k=1}^N x_k^2 = B \sum_{k=1}^N x_k - \sum_{k=1}^N x_k y_k$$</li></ul></li><li><p>$$\begin{aligned} \frac{\partial E(A, B)}{\partial B} &= \sum^N 2(Ax_k + B - y_k) \ &= 2 \sum_{k=1}^N (A x_k + B - y_k)\end{aligned}$</p><ul><li>$$0 = A \sum_{k=1}^N x_k + NB - \sum_{k=1}^N y_k$$</li></ul></li></ul><h3 id=power-fit>Power Fit</h3><ul><li>Fitting $f(x) = Ax^M$, where $M$ is known</li><li>Least-squares power curve: minimize the RMS (Root-Mean-Square) error:<ul><li>$$A = \sum_{k=1}^N x_k^M y_k) / (\sum_{k=1}^N x_k^{2M})$$</li></ul></li><li>Least-Squares method: minimize $E(A)$:<ul><li>$$E(A) = \sum (Ax_k^M - y_k)^2$$</li></ul></li><li>Solve $E&rsquo;(A) = 0$:<ul><li>$$E&rsquo;(A) = 2 \sum (A x_k^{2M} - x_k^M y_k)$$</li><li>$$0 = A \sum x_k^{2M} - \sum x_k^M y_k$$</li></ul></li></ul><h3 id=data-linearization>Data Linearization</h3><ul><li>Data linearization : process to transform a non-linear relation into a linear relation</li></ul><h3 id=matrix-formulation>Matrix Formulation</h3><ul><li>Lienar system:<ul><li>$F^TFC = F^TY$ for the coeeficient matrix $C$</li></ul></li></ul><h3 id=polynomial-fitting>Polynomial Fitting</h3><ul><li>Polynomial function:<ul><li>$$f(x) = c-1 + c_2 x + c_3 x^2 + \cdots + c_{M+1} x^M$$</li></ul></li><li>Least-squares parabola</li></ul><h3 id=nonlinear-least-squares-method>Nonlinear Least-Squares Method</h3><ul><li>Nonlinear system can be solved with Newton&rsquo;s Method:<ul><li>Time consuming</li><li>Requires good starting values for $A$ and $C$</li></ul></li><li>Minimize $E(A, C)$ directly using optimization methods</li></ul><h2 id=optimization>Optimization</h2><ul><li>Methods for locating extrema(maxima or minima) of functions</li></ul><h3 id=functions-of-one-variable>Functions of One Variable</h3><ul><li><p>The function $f$ has a local minimum value at $x=p$, if there exists an open interval $i$ containing $p$ s.t. $f(p) \le f(x)$ for all $ x \in I$</p></li><li><p>$f$ has a local maximum value at $x = p$, if there exists an open interval $i$ containing $p$ s.t. $f(x) \le f(p)$ for all $ x \in I$</p></li><li><p>$f$ has a local extremum value at $x=p$, if it has local minimum or maximum value at $x=p$.</p></li><li><p>Theorem 1:</p><ul><li>Suppose that $f(x)$ is continuous on $i= [a,b]$ and is differentiable on $(a, b)$:<ul><li>If $f&rsquo;(x) > 0$ for all $x \in 9a, b)$, then $f(x)$ is increasing on $I$.</li><li>If $f&rsquo;(x) &lt; 0$ for all $x \in (a, b)$, then $f(x)$ is decreasing on $I$.</li></ul></li></ul></li><li><p>Theorem 3:</p><ul><li>Suppose that $f(x)$ is continuous on $i= [a,b]$ and $f&rsquo;(x)$ is defined for all $x \in (a, b)$, except possibly at $x=p$:<ul><li>If $f&rsquo;(x) &lt; 0$ on $(a, p)$ and $f&rsquo;(x) > 0$ on $(p, b)$, then $f(p)$ is a local minimum.</li><li>If $f&rsquo;(x) > 0$ on $(a, p)$ and $f&rsquo;(x) &lt; 0$ on $(p, b)$, then $f(p)$ is a local maximum.</li></ul></li></ul></li><li><p>Theorem 4:</p><ul><li>Suppose that $f(x)$ is continuous on $i= [a,b]$ and $f&rsquo;,f&rsquo;&rsquo;$ is defined for all $x \in (a, b)$, Also, $p \in (a, b)$ is a critical point where $f&rsquo;(p) = 0$:<ul><li>If $f&rsquo;&rsquo;(p) > 0$, then $f(p)$ is a local minimum of $f$.</li><li>If $f&rsquo;&rsquo;(p) &lt; 0$, then $f(p)$ is a local maximum of $f$.</li><li>If $f&rsquo;&rsquo;(p) = 0$, then this test is inconclusive.</li></ul></li></ul></li></ul><h3 id=bracketing-search-methos>Bracketing Search Methos</h3><ul><li>Evaluate the function many times and search for a local minimum/maximum</li><li>To reduce the number of function evaluations, a good strategy is needed for determining where the function is to be evaluated</li><li>Definition:<ul><li>The function $f(x)$ is unimodal on $I = [a, b]$, if there exists a unique number $p \in I$ such that:<ul><li>$f(x)$ is decreasing on $[a, p]$</li><li>$f(x)$ is increasing on $[p, b]$</li></ul></li></ul></li></ul><h3 id=golden-ratio-search>Golden Ratio Search</h3><ul><li>If $f(x)$ is unimodal on $[a,b]$, then it is possible to replace the interval with a subinterval on which $f(x)$ takes on its miniumum value</li><li>Select two interior points $c &lt; d \Rightarrow a &lt; c &lt; d &lt; b$</li><li>$f(c), f(d) &lt; max(f(a), f(b))$:<ul><li>When $f(c) \le f(d)$:<ul><li>The minimum must occur in $[a,d]$</li><li>New subinterval $[a, d]$</li></ul></li><li>When $f(d) &lt; f(c)$:<ul><li>The minimum must occur in $[c, d]$</li><li>New subinterval $[c, b]$</li></ul></li></ul></li><li>$c = ra + (1 - r)b$</li><li>$d = (1 - r)a + rb$</li><li>$r = \frac{-1 + \sqrt 5}{2}$</li></ul><h3 id=fibonacci-search>Fibonacci Search</h3><ul><li><p>The value r is not constant on each subinterval</p></li><li><p>Number of subintervals (iterations) is predetermined and based on the specified tolerance</p></li><li><p>The interior poitns $c_k$ and $d_k$ of the k-th subinterval $[a_k, b_k]$ are:</p><ul><li>$c_k = a_k + (1 - \frac{F_{n-k-1}{F_{n-k}}})(b_k - a_k)$</li><li>$d_k = a_k + \frac{F_{n-k-1}}{F_{n-k}}(b_k - a_k)$</li></ul></li><li><p>For a tolerance $\epsilon$, find the smallest value of $n$ such that:</p><ul><li>$$\frac{b_0 - a_0}{F_n} &lt; \epsilon, F_n > \frac{b_0 - a_0}{\epsilon}$$</li></ul></li></ul><h3 id=multidimensional-optimization>Multidimensional Optimization</h3><ul><li>Find the extremum of a function of several variables:<ul><li>Direct method (No derivatives)</li><li>Gradient method (Use derivatives)</li></ul></li></ul><h4 id=direct-methods---random-search>Direct Methods - Random Search</h4><ul><li>Based on evaluation of the fuction randomly at selected values of the independent variables.</li><li>If a sufficient number of samples are ocnducted, the optimum will be eventually located</li><li>Advantages:<ul><li>Works even for discontiuous and nondifferentiable functions.</li></ul></li><li>Disadvantages:<ul><li>As the number of independent variables grows, the task can become onerous.</li><li>Not efficient, it does not account for the behavior of underlying function</li></ul></li></ul><h4 id=direct-method---univariate-and-pattern-search>Direct Method - Univariate and Pattern Search</h4><ul><li>More efficient than random search and still doesn&rsquo;t require derivative evaluation</li><li>The baisc strategy:<ul><li>Change one variable at a time while the other variables are held constant.</li><li>Problem is reduced to a sequence of one-dimensional searches that can be solved by variety of methods</li><li>The search becomes less efficient as you approach the maximum.</li></ul></li></ul><h3 id=gradient>Gradient</h3><ul><li>Graident of a function $f$, $\nabla f$, tells us:<ul><li>$$\nabla f = \begin{bmatrix} \frac{\partial f}{\partial x_1} \ \vdots \ \frac{\partial f}{\partial x_n} \end{bmatrix}$</li></ul></li></ul><h3 id=hessian-matrix-of-hessian-of-f>Hessian Matrix (of Hessian of $f$)</h3><ul><li>$$H = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partialx_1 \partial x} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \ \vdots & \vdots & \ddots & \vdots \ \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partialx_n^2} \end{bmatrix}$</li><li>Also known as the matrix of second partial derivatives.</li><li>It provides a way to discern if a funciton has reached an optimum or not.</li></ul><h2 id=solution-of-nonlinear-equation>Solution of Nonlinear Equation</h2><h3 id=basis-of-bisection-method>Basis of Bisection Method</h3><ul><li>An equation $f(x) = 0$, where $f(x)$ is a real continuous function, has at least one root between $x_l$ and $x_u$ if $f(x_l)f(x_u) &lt; 0$.</li><li>If</li></ul></div><hr><div class=list-files><ul class=section-tree></ul></div></article><script src=/js/wikilink.js></script><div><script src=https://utteranc.es/client.js repo=minuk-dev/minuk-dev.github.io issue-term=pathname theme=github-dark crossorigin=anonymous async></script></div></main><footer class=footer><div class=footer-left></div><div class=footer-right><ul class=social><li><a href=/about>About</a></li><li><a href=https://github.com/minuk-dev>Github</a></li></ul></div></footer></body><script src=/js/dir_toggle.js></script><script src=/js/codeblock_copy.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.1/font/bootstrap-icons.css><script type=module>
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs";
mermaid.initialize({
  startOnLoad: true,
  theme: "dark",
});
</script></html>
<!doctype html><html lang=ko-kr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>lectures/algorithm</title><link rel=icon href=https://makerdark98.dev/images/Rb.png><meta property="og:title" content="lectures/algorithm"><meta property="og:description" content="MD98 블로그 : lectures/algorithm"><meta property="og:image" content="https://makerdark98.dev/images/Rb.png"><style>html body{font-family:raleway,sans-serif;background-color:#fff}:root{--accent: #00a3d2;--border-width:  5px }</style><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Raleway"><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css integrity=sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u crossorigin=anonymous><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css integrity=sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN crossorigin=anonymous><link rel=stylesheet href=/css/main.css><link rel=stylesheet href=/css/copy-btn.css><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script src=/js/copy-btn.js></script><script src=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js integrity=sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-98056974-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-98056974-1');</script><script>window.MathJax={jax:["input/TeX","output/CommonHTML"],extensions:["tex2jax.js","asciimath2jax.js","mml2jax.js","MathMenu.js","MathZoom.js"],TeX:{extensions:["AMSmath.js","AMSsymbols.js","autoload-all.js"]},tex2jax:{inlineMath:[['$','$'],["\\(","\\)"]],processEscapes:true},showProcessingMessages:false,messageStyle:"none",menuSettings:{zoom:"Click"},AuthorInit:function(){MathJax.Hub.Register.StartupHook("End",function(){var timeout=false,delay=250;var shrinkMath=function(){var dispFormulas=document.getElementsByClassName("formula");if(dispFormulas){var contentTest=document.getElementsByTagName("body")[0];var nodesWidth=contentTest.offsetWidth;var mathIndent=MathJax.Hub.config.displayIndent;var mathIndentValue=mathIndent.substring(0,mathIndent.length-2);for(var i=0;i<dispFormulas.length;i++){var dispFormula=dispFormulas[i];var wrapper=dispFormula.getElementsByClassName("MathJax_Preview")[0].nextSibling;var child=wrapper.firstChild;wrapper.style.transformOrigin="center";var oldScale=child.style.transform;var newValue=Math.min(0.80*dispFormula.offsetWidth/child.offsetWidth,1.0).toFixed(2);var newScale="scale("+newValue+")";if(!(newScale===oldScale)){wrapper.style.transform=newScale;wrapper.style["margin-left"]=Math.pow(newValue,4)*mathIndentValue+"px";var wrapperStyle=window.getComputedStyle(wrapper);var wrapperHeight=parseFloat(wrapperStyle.height);wrapper.style.height=""+(wrapperHeight*newValue)+"px";if(newValue==="1.00"){wrapper.style.cursor="";wrapper.style.height="";}
else{wrapper.style.cursor="zoom-in";}}}}};shrinkMath();window.addEventListener('resize',function(){clearTimeout(timeout);timeout=setTimeout(shrinkMath,delay);});});}};(function(d,script){script=d.createElement('script');script.type='text/javascript';script.async=true;script.onload=function(){};script.src='https://cdn.mathjax.org/mathjax/latest/MathJax.js';d.getElementsByTagName('head')[0].appendChild(script);}(document));</script><meta name=google-site-verification content="g_3tJyj-KkW-_wKx7Ij5GimHV1nKPZXetCz8ydbBAfA"></head><body><nav class="navbar navbar-default navbar-fixed-top"><div class=container><div class=navbar-header><a class="navbar-brand visible-xs" href=#>lectures/algorithm</a>
<button class=navbar-toggle data-target=.navbar-collapse data-toggle=collapse>
<span class=icon-bar></span><span class=icon-bar></span><span class=icon-bar></span></button></div><div class="collapse navbar-collapse"><ul class="nav navbar-nav"><li><a href=/>Home</a></li><li><a href=/wiki/>Wiki</a></li><li><a href=/posts/>Posts</a></li><li><a href=/about/>About</a></li></ul><ul class="nav navbar-nav navbar-right"><li class=navbar-icon><a href=mailto:makerdark98@gmail.com><i class="fa fa-envelope-o"></i></a></li><li class=navbar-icon><a href=https://github.com/makerdark98/><i class="fa fa-github"></i></a></li><li><div style=max-width:250px;display:inline-block;max-height:40px><script async src="https://cse.google.com/cse.js?cx=003491619885022567520:wnnypdnx4aj"></script><div class=gcse-search></div></div></li></ul></div></div></nav><main><div class=navigator style=display:flex><div style=display:flex><div class=parent-doc style=flex:none><button class="btn btn-link" onclick="(function(elem){elem.querySelector('a').click();})(this);">
<i class="fa fa-arrow-left"></i>[[lecture]]</button></div></div><div style=margin-left:auto><div style=display:flex><div style=margin-left:auto><div class=wiki-history></div></div></div><script src=/js/localhistory.js></script><script>var now="algorithm"
pushHistory(now);</script></div></div><div><h2>lectures/algorithm</h2><a href=https://github.com/makerdark98/makerdark98.github.io/blame/master/src/content/wiki/lectures/algorithm.md><h5>created : Tue, 07 Apr 2020 20:37:08 +0900</h5><h5>modified : Sat, 27 Jun 2020 15:23:12 +0900</h5></a><a href=https://makerdark98.dev/tags/lecture><kbd class=item-tag>lecture</kbd></a>
<a href=https://makerdark98.dev/tags/algorithm><kbd class=item-tag>algorithm</kbd></a></div><aside class=navbar id=nav-toc style=text-align:left><nav id=TableOfContents><ul><li><a href=#advantages-of-divide-and-conquer>Advantages of Divide and Conquer</a></li><li><a href=#implementation-issues>Implementation issues</a></li><li><a href=#general-method>General Method</a></li><li><a href=#divide-and-conquer-strategy>Divide and Conquer Strategy</a></li></ul><ul><li><a href=#linear-time-merging>Linear-time merging</a></li><li><a href=#analysis-of-merge-sort>Analysis of merge sort</a></li></ul><ul><li><a href=#linear-time-partitioning>Linear-time Partitioning</a></li></ul><ul><li><a href=#adjacency-matrix>Adjacency Matrix</a></li><li><a href=#adjacency-list>Adjacency List</a></li></ul><ul><li><a href=#dijkstras-algorithm>Dijkstra&rsquo;s Algorithm</a></li><li><a href=#algorithm--complexity>Algorithm : Complexity</a></li></ul><ul><li><a href=#complexity>Complexity</a></li></ul><ul><li><a href=#complexity-1>Complexity</a></li></ul><ul><li><a href=#floyds-algorithm-time-complexity--on3>Floyd&rsquo;s algorithm (Time complexity : O(n^3))</a></li><li><a href=#multi-stage-graphs-time-complexity--thetav--e>Multi-Stage Graphs (Time complexity : Theta(|V| + |E|))</a></li></ul><ul><li><a href=#search>Search</a></li><li><a href=#traversal>Traversal</a></li></ul><ul><li><a href=#inorder-traversal>Inorder Traversal</a></li><li><a href=#preorder-traversal>Preorder Traversal</a></li><li><a href=#postoder-traversal>Postoder Traversal</a></li></ul><ul><li><a href=#non-recursive-inorder-traversal>Non-Recursive Inorder Traversal</a></li><li><a href=#non-recursive-preorder-traversal>Non-Recursive Preorder Traversal</a></li><li><a href=#non-recursive-postorder-traversal>Non-Recursive Postorder Traversal</a></li></ul></nav></aside><div align=start class=content data-spy=scroll data-offset=20 data-target=#nav-toc style=position:relative><ol start=8><li>Basic Sorting algorithm
The most common uses of sorted sequences are</li></ol><ul><li>making lookup or search efficient;</li><li>making merging of sequences efficient</li><li>enable processing of data in a defined order</li></ul><h1 id=sorting-algorithm>Sorting algorithm</h1><p>The output of any sorting algorithm must satisfy two conditions</p><ul><li>The output is in non-decreasing order: each element is no smaller than the previous element according to the desired total order</li><li>The output is a permutation meaning that a reordering, yet retaining all of the original elements of the input.</li></ul><h1 id=classification-of-sorting-algorithms>Classification of Sorting Algorithms</h1><ul><li>Computational Complexity : O(nlogn)</li><li>Memory usage : O(1), sometimes O(log(n))</li><li>Recursion</li><li>Stability</li><li>Whether or not they are a comparison sort</li><li>Adaptability</li></ul><h1 id=common-sorting-algorithms>Common sorting algorithms</h1><ul><li>Bubble sort : Exchange two adjacent elements if they are out of order. Repeat until array is sorted.</li><li>Selection sort : Find the smallest element in the array, and put it in the proper place. Swap it with the value in the first position. Repeat until array is sorted.</li><li>Insertion sort : Scan successive elements for an out-of-order item, then insert the item in the proper place.</li><li>Merge sort : Divide the list of elements in two parts, sort the two parts individually and then merge it.</li><li>Quick sort : Partition the array into two segments. In the first segment, all elements are less than or equal to the pivot value. In the second segment, all elements are greater than or equal to the pivot value. Finally, sort the two segments recursively.</li></ul><ol start=9><li>Divide and Conqure</li></ol><h1 id=divide-and-conquer-paradigm>Divide and Conquer Paradigm</h1><h2 id=advantages-of-divide-and-conquer>Advantages of Divide and Conquer</h2><ul><li>Solving difficult problems</li><li>Algorithm efficiency<ul><li>Karatsuba&rsquo;s fast multiplication method, quick and merge sort, Strassen algorithm for matrix multiplication, fast Fourier transforms.</li></ul></li><li>Parallelism<ul><li>multi-processor machines, sub-problems can be executed on different processors.</li></ul></li><li>Memory access<ul><li>An algorithm designed to exploit the cache in this way is called cache-oblivious</li><li>D&C(Divide and Conquer) algorithms can be designed for important algorithms such as sorting, FFTs, matrix multiplication to be optimal cache-oblivious algorithms</li></ul></li><li>Roundoff control</li></ul><h2 id=implementation-issues>Implementation issues</h2><ul><li>Recursion<ul><li>If D&C algorithms are naturally implemented as recursive procedures, the partial sub-problems leading to the one currently being solved are automatically stored in the procedure call stack.</li><li>A recursive function is a function that calls itself within its definition.</li></ul></li><li>Explicit stack<ul><li>If D&C algorithms are implemented by a non-recursive program that stored the partial sub-problems in some explicit data structure, such as a stack, queue, or priority queue.</li></ul></li><li>Stack size<ul><li>In recursive implementations of D&C algorithms, one must make sure that there is sufficient memory allocated for the recursion stack, otherwise the execution may fail because of stack overflow.</li></ul></li><li>Choosing the base cases</li><li>Sharing repeated subproblems</li></ul><h2 id=general-method>General Method</h2><h2 id=divide-and-conquer-strategy>Divide and Conquer Strategy</h2><ol><li><p>divide the problem instance into two or more smaller instances of the same problem, solve the smaller instances recursively, and assemble the solutions to form a solution of the original instance.</p></li><li><p>The recursion stop when an instance is reached which too small to divide</p></li><li><p>When dividing the instance, one can either use whatever division comes most easily to hand or invest time in making the division carefully so that the assembly is simplified.</p></li><li><p>Effective Sorting Algorithm</p></li></ol><h1 id=review--divide-and-conquer-algorithms>Review : Divide and conquer algorithms</h1><p><img src=https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2f9a9793-fbfb-4a26-bc7a-91a20bfb9a6d/Untitled.png alt=https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2f9a9793-fbfb-4a26-bc7a-91a20bfb9a6d/Untitled.png></p><h1 id=overview-of-merge-sort>Overview of Merge Sort</h1><p>Because we&rsquo;re using divide-and-conquer to sort, we need to decide what our subproblems are going to look like. The full problem is to sort an entire array. Let&rsquo;s say that a subproblem is to sort a subarray.</p><p>In particular, we&rsquo;ll think of a subproblem as sorting the subarray string at index p and going through index r. It will be convenient to have a notation for a subarray, so let&rsquo;s say that array[p..r] denotes this subarray of array.</p><p>Note that we&rsquo;re using &ldquo;tow-dot&rdquo; notation just to describe the algorithm, rater than a particular implementation of the algorithm in code. In terms of our notation, for an array of n elements, we can say that the original problem is to sort array[0..n-1].</p><p>Here&rsquo;s how merge sort uses divide-and-conquer:</p><ol><li>Divide by finding the number q of the position midway between p and r. Do this step the same way we found the midpoint in binary search: add p and r, divide by 2, and round down.</li><li>Conquer by recursively sorting the subarrays in each of the two subproblems created by the dividing step. That is, recursively sort the subarray array[p..q] and recursively sort the subarray array[q+1..r].</li><li>Combine by merging the two sorted subarrays back into the single sorted subarray array[p..r].</li></ol><h2 id=linear-time-merging>Linear-time merging</h2><p>THe remaining piece of merge sort is the merge function, which merges two adjacent sorted subarrays, array[p..q] and array[q+1..r] into a single sorted subarray in array[p..r]. We&rsquo;ll see how to construct this function so that it&rsquo;s as efficient as possible.</p><h2 id=analysis-of-merge-sort>Analysis of merge sort</h2><ol><li>The divide step takes constant time, regardless of the subarray size. After all, the divide step just computes the midpoint q of the indices p and r. Recall that in big-Theta notation, we indicate constant time by Theta(1).</li><li>The conquer step, where we recursively sort two subarrays of approximately n/2 elements each, takes some amount of time, but we&rsquo;ll account for that time when we consider the subproblems.</li><li>The combine step merges a total of n elements, taking Theta(n) time.</li></ol><p><img src=https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e67189ca-6023-4485-8062-7b5e04603a16/Untitled.png alt=https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e67189ca-6023-4485-8062-7b5e04603a16/Untitled.png></p><h1 id=overview-of-quick-sort>Overview of Quick Sort</h1><ol><li>Divide by choosing any element in the subarray array[p..r]. Call this element the pivot. Rearrange the elements in array[p..r] so that all elements in array[p..r] that are less than or equal to the pivot are to its left and all elements that are greater than the pivot are to its right. We call this procedure partitioning . At this point, it doesn&rsquo;t matter what order the elements to the left of the pivot are in relation to each other, and the same holds for the elements to the right of the pivot. We just care that each element is somewhere on the correct side of the pivot.</li><li>Conquer by recursively sorting the subarrays array[p..q-1] (all elements to the left of the pivot, which must be less than or equal to the pivot) and array[q+1..r]. All elements to the right of the pivot, which must be greater than the pivot.</li><li>Combine by doing nothing. Once the conquer step recursively sorts, we are done.</li></ol><h2 id=linear-time-partitioning>Linear-time Partitioning</h2><ul><li>Best-case running time</li></ul><p><img src=https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9d8abd3b-d978-4a02-9a48-a0537fc9798c/Untitled.png alt=https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9d8abd3b-d978-4a02-9a48-a0537fc9798c/Untitled.png></p><ul><li>Average-case running time</li></ul><p><img src=https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6ac383e4-4ea5-4f12-acb7-e0e9454e68a4/Untitled.png alt=https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6ac383e4-4ea5-4f12-acb7-e0e9454e68a4/Untitled.png></p><ol start=12><li>Greedy Algorithm</li></ol><h1 id=introduction>Introduction</h1><ul><li>The greedy method is a simple strategy of progressively building up a solution, one element at a time, by choosing the best possible element at each stage.</li></ul><h1 id=knapsack-problem>Knapsack Problem</h1><ul><li>p[i]/w[i] ratio</li></ul><h1 id=optimal-storage-on-tapes>Optimal Storage on Tapes</h1><h1 id=job-sequencing-with-deadlines>Job Sequencing with Deadlines</h1><ul><li>sort p_i, put last</li></ul><h1 id=optimal-merge-patterns>Optimal Merge Patterns</h1><p><img src=https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5ce9d81e-671d-45bb-b9ad-15413a88913b/Untitled.png alt=https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5ce9d81e-671d-45bb-b9ad-15413a88913b/Untitled.png></p><h1 id=huffman-codes>Huffman Codes</h1><p><img src=https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4d71a77e-cc28-4ad4-9206-d87e14b813a0/Untitled.png alt=https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4d71a77e-cc28-4ad4-9206-d87e14b813a0/Untitled.png></p><p>13.Graph Algorithm</p><h1 id=basic-definitions>Basic definitions</h1><ul><li>Graph G= (V,E) where V is a set of vertices and E is a set of edges</li><li>Directed graph G = &lt;V,E> where E consists of ordered pairs</li><li>Weighted graph G with a weight function w_g(e) where e in E</li><li>Degree of a vertex v : deg(v)<ul><li>The number of incoming edges to v: indeg(v)</li><li>The number of outgoing edges from v : outdeg(v)</li></ul></li></ul><h1 id=representation-of-graphs>Representation of Graphs</h1><ul><li>G=(V, E) where V is a set of vertices {v_1, &mldr; , v_n}</li></ul><h2 id=adjacency-matrix>Adjacency Matrix</h2><p><img src=https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bb8cd6cc-1c54-4d2e-8786-5121420ff916/Untitled.png alt=https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bb8cd6cc-1c54-4d2e-8786-5121420ff916/Untitled.png></p><h2 id=adjacency-list>Adjacency List</h2><p><img src=https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0e6d4092-6a9e-46d7-a369-8fa655795c15/Untitled.png alt=https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0e6d4092-6a9e-46d7-a369-8fa655795c15/Untitled.png></p><h1 id=path-cycles-and-subgraphs>Path, Cycles, and Subgraphs</h1><ul><li>Path P : a sequence of vertices P = (v_1, &mldr; v_k) where all i, (v_i, v_{i+1}) in E<ul><li>Simple path : v_i ≠ v_j where v_i, v_j in P</li></ul></li><li>Cycle : a sequence of vertices P = (v_1, &mldr; v_k, v_{k+1} = v_1) where all i, (v_i, v_{i+1}) in E<ul><li>Simple Cycle : v_i ≠ v_j where v_i, v_j in P except the pair v_1, v_{k+1}</li></ul></li><li>Subgraphs : a graph G<code> = (V</code>, E<code>) is G \subset G iff V</code> \subset V and E` \subset E<ul><li>Connected graph : for every pair of vertices (u,v) there exists P = (u,&mldr;,v)</li></ul></li></ul><h1 id=trees-and-spanning-trees>Trees and Spanning Trees</h1><ul><li>Tree : a connected asyclic graph<ul><li>Spanning tree : a tree that contains all vertices in V and is a subgraph of G</li></ul></li><li>Let T be a spanning tree of a graph G. Then<ul><li>Any two vertices in T are connected by a unique simple path.</li><li>If any edge is removed from T, then T becomes diconnected.</li><li>If we add any edge into T, then the new graph will contain a cycle.</li><li>Number of edges in T is n-1</li></ul></li></ul><h1 id=minimum-spanning-trees-mst>Minimum Spanning Trees (MST)</h1><ul><li>Weight of a spanning tree w(T0) : the sum of weights of all edges in T</li><li>Minimum Spanning Tree : a spanning tree with the smallest possible weight</li><li>Examples of MST : Network Design, Airline routes</li></ul><h1 id=mst-related-algorithms>MST related algorithms</h1><ul><li>Kruskal&rsquo;s algorithm : For a graph with n vertices, keep on adding the least cost edge until n-1 edges added</li><li>Prim&rsquo;s algorithm : By focusing on vertices instead of edges, yielding a simple algorithm(implementation)</li><li>Dijkstra&rsquo;s algorithm : The single Source Shortest-Path problem</li></ul><h2 id=dijkstras-algorithm>Dijkstra&rsquo;s Algorithm</h2><ul><li><p>Pseudocode</p><p>function Dijkstra(graph, Source):
create vertex set Q
for each vertex v in Graph: // Initialization
dist[v] &lt;- INFINITY // Unknown distance from source to v
prev[v] &lt;- UNDIFINED // Previous node in optimal path from source
add v to Q
dist[source] &lt;- Q
while Q is not empty:
u &lt;- vertex in Q with min dist[u]
remove u from Q
for each neighbor v of u:
alt &lt;- dist[u] + length(u, v)
if alt &lt; dist[v]:
dist[v] &lt;- alt
prev[v] &lt;- u
return dist[], prev[]</p></li><li><p>Pseudocode (If we are only interested in a shortest path between vertices source and target)</p><p>S &lt;- empty sequence
u &lt;- target
if prev[u] is defined or u = source:
while u is defined:
insert u at the beginning of S
u &lt;- prev[u]</p></li><li><p>Algorithm using priority queue</p><p>function Dijkstra(Graph, source):
dist[source] &lt;- 0
create vertex set Q
for each vertex V in Graph:
if V != source
dist[v] &lt;- INFINITY
prev[v] &lt;- UNDEFINED
Q.add_with_priority(v, dist[v])
while Q is not empty:
u &lt;- Q.extract_min()
for each neighbor v of u:
alt &lt;- dist[u] + length(u, v)
if alt &lt; dist[v]
dist[v] &lt;- alt
prev[v] &lt;- u
Q.descreate_priority(v, alt)
return dist, prev</p></li></ul><h2 id=algorithm--complexity>Algorithm : Complexity</h2><ul><li>O(|V|^2)</li><li>O(|E| + |V|^2) = O(|V|^2)</li><li>If sparse graphs, Theta((|E| + |V|) * log|V|)</li></ul><h1 id=kruskals-algorithm>Kruskal&rsquo;s Algorithm</h1><ul><li><p>Kruskal&rsquo;s algorithm is a minimum-spanning-tree algorithm which finds an edge of the least possible weight that connects any two trees in the forest.</p><p>KRUSKAL(G):
A = 0
foreach v in G.V:
MAKE-SET(v)
foreach (u, v) in G.E ordered byu weight(u, v) increasing:
if FIND-SET(u) != FIND-SET(v):
A = A \union { (u, v) }
UNION(u, v)
return A</p></li></ul><h2 id=complexity>Complexity</h2><ul><li>O(E log V)</li></ul><h1 id=prims-algorithm>Prim&rsquo;s Algorithm</h1><ol><li>Initialize a tree with a single vertex, chosen arbitarily from the graph.</li><li>Grow the tree by one edge: of the edges that connect the tree to vertices not yet in the tree, find the minimum-weight edge, and transfer it to the tree</li><li>repeat step 2 until all vertices are in the tree</li></ol><h2 id=complexity-1>Complexity</h2><ul><li>Adjacency Matrix : O(|V|^2)</li><li>Binary heap and adjacency List : O(|E| log |V|)</li></ul><ol start=14><li>Dynamic algorithm</li></ol><ul><li>Dynamic Programming (Bellman equation)</li></ul><h1 id=introduction-1>Introduction</h1><ul><li>Steps in a dynamic programming solution<ul><li>Verify that the principle of optimality holds</li><li>Set up the dynamic-programming recurrence equations</li><li>Solve the dynamic-programming recurrence equations for the value of the optimal solution</li><li>Perform a trace back step in which the solution itself is constructed</li></ul></li><li>Core of dynamic programming<ul><li>Do not re-compute previously computed ones!</li><li>This method is also called &ldquo;Memoization&rdquo;</li></ul></li><li>Difficulties<ul><li>It may be impossible to decompose original problem into smaller ones</li><li>Impractical number of sub-problems</li></ul></li></ul><h1 id=all-pairs-shortest-paths>All pairs shortest paths</h1><h2 id=floyds-algorithm-time-complexity--on3>Floyd&rsquo;s algorithm (Time complexity : O(n^3))</h2><pre><code>function floyd(cost, a, n){
  for i := 1 to n do {
    for j := 1 to n do {
      a[i,j] := cost[i,j];
    }
  }
  for k := 1 to n do {
    for i := 1 to n do {
      for j := 1 to n do {
        a[i,j] := min(a[i,j], a[i,k] + a[k,j]);
      }
    }
  }
}
</code></pre><h2 id=multi-stage-graphs-time-complexity--thetav--e>Multi-Stage Graphs (Time complexity : Theta(|V| + |E|))</h2><pre><code>function fgraph(G, k, n, p){
  cost[n] : = 0;
  for j := n-1 to 1 step -1 do {
    let r be a vertex such that (j,r) is an edge of G and c[j,r] + cost[r] is minimum;
    cost[j] := c[j,r] + cost[r];
    d[j] := r;
  }
  p[1] := 1;
  p[k] := n;
  for j:= 2 to k-1 do {
    p[j] := d[p[j-1]];
  }
}
</code></pre><h1 id=the-knapsack-problem>The Knapsack Problem</h1><ul><li>w_k = the weight of each type-k item, for k = 1,2,&mldr;,N</li><li>r_k = the value associated with each type-k item, for k = 1,2,&mldr;,N</li><li>c = the weight capacity of the knapsack.</li></ul><ol start=15><li>Basic Traversal Methods</li></ol><h1 id=definitions>Definitions</h1><h2 id=search>Search</h2><ul><li>finding a path or traversal</li><li>between a start node and one of a set of goal nodes</li><li>a study of states and their transitions</li></ul><h2 id=traversal>Traversal</h2><ul><li>the search that involved the examination of every vertex in the tree.</li></ul><h1 id=techniques-for-traversal-of-a-binary-tree>Techniques for Traversal of a Binary Tree</h1><ul><li>Three ways to traverse a binary tree : In-order, Pre-order, Post-order</li><li>Common Point : The left subtree is traversed before The right subtree</li><li>Difference Point : THe time at which a node is visited.</li></ul><h2 id=inorder-traversal>Inorder Traversal</h2><pre><code>Algorithm inorder(Node t)
{
  if t!=0, then
  {
    inorder(t-&gt;left_child);
    visit(t);
    inorder(t-&gt;right_child);
  }
}
</code></pre><h2 id=preorder-traversal>Preorder Traversal</h2><pre><code>Algorithm preorder(Node t)
{
  if t!=0, then
    {
    visit(t);
    preorder(t-&gt;left_child);
    preorder(t-&gt;right_child);
  }
}
</code></pre><h2 id=postoder-traversal>Postoder Traversal</h2><pre><code>Algorithm postorder(Node t)
{
  if t!=0, then
    {
    postorder(t-&gt;left_child);
    postorder(t-&gt;right_child);
      visit(t);
  }
}
</code></pre><h1 id=non-recursive-binary-tree-traversal-algorithms>Non-Recursive Binary Tree Traversal Algorithms</h1><h2 id=non-recursive-inorder-traversal>Non-Recursive Inorder Traversal</h2><pre><code>Algorithm inorder(){
            stack[1] = 0;
            vertex = root;
top:	while(vertex!=0){
                push the vertex into the stack, vertex = leftchild(vertex)
            }
            pop the elemenet from the stack and make it as vertex
            while(vertex!=0){
                print the vertex node
                if(rightchild(vertex)!=0){
                    vertex = rightchild(vertex)
                    goto top
                }
            pop the element from the stack and made it as vertex
            }
}
</code></pre><h2 id=non-recursive-preorder-traversal>Non-Recursive Preorder Traversal</h2><pre><code>Algorithm Preorder(){
            stack[1] = 0;
            vertex = root;
            while(vertex!=0){
                print the vertex node
                if(rightchild(vertex)!=0){
                    push the right child of vertex into the stack
                if(leftchild(vertex) !=0){
                    vertex := leftson(vertex)
              else
                    pop the element from the stack and made it as vertex
            }
}
</code></pre><h2 id=non-recursive-postorder-traversal>Non-Recursive Postorder Traversal</h2><pre><code>Algorithm postorder(){
            stack[1] = 0;
            vertex = root;
top:	while(vertex!=0){
                push vertex onto stack
                if(rightchild(vertex)!=0)
                    push -vertex onto stack
                vertex := leftson(vertex)
            }
            pop from stack and make it as vertex
            while(vertex &gt; 0){
                print the vertex node, pop from stack and make it as vertex
            }
            if(vertex &lt; 0){
                vertex := -vertex
                goto top
            }
}
</code></pre></div><script src=/js/wikilink.js></script><div><script src=https://utteranc.es/client.js repo=makerdark98/makerdark98.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></main><footer><p class="copyright text-muted">© All rights reserved. Powered by <a href=https://gohugo.io>Hugo</a> and <a href=https://github.com/calintat/minimal>Minimal</a>.</p></footer></body></html>
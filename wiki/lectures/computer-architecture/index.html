<!doctype html><html lang=ko-kr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>lectures/computer architecture</title><link rel=icon href=http://makerdark98.dev/images/Rb.png><style>html body{font-family:raleway,sans-serif;background-color:#fff}:root{--accent: #00a3d2;--border-width:  5px }</style><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Raleway"><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css integrity=sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u crossorigin=anonymous><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css integrity=sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN crossorigin=anonymous><link rel=stylesheet href=/css/main.css><link rel=stylesheet href=/css/copy-btn.css><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script src=/js/copy-btn.js></script><script src=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js integrity=sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-98056974-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-98056974-1');</script><script>window.MathJax={jax:["input/TeX","output/CommonHTML"],extensions:["tex2jax.js","asciimath2jax.js","mml2jax.js","MathMenu.js","MathZoom.js"],TeX:{extensions:["AMSmath.js","AMSsymbols.js","autoload-all.js"]},tex2jax:{inlineMath:[['$','$'],["\\(","\\)"]],processEscapes:true},showProcessingMessages:false,messageStyle:"none",menuSettings:{zoom:"Click"},AuthorInit:function(){MathJax.Hub.Register.StartupHook("End",function(){var timeout=false,delay=250;var shrinkMath=function(){var dispFormulas=document.getElementsByClassName("formula");if(dispFormulas){var contentTest=document.getElementsByTagName("body")[0];var nodesWidth=contentTest.offsetWidth;var mathIndent=MathJax.Hub.config.displayIndent;var mathIndentValue=mathIndent.substring(0,mathIndent.length-2);for(var i=0;i<dispFormulas.length;i++){var dispFormula=dispFormulas[i];var wrapper=dispFormula.getElementsByClassName("MathJax_Preview")[0].nextSibling;var child=wrapper.firstChild;wrapper.style.transformOrigin="center";var oldScale=child.style.transform;var newValue=Math.min(0.80*dispFormula.offsetWidth/child.offsetWidth,1.0).toFixed(2);var newScale="scale("+newValue+")";if(!(newScale===oldScale)){wrapper.style.transform=newScale;wrapper.style["margin-left"]=Math.pow(newValue,4)*mathIndentValue+"px";var wrapperStyle=window.getComputedStyle(wrapper);var wrapperHeight=parseFloat(wrapperStyle.height);wrapper.style.height=""+(wrapperHeight*newValue)+"px";if(newValue==="1.00"){wrapper.style.cursor="";wrapper.style.height="";}
else{wrapper.style.cursor="zoom-in";}}}}};shrinkMath();window.addEventListener('resize',function(){clearTimeout(timeout);timeout=setTimeout(shrinkMath,delay);});});}};(function(d,script){script=d.createElement('script');script.type='text/javascript';script.async=true;script.onload=function(){};script.src='https://cdn.mathjax.org/mathjax/latest/MathJax.js';d.getElementsByTagName('head')[0].appendChild(script);}(document));</script><meta name=google-site-verification content="g_3tJyj-KkW-_wKx7Ij5GimHV1nKPZXetCz8ydbBAfA"></head><body><nav class="navbar navbar-default navbar-fixed-top"><div class=container><div class=navbar-header><a class="navbar-brand visible-xs" href=#>lectures/computer architecture</a>
<button class=navbar-toggle data-target=.navbar-collapse data-toggle=collapse>
<span class=icon-bar></span><span class=icon-bar></span><span class=icon-bar></span></button></div><div class="collapse navbar-collapse"><ul class="nav navbar-nav"><li><a href=/>Home</a></li><li><a href=/wiki/>Wiki</a></li><li><a href=/posts/>Posts</a></li><li><a href=/about/>About</a></li></ul><ul class="nav navbar-nav navbar-right"><li class=navbar-icon><a href=mailto:makerdark98@gmail.com><i class="fa fa-envelope-o"></i></a></li><li class=navbar-icon><a href=https://github.com/makerdark98/><i class="fa fa-github"></i></a></li><li><div style=max-width:250px;display:inline-block;max-height:40px><script async src="https://cse.google.com/cse.js?cx=003491619885022567520:wnnypdnx4aj"></script><div class=gcse-search></div></div></li></ul></div></div></nav><main><div class=navigator style=display:flex><div style=display:flex><div class=parent-doc style=flex:none><button class="btn btn-link" onclick="(function(elem){elem.querySelector('a').click();})(this);">
<i class="fa fa-arrow-left"></i>[[lecture]]</button></div></div><div style=margin-left:auto><div style=display:flex><div style=margin-left:auto><div class=wiki-history></div></div></div><script src=/js/localhistory.js></script><script>var now="computer architecture"
pushHistory(now);</script></div></div><div><h2>lectures/computer architecture</h2><a href=https://github.com/makerdark98/makerdark98.github.io/blame/master/src/content/wiki/lectures/computer%20architecture.md><h5>created : Tue, 07 Apr 2020 20:37:08 +0900</h5><h5>modified : Sat, 27 Jun 2020 15:23:27 +0900</h5></a><a href=http://makerdark98.dev/tags/lecture><kbd class=item-tag>lecture</kbd></a>
<a href=http://makerdark98.dev/tags/comuter-architecture><kbd class=item-tag>comuter architecture</kbd></a></div><aside class=navbar id=nav-toc style=text-align:left><nav id=TableOfContents><ul><li><a href=#disk-sectors-and-access>Disk Sectors and Access</a></li><li><a href=#disk-access-example>Disk Access Example</a></li></ul><ul><li><a href=#direct-mapped-cache>Direct Mapped Cache</a></li><li><a href=#tags-and-valid-bits>Tags and Valid Bits</a></li></ul><ul><li><a href=#write-through>Write-through</a></li><li><a href=#write-back>Write-Back</a></li></ul></nav></aside><div align=start class=content data-spy=scroll data-offset=20 data-target=#nav-toc style=position:relative><ol start=5><li>Large and Fast : Exploiting Memory Hierarchy5.</li></ol><h1 id=principle-of-locality>Principle of Locality</h1><ul><li>Temporal locality</li><li>Spatial locality</li></ul><h1 id=taking-advantage-of-locality>Taking Advantage of Locality</h1><ul><li>Memory hierarchy<ul><li>Store everything on disk (lowest level)</li><li>Copy recently accessed (and nearby) items from disk to smaller DRAM(e.g. Main Memory)</li><li>copy more recently accessed (and nearby) items from DRAM to smaller SRAM memory(e.g. Cache memory attached to CPU)</li></ul></li></ul><h1 id=memory-hierarchy-levels>Memory Hierarchy Levels</h1><ul><li>A block (aka line) : unit of copying</li><li>If accessed data is present in upper level<ul><li>Hit : access satisfied by upper level<ul><li>Hit ratio: hits/accesses</li></ul></li></ul></li><li>If accessed data is absent<ul><li>Miss : block copied from lower level<ul><li>Time taken : miss penalty</li><li>Miss ratio : misses/accesses = 1 - hit ratio</li></ul></li><li>Then accessed data supplied from upper level</li></ul></li></ul><h1 id=memory-technology>Memory Technology</h1><ul><li>Static RAM (SRAM) : 0.5ns - 2.5ns, $2000 - $5000 per GB</li><li>Dynamic RAM(DRAM) : 50ns - 70ns, $20 - $75 per GB</li><li>Magnetic disk : 5ms - 20ms, $0.20 - $2 per GB</li><li>Ideal memory<ul><li>Access time of SRAM</li><li>Capacity and cost/GB of disk</li></ul></li></ul><h1 id=sram-technology>SRAM Technology</h1><ul><li>Data stored using 6~8 transistors in an IC<ul><li>fast, but expensive</li><li>fixed access time to any datum</li><li>no refresh needed</li><li>usually for caches, integrated on processor chips</li></ul></li></ul><h1 id=dram-technology>DRAM Technology</h1><ul><li>Data stored as a charge in a capacitor<ul><li>Single transistor used to access the charge</li><li>Must periodically be refreshed<ul><li>Read contents and write back</li><li>Performed on a DRAM &ldquo;row&rdquo;</li></ul></li></ul></li><li>Synchronous DRAM (SDRAM)<ul><li>DRAM with clocks to improve bandwidth</li></ul></li><li>Ex> Double data rate (DDR) DRAM<ul><li>Transfer on rising and falling clock edges</li></ul></li></ul><h1 id=flash-memory>Flash Memory</h1><ul><li>Non-volatile semiconductor storage ( a type of EEPROM)<ul><li>100x - 1000x faster than disk</li><li>smaller, lower power, more robust</li><li>But more $/GB (between disk and DRAM)</li></ul></li><li>Types<ul><li>NOR flash : bit cell like a NOR gate<ul><li>Random read/write access</li><li>Used for instruction memory in embedded systems</li></ul></li><li>NAND flash : bit cell like a NAND gate<ul><li>Denser (bits/area), but block-at-a-time access</li><li>Cheaper per GB</li><li>Used for USB keys, media storage, &mldr;</li></ul></li></ul></li><li>Flash bits wears out after 1000&rsquo;s of accesses<ul><li>Not suitable for direct RAM or disk replacemenet</li><li>Wear leveling : remap data to less used blocks</li></ul></li></ul><h1 id=disk-storage>Disk Storage</h1><ul><li>Nonvolatile, rotating magnetic storage</li></ul><h2 id=disk-sectors-and-access>Disk Sectors and Access</h2><ul><li>Each sector records : Sector ID, Data(512 bytes, 4096 bytes proposed), Error correcting code (ECC), Synchronization fields and gaps</li><li>Access to a sector involves : Queuing delay if other accesses are pending, Seek(move the heads), Rotational latency, Data transfer,Controller overhead</li></ul><h2 id=disk-access-example>Disk Access Example</h2><ul><li><p>Given : 512B sector, 15000rpm, 4ms average seek time, 100MB/s transfer rate, 0.2ms controller overhead, idle disk</p></li><li><p>Average read time</p><p>= 4ms (seek time)</p><ul><li><p>1/2 / (15000 (rpm) / 60 (min/s) ) (rotational latency)</p></li><li><p>512 / 100 MB/s (transfer time)</p></li><li><p>0.2ms controller delay</p></li></ul><p>= 6.2ms</p></li><li><p>If actual average seek time is 1 ms ⇒ Average read time = 3.2ms</p></li></ul><h1 id=disk-performance-issues>Disk Performance Issues</h1><ul><li>Manufacturers quote average seek time<ul><li>Based on all possible seeks</li><li>Locality and OS scheduling lead to smaller actual average seek times</li></ul></li><li>Smart disk controller allocate physical sectors on disk<ul><li>Present logical sector interface to host</li><li>SCSI, ATA, SATA</li></ul></li><li>Disk drives include caches<ul><li>Prefetch sectors in anticipation of access</li><li>Avoid seek and rotational delay</li></ul></li></ul><h1 id=the-basics-of-cache-memory>The Basics of Cache Memory</h1><ul><li>Cache memory : The level of the memory hierarchy closest to the CPU</li></ul><h2 id=direct-mapped-cache>Direct Mapped Cache</h2><ul><li>Location determined by address</li><li>Direct mapped : only one choice ( ex : (block address) modulo (#Blocks in cache))</li></ul><h2 id=tags-and-valid-bits>Tags and Valid Bits</h2><ul><li>How do we know which particular block is stored in a cache location<ul><li>Store block address as well as the data</li><li>Actually, only need the high-order bits</li><li>Called the tag</li></ul></li><li>What if there is no data in a location?<ul><li>Valid bit : 1 = present, 0 = not present, Initially 0</li></ul></li></ul><h1 id=block-size-considerations>Block Size Considerations</h1><ul><li>Larger blocks should reduce miss rate</li><li>But in fixed-sized cache<ul><li>Larger blocks ⇒ fewer of them (increased miss rate)</li><li>Larger blocs ⇒ pollution</li></ul></li><li>Larger miss penalty<ul><li>Can override benefit of reduced miss rate</li><li>Early restart and critical-word-first can help</li></ul></li></ul><h1 id=cache-read-misses>Cache (Read) Misses</h1><ul><li>On cache hit, CPU proceeds normally</li><li>On cache miss<ul><li>Stall the CPU pipeline</li><li>Fetch block from next level of hierarchy<ul><li>Instruct main memory to perform a read and wait form the memory to complete its access</li><li>Write content into the cache entry (including data, tag, and valid bit)</li></ul></li><li>Restart cache access<ul><li>For <em>instruction cache</em> miss<ul><li>Send original PC value (that is, PC -4), and restart instruction fetch</li></ul></li><li>For <em>data cache</em> miss<ul><li>Complete data access</li></ul></li></ul></li></ul></li></ul><h1 id=cache-write-misses>Cache (Write) Misses</h1><ul><li>Similar to cache read misses, but need extra steps</li><li>What should happen on a write miss?<ul><li>On data-write hit, could just update the block in cache</li><li>But if &lsquo;store&rsquo; writes something only into the cache, then memory will have different value from that in the cache ⇒ inconsistency!</li></ul></li></ul><h2 id=write-through>Write-through</h2><ul><li>Always update both the cache and the memory.<ul><li>write to cache first, then write also to memory</li><li>simple</li></ul></li><li>But makes writes take longer<ul><li>if base CPI = 1, 10% of instructions are stores, write to memory takes 100 cycles → Effective CPI = 1 + 0.1x100 = 11</li></ul></li><li>Solution : need write buffer<ul><li>Holds data waiting to be written to memory</li><li>CPU continues immediately<ul><li>only stalls on write if write buffer is already full</li></ul></li></ul></li></ul><h2 id=write-back>Write-Back</h2><ul><li>update only the cache, and write the modified block to memory when it is replaced<ul><li>On data-write hit, just update the block in cache<ul><li>Keep track of whether each block is &ldquo;dirty&rdquo;</li></ul></li><li>When a dirty block is replaced<ul><li>Write it back to meory</li><li>Can use a write buffer to allow replacing block to be read first</li></ul></li></ul></li><li>Usually faster, but much more complex<ul><li>we cannot overwrite the cache before detecting miss (since previous value may be dirty)</li><li>thus, we must detect first, and when write to cache(→extra cycle)</li></ul></li></ul><h1 id=write-allocation>Write Allocation</h1><ul><li>On a write miss, should we fetch the memory into cache?</li><li>For write-through, two alternatives:<ul><li>Allocate on miss: fetch the block</li><li>Write around : don&rsquo;t fetch the block<ul><li>Since programs often write a whole block before reading it</li></ul></li></ul></li><li>For write-back<ul><li>Usually fetch the block</li></ul></li></ul><h1 id=measuring-cache-performance>Measuring Cache Performance</h1><ul><li>Cpu time = (CPU execution time) + (Memory-stall time)<ul><li>Program execution cycles<ul><li>Includes cache hit time</li></ul></li><li>Memory stall cycles<ul><li>Mainly from cache misses</li></ul></li></ul></li><li>With simplifying assumptions<ul><li><p>assuming similar cycles for read and write (ignoring write buffer stalls)</p></li><li><p>Memory stall cycles =</p><p>Memory accesses / Program * Miss rate * Miss penalty</p><ul><li>Instructions / Program * Misses/Instruction * Miss penalty</li></ul></li></ul></li></ul><h1 id=average-access-time>Average Access Time</h1><ul><li>Hit time is also important for performance<ul><li>although we assumed this as part of CPU execution time..</li></ul></li><li>Average memory access time (AMAT)<ul><li>AMAT = Hit time + Miss rate * Miss penalty</li></ul></li></ul><h1 id=associative-caches>Associative Caches</h1><ul><li>Unlike &lsquo;Direct-mapped&rsquo; Caches<ul><li>where there is only one cache entry location for any block.</li></ul></li><li>Fully assocative<ul><li>Allow a given block to go in any cache entry</li><li>More flexible, and reduces miss rate!</li><li>Requires all entries to be searched at once</li><li>Comparator per entry → Expensive!</li></ul></li><li>M-way Set assocative<ul><li>Each set contains M entries</li><li>Block number determines which set</li><li>Search all entries in a given set at once</li><li>M comparators (less expensive)</li></ul></li></ul><h1 id=how-much-associativity>How much Associativity</h1><ul><li>Increased associativity decreases miss rate<ul><li>But with diminishing returns</li><li>Also, may increase search time or cost</li><li>Choice depends on the cost of a miss versus the cost of implementing associativity, both in time and in extra hardware</li></ul></li><li>Size of Tags versus set associativity</li></ul><h1 id=replacement-policy>Replacement Policy</h1><ul><li>Direct mapped : no choice</li><li>Set associative<ul><li>Prefer non-valid entry, if there is one</li><li>Otherwise, choose among entries in the set → but how?</li></ul></li><li>Least-recently used(LRU)<ul><li>Choose the one unused for the longest time<ul><li>Simple for 2-way, manageable for 4-way, too hard beyond that</li></ul></li></ul></li><li>Random<ul><li>Gives approximately the same performance as LRU for high associativity</li></ul></li></ul><h1 id=multilevel-caches>Multilevel Caches</h1><ul><li>Primary cache attached to CPU<ul><li>small, but fast</li><li>Focus on minimal hit time<ul><li>L-1 cache usually smaller than a single cache</li><li>L-1 block size smaller than L-2 block size</li></ul></li></ul></li><li>Level-2(L-2) cache services misses from primary cache<ul><li>Larger, slower, but still faster than main memory</li><li>Focus on low miss rate to avoid main memory access</li><li>Hit time has less overall impact</li></ul></li><li>Main memory services L-2 cache misses</li><li>Some high-end systems include L-3 cache</li></ul><h1 id=encodingdecoding>Encoding/Decoding</h1><ul><li>Encoding/Decoding<ul><li>error detection/correction</li><li>encryption/decryption</li><li>compression/decompression</li><li>modulation/demodulation</li></ul></li><li>If error is detected<ul><li>may drop data</li><li>may try to fix the error</li></ul></li></ul></div><script src=/js/wikilink.js></script><div><script src=https://utteranc.es/client.js repo=makerdark98/makerdark98.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></main><footer><p class="copyright text-muted">© All rights reserved. Powered by <a href=https://gohugo.io>Hugo</a> and <a href=https://github.com/calintat/minimal>Minimal</a>.</p></footer></body></html>